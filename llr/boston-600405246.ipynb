{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression():\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self,d):\n",
    "        self.d=d+1\n",
    "        self.w = np.zeros(self.d)\n",
    "        self.alpha = 1\n",
    "\n",
    "    def set_learning_rate(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, x, y, iteration=1500):\n",
    "        self.y = y\n",
    "        \n",
    "        self.x = np.append(x, np.ones((x.shape[0], 1)), axis=1)\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            self.make_one_update()\n",
    "\n",
    "    # Class function to make an update for w\n",
    "    def make_one_update(self):\n",
    "        w_current = self.w\n",
    "        # Set step size \n",
    "        step = (-1)*self.alpha*self.compute_gradient(w_current)\n",
    "        w_update = w_current + step\n",
    "    \n",
    "        current_loss = self.sq_loss(w_current)\n",
    "        update_loss = self.sq_loss(w_update)\n",
    "        if current_loss > update_loss:\n",
    "            print(\"Loss decreases to \", update_loss,)\n",
    "        else:\n",
    "            print(\"Loss increases to \", update_loss,)\n",
    "        self.w = w_update\n",
    "    \n",
    "    def norm_w(self):\n",
    "        return np.inner(self.w,self.w)\n",
    "    \n",
    "    def compute_gradient(self, w_current):\n",
    "        grad_v = np.zeros(self.d)\n",
    "        for x in range(self.d):\n",
    "            for i in range(self.x.shape[0]):\n",
    "                grad_v[x] += 2*self.x[i][x]*(np.inner(self.w,self.x[i]) - self.y[i])\n",
    "#         print(\"The norm of grad vector is \", math.sqrt(np.inner(grad_v, grad_v)))\n",
    "        return grad_v\n",
    "\n",
    "    def sq_loss(self, w):\n",
    "        loss = 0\n",
    "        for i in range(self.x.shape[0]):\n",
    "            loss += pow((np.inner(w,self.x[i]))-y[i],2)\n",
    "        return loss/np.shape(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regreesion_closedform():\n",
    "    def __init__(self,d):\n",
    "        self.w = np.zeros(d)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        a = np.linalg.pinv(np.inner(np.transpose(x),np.transpose(x)))\n",
    "        b = np.inner(np.transpose(x),y)\n",
    "        self.w = np.inner(a,b)\n",
    "        print(self.w)\n",
    "    \n",
    "    def norm_w(self):\n",
    "        return np.inner(self.w,self.w)   \n",
    "    \n",
    "    def sq_loss(self):\n",
    "        loss = 0\n",
    "        for i in range(self.x.shape[0]):\n",
    "            loss += pow((np.inner(self.w,self.x[i]))-y[i],2)\n",
    "        return loss/np.shape(x)[0]\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "x = boston.data\n",
    "y= boston.target\n",
    "d = np.shape(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = linear_regreesion_closedform(d)\n",
    "h2.fit(x,y)\n",
    "print(\"lost =\",h2.sq_loss()) \n",
    "\n",
    "#lost = 24.166099330126507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = linear_regression(d)\n",
    "# h.set_learning_rate(0.000000005)\n",
    "h.set_learning_rate(0.0000000062)\n",
    "\n",
    "\n",
    "h.fit(x, y, iteration=4000)\n",
    "# in last iteration: Loss decreases to  55.30958009178475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.norm_w(),h2.norm_w())\n",
    "# 0.24094474364830304 52.82199078521046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
