{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression():\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self,d):\n",
    "        self.d=d+1\n",
    "        self.w = np.zeros(self.d)\n",
    "        self.alpha = 1\n",
    "\n",
    "    def set_learning_rate(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, x, y, iteration=1500):\n",
    "        self.y = y\n",
    "        \n",
    "        self.x = np.append(x, np.ones((x.shape[0], 1)), axis=1)\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            self.make_one_update()\n",
    "\n",
    "    # Class function to make an update for w\n",
    "    def make_one_update(self):\n",
    "        w_current = self.w\n",
    "        # Set step size \n",
    "        step = (-1)*self.alpha*self.compute_gradient(w_current)\n",
    "        w_update = w_current + step\n",
    "    \n",
    "        current_loss = self.sq_loss(w_current)\n",
    "        update_loss = self.sq_loss(w_update)\n",
    "        if current_loss > update_loss:\n",
    "            print(\"Loss decreases to \", update_loss,)\n",
    "        else:\n",
    "#             print(\"Loss increases to \", update_loss,)\n",
    "        self.w = w_update\n",
    "    \n",
    "    def norm_w(self):\n",
    "        return np.inner(self.w,self.w)\n",
    "    \n",
    "    def compute_gradient(self, w_current):\n",
    "        grad_v = np.zeros(self.d)\n",
    "        for x in range(self.d):\n",
    "            for i in range(self.x.shape[0]):\n",
    "                grad_v[x] += 2*self.x[i][x]*(np.inner(self.w,self.x[i]) - self.y[i])\n",
    "#         print(\"The norm of grad vector is \", math.sqrt(np.inner(grad_v, grad_v)))\n",
    "        return grad_v\n",
    "\n",
    "    def sq_loss(self, w):\n",
    "        loss = 0\n",
    "        for i in range(self.x.shape[0]):\n",
    "            loss += pow((np.inner(w,self.x[i]))-y[i],2)\n",
    "        return loss/np.shape(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regreesion_closedform():\n",
    "    def __init__(self,d):\n",
    "        self.w = np.zeros(d)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        a = np.linalg.pinv(np.inner(np.transpose(x),np.transpose(x)))\n",
    "        print(a.shape[0],a.shape[1])\n",
    "        b = np.inner(np.transpose(x),y)\n",
    "        self.w = np.inner(a,b)\n",
    "        print(self.w)\n",
    "    \n",
    "    def norm_w(self):\n",
    "        return np.inner(self.w,self.w)   \n",
    "    \n",
    "    def sq_loss(self,w):\n",
    "        loss = 0\n",
    "        for i in range(self.x.shape[0]):\n",
    "            loss += pow((np.inner(w,self.x[i]))-y[i],2)\n",
    "        return loss/np.shape(x)[0]\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "x = boston.data\n",
    "y= boston.target\n",
    "d = np.shape(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n",
      "[-9.28965170e-02  4.87149552e-02 -4.05997958e-03  2.85399882e+00\n",
      " -2.86843637e+00  5.92814778e+00 -7.26933458e-03 -9.68514157e-01\n",
      "  1.71151128e-01 -9.39621540e-03 -3.92190926e-01  1.49056102e-02\n",
      " -4.16304471e-01]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'linear_regreesion_closedform' object has no attribute 'sq_lost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-929d2875e776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_regreesion_closedform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msq_lost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'linear_regreesion_closedform' object has no attribute 'sq_lost'"
     ]
    }
   ],
   "source": [
    "h2 = linear_regreesion_closedform(d)\n",
    "h2.fit(x,y)\n",
    "print(h2.sq_lost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  12123375.837606803\n",
      "Loss decreases to  544.5566218907381\n",
      "The norm of grad vector is  11675220.59562614\n",
      "Loss decreases to  503.4442041309936\n",
      "The norm of grad vector is  11246077.748987323\n",
      "Loss decreases to  467.5013897484403\n",
      "The norm of grad vector is  10834558.803055305\n",
      "Loss decreases to  435.74437443727805\n",
      "The norm of grad vector is  10439496.88909144\n",
      "Loss decreases to  407.4275979911766\n",
      "The norm of grad vector is  10059897.160771888\n",
      "Loss decreases to  381.9808799534804\n",
      "The norm of grad vector is  9694898.982464815\n",
      "Loss decreases to  358.9635547243195\n",
      "The norm of grad vector is  9343747.115688393\n",
      "Loss decreases to  338.03097912163383\n",
      "The norm of grad vector is  9005769.756763842\n",
      "Loss decreases to  318.910046942212\n",
      "The norm of grad vector is  8680361.782323098\n",
      "Loss decreases to  301.38126250531224\n",
      "The norm of grad vector is  8366971.94878888\n",
      "Loss decreases to  285.26559235965806\n",
      "The norm of grad vector is  8065093.091433205\n",
      "Loss decreases to  270.4147995590834\n",
      "The norm of grad vector is  7774254.597940069\n",
      "Loss decreases to  256.70431780421126\n",
      "The norm of grad vector is  7494016.606409603\n",
      "Loss decreases to  244.02797940615935\n",
      "The norm of grad vector is  7223965.5109656025\n",
      "Loss decreases to  232.29409770420142\n",
      "The norm of grad vector is  6963710.459342262\n",
      "Loss decreases to  221.42254035211596\n",
      "The norm of grad vector is  6712880.603603193\n",
      "Loss decreases to  211.3425286594169\n",
      "The norm of grad vector is  6471122.9233168475\n",
      "Loss decreases to  201.990970028734\n",
      "The norm of grad vector is  6238100.484546159\n",
      "Loss decreases to  193.31118281023072\n",
      "The norm of grad vector is  6013491.031318954\n",
      "Loss decreases to  185.25191093669275\n",
      "The norm of grad vector is  5796985.831428198\n",
      "Loss decreases to  177.76655339085687\n",
      "The norm of grad vector is  5588288.717442541\n",
      "Loss decreases to  170.81255371303953\n",
      "The norm of grad vector is  5387115.278186129\n",
      "Loss decreases to  164.35090943514\n",
      "The norm of grad vector is  5193192.166807998\n",
      "Loss decreases to  158.34577201991704\n",
      "The norm of grad vector is  5006256.499764934\n",
      "Loss decreases to  152.764115677804\n",
      "The norm of grad vector is  4826055.327237528\n",
      "Loss decreases to  147.57545911727357\n",
      "The norm of grad vector is  4652345.160179116\n",
      "Loss decreases to  142.75162843322764\n",
      "The norm of grad vector is  4484891.542732451\n",
      "Loss decreases to  138.26655236880396\n",
      "The norm of grad vector is  4323468.661420102\n",
      "Loss decreases to  134.09608340330246\n",
      "The norm of grad vector is  4167858.9845332997\n",
      "Loss decreases to  130.21783974357314\n",
      "The norm of grad vector is  4017852.9266703604\n",
      "Loss decreases to  126.61106448909212\n",
      "The norm of grad vector is  3873248.5345306923\n",
      "Loss decreases to  123.25649911897779\n",
      "The norm of grad vector is  3733851.1909445426\n",
      "Loss decreases to  120.1362690975788\n",
      "The norm of grad vector is  3599473.334781285\n",
      "Loss decreases to  117.23377987590182\n",
      "The norm of grad vector is  3469934.194881533\n",
      "Loss decreases to  114.53362192419175\n",
      "The norm of grad vector is  3345059.5365404356\n",
      "Loss decreases to  112.02148369924828\n",
      "The norm of grad vector is  3224681.4193602353\n",
      "Loss decreases to  109.68407165250521\n",
      "The norm of grad vector is  3108637.965512062\n",
      "Loss decreases to  107.50903653894919\n",
      "The norm of grad vector is  2996773.137616781\n",
      "Loss decreases to  105.4849054053177\n",
      "The norm of grad vector is  2888936.5255852803\n",
      "Loss decreases to  103.60101872801421\n",
      "The norm of grad vector is  2784983.1418592767\n",
      "Loss decreases to  101.84747224353542\n",
      "The norm of grad vector is  2684773.2245719144\n",
      "Loss decreases to  100.2150630719449\n",
      "The norm of grad vector is  2588172.048208574\n",
      "Loss decreases to  98.69523978063269\n",
      "The norm of grad vector is  2495049.741396459\n",
      "Loss decreases to  97.28005607398302\n",
      "The norm of grad vector is  2405281.1114898175\n",
      "Loss decreases to  95.96212782655532\n",
      "The norm of grad vector is  2318745.4756484493\n",
      "Loss decreases to  94.73459320446686\n",
      "The norm of grad vector is  2235326.4981322577\n",
      "Loss decreases to  93.59107564286627\n",
      "The norm of grad vector is  2154912.0335552488\n",
      "Loss decreases to  92.52564946756618\n",
      "The norm of grad vector is  2077393.9758596935\n",
      "Loss decreases to  91.53280796660893\n",
      "The norm of grad vector is  2002668.1127858944\n",
      "Loss decreases to  90.60743373326565\n",
      "The norm of grad vector is  1930633.9856256829\n",
      "Loss decreases to  89.74477111602407\n",
      "The norm of grad vector is  1861194.7540587909\n",
      "Loss decreases to  88.94040062380718\n",
      "The norm of grad vector is  1794257.0658812039\n",
      "Loss decreases to  88.19021514614863\n",
      "The norm of grad vector is  1729730.9314433383\n",
      "Loss decreases to  87.49039785852939\n",
      "The norm of grad vector is  1667529.6026239954\n",
      "Loss decreases to  86.83740169265955\n",
      "The norm of grad vector is  1607569.4561733988\n",
      "Loss decreases to  86.22793026027962\n",
      "The norm of grad vector is  1549769.8812654838\n",
      "Loss decreases to  85.65892012714913\n",
      "The norm of grad vector is  1494053.1711059944\n",
      "Loss decreases to  85.12752434135358\n",
      "The norm of grad vector is  1440344.418448956\n",
      "Loss decreases to  84.63109712694396\n",
      "The norm of grad vector is  1388571.414879717\n",
      "Loss decreases to  84.16717966030662\n",
      "The norm of grad vector is  1338664.5537282093\n",
      "Loss decreases to  83.73348685255117\n",
      "The norm of grad vector is  1290556.7364811238\n",
      "Loss decreases to  83.32789506667945\n",
      "The norm of grad vector is  1244183.2825665935\n",
      "Loss decreases to  82.9484307033606\n",
      "The norm of grad vector is  1199481.8423896448\n",
      "Loss decreases to  82.59325959384076\n",
      "The norm of grad vector is  1156392.313501171\n",
      "Loss decreases to  82.2606771428803\n",
      "The norm of grad vector is  1114856.7597874058\n",
      "Loss decreases to  81.94909916865883\n",
      "The norm of grad vector is  1074819.3335710543\n",
      "Loss decreases to  81.65705339034926\n",
      "The norm of grad vector is  1036226.2005191327\n",
      "Loss decreases to  81.38317151755294\n",
      "The norm of grad vector is  999025.4672564096\n",
      "Loss decreases to  81.12618189903459\n",
      "The norm of grad vector is  963167.111586985\n",
      "Loss decreases to  80.8849026912035\n",
      "The norm of grad vector is  928602.915230024\n",
      "Loss decreases to  80.6582355095887\n",
      "The norm of grad vector is  895286.3989791184\n",
      "Loss decreases to  80.4451595291581\n",
      "The norm of grad vector is  863172.7601979328\n",
      "Loss decreases to  80.24472600174266\n",
      "The norm of grad vector is  832218.8125679789\n",
      "Loss decreases to  80.05605316107467\n",
      "The norm of grad vector is  802382.9280073689\n",
      "Loss decreases to  79.87832148803511\n",
      "The norm of grad vector is  773624.9806823052\n",
      "Loss decreases to  79.71076931064138\n",
      "The norm of grad vector is  745906.29303586\n",
      "Loss decreases to  79.5526887151083\n",
      "The norm of grad vector is  719189.5837613214\n",
      "Loss decreases to  79.40342174598955\n",
      "The norm of grad vector is  693438.9176499487\n",
      "Loss decreases to  79.2623568749612\n",
      "The norm of grad vector is  668619.6572455086\n",
      "Loss decreases to  79.12892571925303\n",
      "The norm of grad vector is  644698.416240363\n",
      "Loss decreases to  79.00259999208005\n",
      "The norm of grad vector is  621643.014550188\n",
      "Loss decreases to  78.88288866866786\n",
      "The norm of grad vector is  599422.4350066624\n",
      "Loss decreases to  78.76933535263383\n",
      "The norm of grad vector is  578006.7816095965\n",
      "Loss decreases to  78.66151582855485\n",
      "The norm of grad vector is  557367.2392820223\n",
      "Loss decreases to  78.55903578756134\n",
      "The norm of grad vector is  537476.0350737983\n",
      "Loss decreases to  78.46152871372365\n",
      "The norm of grad vector is  518306.4007611598\n",
      "Loss decreases to  78.3686539198617\n",
      "The norm of grad vector is  499832.5367915049\n",
      "Loss decreases to  78.28009472221565\n",
      "The norm of grad vector is  482029.577524451\n",
      "Loss decreases to  78.19555674415976\n",
      "The norm of grad vector is  464873.55772194144\n",
      "Loss decreases to  78.11476633983489\n",
      "The norm of grad vector is  448341.380241764\n",
      "Loss decreases to  78.03746912922291\n",
      "The norm of grad vector is  432410.7848904461\n",
      "Loss decreases to  77.9634286367844\n",
      "The norm of grad vector is  417060.3183929883\n",
      "Loss decreases to  77.89242502633559\n",
      "The norm of grad vector is  402269.30543833325\n",
      "Loss decreases to  77.82425392536368\n",
      "The norm of grad vector is  388017.82076086244\n",
      "Loss decreases to  77.75872533245483\n",
      "The norm of grad vector is  374286.66221955285\n",
      "Loss decreases to  77.6956626019597\n",
      "The norm of grad vector is  361057.32483767334\n",
      "Loss decreases to  77.63490150043681\n",
      "The norm of grad vector is  348311.9757671485\n",
      "Loss decreases to  77.57628932979756\n",
      "The norm of grad vector is  336033.43014286814\n",
      "Loss decreases to  77.51968411243796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  324205.1277933493\n",
      "Loss decreases to  77.46495383397549\n",
      "The norm of grad vector is  312811.11077524076\n",
      "Loss decreases to  77.41197573951654\n",
      "The norm of grad vector is  301836.00170016184\n",
      "Loss decreases to  77.36063567967204\n",
      "The norm of grad vector is  291264.9828233748\n",
      "Loss decreases to  77.31082750280264\n",
      "The norm of grad vector is  281083.77586472145\n",
      "Loss decreases to  77.26245249022568\n",
      "The norm of grad vector is  271278.62253317656\n",
      "Loss decreases to  77.21541883134712\n",
      "The norm of grad vector is  261836.26572721015\n",
      "Loss decreases to  77.1696411358942\n",
      "The norm of grad vector is  252743.93138401816\n",
      "Loss decreases to  77.12503998062783\n",
      "The norm of grad vector is  243989.31095147695\n",
      "Loss decreases to  77.08154148809643\n",
      "The norm of grad vector is  235560.5444574632\n",
      "Loss decreases to  77.03907693516449\n",
      "The norm of grad vector is  227446.2041519367\n",
      "Loss decreases to  76.99758238921349\n",
      "The norm of grad vector is  219635.27869793482\n",
      "Loss decreases to  76.95699837005596\n",
      "The norm of grad vector is  212117.15788837272\n",
      "Loss decreases to  76.91726953574822\n",
      "The norm of grad vector is  204881.61786624274\n",
      "Loss decreases to  76.87834439060838\n",
      "The norm of grad vector is  197918.80682657214\n",
      "Loss decreases to  76.84017501387228\n",
      "The norm of grad vector is  191219.23117920617\n",
      "Loss decreases to  76.80271680752851\n",
      "The norm of grad vector is  184773.74215224406\n",
      "Loss decreases to  76.7659282619747\n",
      "The norm of grad vector is  178573.52281673162\n",
      "Loss decreases to  76.72977073823695\n",
      "The norm of grad vector is  172610.07551401484\n",
      "Loss decreases to  76.69420826558047\n",
      "The norm of grad vector is  166875.20966799272\n",
      "Loss decreases to  76.65920735342414\n",
      "The norm of grad vector is  161361.0299654193\n",
      "Loss decreases to  76.62473681654652\n",
      "The norm of grad vector is  156059.92488831896\n",
      "Loss decreases to  76.59076761264483\n",
      "The norm of grad vector is  150964.55558361538\n",
      "Loss decreases to  76.55727269137378\n",
      "The norm of grad vector is  146067.84505615162\n",
      "Loss decreases to  76.52422685405061\n",
      "The norm of grad vector is  141362.96767243935\n",
      "Loss decreases to  76.49160662327564\n",
      "The norm of grad vector is  136843.33896372395\n",
      "Loss decreases to  76.45939012176356\n",
      "The norm of grad vector is  132502.60571829823\n",
      "Loss decreases to  76.42755695973825\n",
      "The norm of grad vector is  128334.636354413\n",
      "Loss decreases to  76.3960881302818\n",
      "The norm of grad vector is  124333.51156665882\n",
      "Loss decreases to  76.36496591207879\n",
      "The norm of grad vector is  120493.51524029582\n",
      "Loss decreases to  76.33417377903105\n",
      "The norm of grad vector is  116809.12562966331\n",
      "Loss decreases to  76.30369631625774\n",
      "The norm of grad vector is  113275.00679854325\n",
      "Loss decreases to  76.27351914202958\n",
      "The norm of grad vector is  109886.00032210613\n",
      "Loss decreases to  76.24362883521744\n",
      "The norm of grad vector is  106637.1172518371\n",
      "Loss decreases to  76.21401286786529\n",
      "The norm of grad vector is  103523.53034659127\n",
      "Loss decreases to  76.18465954252638\n",
      "The norm of grad vector is  100540.56657459668\n",
      "Loss decreases to  76.15555793402446\n",
      "The norm of grad vector is  97683.69989280228\n",
      "Loss decreases to  76.12669783532733\n",
      "The norm of grad vector is  94948.5443113743\n",
      "Loss decreases to  76.09806970724442\n",
      "The norm of grad vector is  92330.84725233486\n",
      "Loss decreases to  76.06966463167431\n",
      "The norm of grad vector is  89826.48321226309\n",
      "Loss decreases to  76.04147426815406\n",
      "The norm of grad vector is  87431.44773957574\n",
      "Loss decreases to  76.01349081347665\n",
      "The norm of grad vector is  85141.85173711405\n",
      "Loss decreases to  75.98570696415779\n",
      "The norm of grad vector is  82953.91610056382\n",
      "Loss decreases to  75.9581158815532\n",
      "The norm of grad vector is  80863.966702551\n",
      "Loss decreases to  75.93071115943695\n",
      "The norm of grad vector is  78868.42973108287\n",
      "Loss decreases to  75.90348679386852\n",
      "The norm of grad vector is  76963.82738932804\n",
      "Loss decreases to  75.8764371551857\n",
      "The norm of grad vector is  75146.77396156648\n",
      "Loss decreases to  75.84955696197333\n",
      "The norm of grad vector is  73413.97224748247\n",
      "Loss decreases to  75.82284125686876\n",
      "The norm of grad vector is  71762.21036389726\n",
      "Loss decreases to  75.79628538407329\n",
      "The norm of grad vector is  70188.35890962098\n",
      "Loss decreases to  75.76988496844989\n",
      "The norm of grad vector is  68689.36848539668\n",
      "Loss decreases to  75.74363589609493\n",
      "The norm of grad vector is  67262.2675570348\n",
      "Loss decreases to  75.71753429627884\n",
      "The norm of grad vector is  65904.16064593224\n",
      "Loss decreases to  75.69157652466009\n",
      "The norm of grad vector is  64612.226827343475\n",
      "Loss decreases to  75.6657591476826\n",
      "The norm of grad vector is  63383.71851313369\n",
      "Loss decreases to  75.64007892807152\n",
      "The norm of grad vector is  62215.96049247754\n",
      "Loss decreases to  75.61453281135087\n",
      "The norm of grad vector is  61106.34920114013\n",
      "Loss decreases to  75.58911791331155\n",
      "The norm of grad vector is  60052.35218770727\n",
      "Loss decreases to  75.56383150836056\n",
      "The norm of grad vector is  59051.50774352159\n",
      "Loss decreases to  75.53867101869\n",
      "The norm of grad vector is  58101.42466214988\n",
      "Loss decreases to  75.51363400421037\n",
      "The norm of grad vector is  57199.782094018155\n",
      "Loss decreases to  75.48871815318876\n",
      "The norm of grad vector is  56344.32946238273\n",
      "Loss decreases to  75.46392127354866\n",
      "The norm of grad vector is  55532.88640805211\n",
      "Loss decreases to  75.43924128477873\n",
      "The norm of grad vector is  54763.34273214365\n",
      "Loss decreases to  75.41467621041095\n",
      "The norm of grad vector is  54033.65830862881\n",
      "Loss decreases to  75.39022417102842\n",
      "The norm of grad vector is  53341.862941349216\n",
      "Loss decreases to  75.3658833777623\n",
      "The norm of grad vector is  52686.05614347594\n",
      "Loss decreases to  75.34165212624565\n",
      "The norm of grad vector is  52064.40682095867\n",
      "Loss decreases to  75.31752879099184\n",
      "The norm of grad vector is  51475.15284520359\n",
      "Loss decreases to  75.29351182016723\n",
      "The norm of grad vector is  50916.60050393378\n",
      "Loss decreases to  75.26959973072849\n",
      "The norm of grad vector is  50387.1238228449\n",
      "Loss decreases to  75.24579110390304\n",
      "The norm of grad vector is  49885.16375415976\n",
      "Loss decreases to  75.22208458098387\n",
      "The norm of grad vector is  49409.22723140666\n",
      "Loss decreases to  75.19847885941864\n",
      "The norm of grad vector is  48957.886092696695\n",
      "Loss decreases to  75.17497268917309\n",
      "The norm of grad vector is  48529.775877357046\n",
      "Loss decreases to  75.15156486934796\n",
      "The norm of grad vector is  48123.59450298764\n",
      "Loss decreases to  75.12825424503171\n",
      "The norm of grad vector is  47738.100831825424\n",
      "Loss decreases to  75.10503970437233\n",
      "The norm of grad vector is  47372.11313673937\n",
      "Loss decreases to  75.08192017585475\n",
      "The norm of grad vector is  47024.50747822925\n",
      "Loss decreases to  75.0588946257663\n",
      "The norm of grad vector is  46694.21600451452\n",
      "Loss decreases to  75.03596205583835\n",
      "The norm of grad vector is  46380.225187166216\n",
      "Loss decreases to  75.01312150105272\n",
      "The norm of grad vector is  46081.57400484084\n",
      "Loss decreases to  74.99037202759953\n",
      "The norm of grad vector is  45797.35208750143\n",
      "Loss decreases to  74.96771273097677\n",
      "The norm of grad vector is  45526.697833153106\n",
      "Loss decreases to  74.94514273422133\n",
      "The norm of grad vector is  45268.796508559004\n",
      "Loss decreases to  74.9226611862623\n",
      "The norm of grad vector is  45022.87834473477\n",
      "Loss decreases to  74.90026726038765\n",
      "The norm of grad vector is  44788.2166372218\n",
      "Loss decreases to  74.87796015281721\n",
      "The norm of grad vector is  44564.125860295244\n",
      "Loss decreases to  74.85573908137304\n",
      "The norm of grad vector is  44349.95980336043\n",
      "Loss decreases to  74.83360328424122\n",
      "The norm of grad vector is  44145.109736875915\n",
      "Loss decreases to  74.81155201881963\n",
      "The norm of grad vector is  43949.002614235374\n",
      "Loss decreases to  74.78958456064194\n",
      "The norm of grad vector is  43761.099315140425\n",
      "Loss decreases to  74.7677002023784\n",
      "The norm of grad vector is  43580.8929351452\n",
      "Loss decreases to  74.74589825290158\n",
      "The norm of grad vector is  43407.90712523541\n",
      "Loss decreases to  74.72417803641773\n",
      "The norm of grad vector is  43241.694484544016\n",
      "Loss decreases to  74.70253889165593\n",
      "The norm of grad vector is  43081.83500859644\n",
      "Loss decreases to  74.68098017111218\n",
      "The norm of grad vector is  42927.93459483573\n",
      "Loss decreases to  74.65950124034464\n",
      "The norm of grad vector is  42779.62360659301\n",
      "Loss decreases to  74.6381014773161\n",
      "The norm of grad vector is  42636.55549614124\n",
      "Loss decreases to  74.61678027178078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  42498.40548700818\n",
      "Loss decreases to  74.59553702471088\n",
      "The norm of grad vector is  42364.869315318276\n",
      "Loss decreases to  74.5743711477633\n",
      "The norm of grad vector is  42235.66202957659\n",
      "Loss decreases to  74.55328206277963\n",
      "The norm of grad vector is  42110.516848011066\n",
      "Loss decreases to  74.53226920132013\n",
      "The norm of grad vector is  41989.184072329146\n",
      "Loss decreases to  74.5113320042284\n",
      "The norm of grad vector is  41871.43005653569\n",
      "Loss decreases to  74.4904699212246\n",
      "The norm of grad vector is  41757.03622929012\n",
      "Loss decreases to  74.469682410524\n",
      "The norm of grad vector is  41645.79816813711\n",
      "Loss decreases to  74.44896893848158\n",
      "The norm of grad vector is  41537.52472384689\n",
      "Loss decreases to  74.42832897925915\n",
      "The norm of grad vector is  41432.03719302028\n",
      "Loss decreases to  74.40776201451295\n",
      "The norm of grad vector is  41329.16853706515\n",
      "Loss decreases to  74.38726753310274\n",
      "The norm of grad vector is  41228.76264561571\n",
      "Loss decreases to  74.36684503081705\n",
      "The norm of grad vector is  41130.673642459646\n",
      "Loss decreases to  74.3464940101183\n",
      "The norm of grad vector is  41034.76523203817\n",
      "Loss decreases to  74.32621397990158\n",
      "The norm of grad vector is  40940.9100846044\n",
      "Loss decreases to  74.30600445526932\n",
      "The norm of grad vector is  40848.98925815349\n",
      "Loss decreases to  74.2858649573203\n",
      "The norm of grad vector is  40758.89165527492\n",
      "Loss decreases to  74.26579501294987\n",
      "The norm of grad vector is  40670.513513126556\n",
      "Loss decreases to  74.24579415466432\n",
      "The norm of grad vector is  40583.75792477859\n",
      "Loss decreases to  74.22586192040478\n",
      "The norm of grad vector is  40498.53439023477\n",
      "Loss decreases to  74.20599785338202\n",
      "The norm of grad vector is  40414.758395496196\n",
      "Loss decreases to  74.18620150192103\n",
      "The norm of grad vector is  40332.35101809712\n",
      "Loss decreases to  74.16647241931474\n",
      "The norm of grad vector is  40251.23855760637\n",
      "Loss decreases to  74.14681016368544\n",
      "The norm of grad vector is  40171.352189651036\n",
      "Loss decreases to  74.12721429785506\n",
      "The norm of grad vector is  40092.62764208647\n",
      "Loss decreases to  74.10768438922204\n",
      "The norm of grad vector is  40015.00489199994\n",
      "Loss decreases to  74.08822000964494\n",
      "The norm of grad vector is  39938.427882299184\n",
      "Loss decreases to  74.06882073533264\n",
      "The norm of grad vector is  39862.844256700664\n",
      "Loss decreases to  74.04948614674056\n",
      "The norm of grad vector is  39788.20511199289\n",
      "Loss decreases to  74.03021582847236\n",
      "The norm of grad vector is  39714.46476650958\n",
      "Loss decreases to  74.01100936918587\n",
      "The norm of grad vector is  39641.58054380547\n",
      "Loss decreases to  73.99186636150557\n",
      "The norm of grad vector is  39569.51257058323\n",
      "Loss decreases to  73.9727864019375\n",
      "The norm of grad vector is  39498.22358797289\n",
      "Loss decreases to  73.95376909079026\n",
      "The norm of grad vector is  39427.678775316985\n",
      "Loss decreases to  73.9348140320983\n",
      "The norm of grad vector is  39357.84558566296\n",
      "Loss decreases to  73.91592083354995\n",
      "The norm of grad vector is  39288.693592213145\n",
      "Loss decreases to  73.89708910641822\n",
      "The norm of grad vector is  39220.194345024334\n",
      "Loss decreases to  73.8783184654952\n",
      "The norm of grad vector is  39152.32123729432\n",
      "Loss decreases to  73.85960852902897\n",
      "The norm of grad vector is  39085.04938061135\n",
      "Loss decreases to  73.84095891866323\n",
      "The norm of grad vector is  39018.355488580266\n",
      "Loss decreases to  73.82236925938098\n",
      "The norm of grad vector is  38952.21776827829\n",
      "Loss decreases to  73.80383917944762\n",
      "The norm of grad vector is  38886.61581902363\n",
      "Loss decreases to  73.7853683103599\n",
      "The norm of grad vector is  38821.530537975\n",
      "Loss decreases to  73.766956286794\n",
      "The norm of grad vector is  38756.9440321099\n",
      "Loss decreases to  73.74860274655741\n",
      "The norm of grad vector is  38692.83953615876\n",
      "Loss decreases to  73.73030733054199\n",
      "The norm of grad vector is  38629.2013360978\n",
      "Loss decreases to  73.71206968267886\n",
      "The norm of grad vector is  38566.01469783108\n",
      "Loss decreases to  73.69388944989461\n",
      "The norm of grad vector is  38503.26580071315\n",
      "Loss decreases to  73.67576628207006\n",
      "The norm of grad vector is  38440.94167558997\n",
      "Loss decreases to  73.65769983199941\n",
      "The norm of grad vector is  38379.030147053156\n",
      "Loss decreases to  73.63968975535057\n",
      "The norm of grad vector is  38317.519779625814\n",
      "Loss decreases to  73.62173571062878\n",
      "The norm of grad vector is  38256.399827613684\n",
      "Loss decreases to  73.60383735913831\n",
      "The norm of grad vector is  38195.66018837553\n",
      "Loss decreases to  73.5859943649478\n",
      "The norm of grad vector is  38135.291358781134\n",
      "Loss decreases to  73.56820639485623\n",
      "The norm of grad vector is  38075.28439464141\n",
      "Loss decreases to  73.5504731183583\n",
      "The norm of grad vector is  38015.63087290963\n",
      "Loss decreases to  73.53279420761281\n",
      "The norm of grad vector is  37956.322856464874\n",
      "Loss decreases to  73.51516933741098\n",
      "The norm of grad vector is  37897.35286130417\n",
      "Loss decreases to  73.49759818514522\n",
      "The norm of grad vector is  37838.71382597752\n",
      "Loss decreases to  73.48008043077975\n",
      "The norm of grad vector is  37780.39908311471\n",
      "Loss decreases to  73.46261575682075\n",
      "The norm of grad vector is  37722.402332901154\n",
      "Loss decreases to  73.44520384828829\n",
      "The norm of grad vector is  37664.71761836942\n",
      "Loss decreases to  73.42784439268861\n",
      "The norm of grad vector is  37607.339302383116\n",
      "Loss decreases to  73.4105370799865\n",
      "The norm of grad vector is  37550.2620461977\n",
      "Loss decreases to  73.39328160257885\n",
      "The norm of grad vector is  37493.48078949016\n",
      "Loss decreases to  73.37607765526909\n",
      "The norm of grad vector is  37436.99073175715\n",
      "Loss decreases to  73.35892493524098\n",
      "The norm of grad vector is  37380.787314988316\n",
      "Loss decreases to  73.3418231420342\n",
      "The norm of grad vector is  37324.8662075274\n",
      "Loss decreases to  73.32477197752013\n",
      "The norm of grad vector is  37269.22328903965\n",
      "Loss decreases to  73.30777114587711\n",
      "The norm of grad vector is  37213.85463651002\n",
      "Loss decreases to  73.29082035356763\n",
      "The norm of grad vector is  37158.75651120128\n",
      "Loss decreases to  73.27391930931513\n",
      "The norm of grad vector is  37103.92534650639\n",
      "Loss decreases to  73.25706772408097\n",
      "The norm of grad vector is  37049.35773663372\n",
      "Loss decreases to  73.24026531104272\n",
      "The norm of grad vector is  36995.050426067886\n",
      "Loss decreases to  73.22351178557196\n",
      "The norm of grad vector is  36941.00029975313\n",
      "Loss decreases to  73.20680686521291\n",
      "The norm of grad vector is  36887.2043739495\n",
      "Loss decreases to  73.19015026966095\n",
      "The norm of grad vector is  36833.65978771579\n",
      "Loss decreases to  73.17354172074263\n",
      "The norm of grad vector is  36780.36379497576\n",
      "Loss decreases to  73.15698094239394\n",
      "The norm of grad vector is  36727.31375712813\n",
      "Loss decreases to  73.14046766064126\n",
      "The norm of grad vector is  36674.507136162574\n",
      "Loss decreases to  73.12400160358085\n",
      "The norm of grad vector is  36621.94148824684\n",
      "Loss decreases to  73.10758250135954\n",
      "The norm of grad vector is  36569.61445775297\n",
      "Loss decreases to  73.09121008615503\n",
      "The norm of grad vector is  36517.5237716922\n",
      "Loss decreases to  73.07488409215753\n",
      "The norm of grad vector is  36465.66723453017\n",
      "Loss decreases to  73.05860425555049\n",
      "The norm of grad vector is  36414.04272335684\n",
      "Loss decreases to  73.04237031449215\n",
      "The norm of grad vector is  36362.64818338632\n",
      "Loss decreases to  73.02618200909733\n",
      "The norm of grad vector is  36311.48162376353\n",
      "Loss decreases to  73.01003908141917\n",
      "The norm of grad vector is  36260.541113657695\n",
      "Loss decreases to  72.99394127543198\n",
      "The norm of grad vector is  36209.82477862173\n",
      "Loss decreases to  72.97788833701294\n",
      "The norm of grad vector is  36159.330797199786\n",
      "Loss decreases to  72.9618800139251\n",
      "The norm of grad vector is  36109.05739776594\n",
      "Loss decreases to  72.94591605580008\n",
      "The norm of grad vector is  36059.00285557754\n",
      "Loss decreases to  72.9299962141215\n",
      "The norm of grad vector is  36009.16549002934\n",
      "Loss decreases to  72.91412024220774\n",
      "The norm of grad vector is  35959.5436620939\n",
      "Loss decreases to  72.89828789519585\n",
      "The norm of grad vector is  35910.135771934976\n",
      "Loss decreases to  72.88249893002504\n",
      "The norm of grad vector is  35860.9402566837\n",
      "Loss decreases to  72.86675310542063\n",
      "The norm of grad vector is  35811.95558836425\n",
      "Loss decreases to  72.85105018187758\n",
      "The norm of grad vector is  35763.180271960184\n",
      "Loss decreases to  72.83538992164554\n",
      "The norm of grad vector is  35714.61284361129\n",
      "Loss decreases to  72.81977208871295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  35666.25186893119\n",
      "Loss decreases to  72.80419644879129\n",
      "The norm of grad vector is  35618.09594143942\n",
      "Loss decreases to  72.78866276929983\n",
      "The norm of grad vector is  35570.14368109731\n",
      "Loss decreases to  72.77317081935135\n",
      "The norm of grad vector is  35522.393732943\n",
      "Loss decreases to  72.75772036973622\n",
      "The norm of grad vector is  35474.84476581755\n",
      "Loss decreases to  72.74231119290808\n",
      "The norm of grad vector is  35427.49547117553\n",
      "Loss decreases to  72.72694306296916\n",
      "The norm of grad vector is  35380.34456197547\n",
      "Loss decreases to  72.71161575565645\n",
      "The norm of grad vector is  35333.39077164374\n",
      "Loss decreases to  72.69632904832608\n",
      "The norm of grad vector is  35286.63285310693\n",
      "Loss decreases to  72.68108271994029\n",
      "The norm of grad vector is  35240.06957788826\n",
      "Loss decreases to  72.66587655105303\n",
      "The norm of grad vector is  35193.699735263704\n",
      "Loss decreases to  72.65071032379632\n",
      "The norm of grad vector is  35147.52213147301\n",
      "Loss decreases to  72.63558382186605\n",
      "The norm of grad vector is  35101.535588982784\n",
      "Loss decreases to  72.62049683050896\n",
      "The norm of grad vector is  35055.73894579764\n",
      "Loss decreases to  72.60544913650887\n",
      "The norm of grad vector is  35010.13105481563\n",
      "Loss decreases to  72.59044052817359\n",
      "The norm of grad vector is  34964.71078322582\n",
      "Loss decreases to  72.57547079532164\n",
      "The norm of grad vector is  34919.47701194438\n",
      "Loss decreases to  72.56053972926922\n",
      "The norm of grad vector is  34874.42863508703\n",
      "Loss decreases to  72.54564712281746\n",
      "The norm of grad vector is  34829.56455947486\n",
      "Loss decreases to  72.53079277023922\n",
      "The norm of grad vector is  34784.8837041719\n",
      "Loss decreases to  72.515976467267\n",
      "The norm of grad vector is  34740.385000051305\n",
      "Loss decreases to  72.5011980110803\n",
      "The norm of grad vector is  34696.06738938933\n",
      "Loss decreases to  72.48645720029292\n",
      "The norm of grad vector is  34651.929825484294\n",
      "Loss decreases to  72.47175383494094\n",
      "The norm of grad vector is  34607.971272299\n",
      "Loss decreases to  72.45708771647067\n",
      "The norm of grad vector is  34564.190704125635\n",
      "Loss decreases to  72.44245864772613\n",
      "The norm of grad vector is  34520.58710527065\n",
      "Loss decreases to  72.4278664329379\n",
      "The norm of grad vector is  34477.15946975878\n",
      "Loss decreases to  72.41331087771066\n",
      "The norm of grad vector is  34433.90680105518\n",
      "Loss decreases to  72.39879178901182\n",
      "The norm of grad vector is  34390.828111803676\n",
      "Loss decreases to  72.38430897515985\n",
      "The norm of grad vector is  34347.92242358061\n",
      "Loss decreases to  72.36986224581293\n",
      "The norm of grad vector is  34305.18876666317\n",
      "Loss decreases to  72.35545141195746\n",
      "The norm of grad vector is  34262.62617981116\n",
      "Loss decreases to  72.34107628589732\n",
      "The norm of grad vector is  34220.23371006058\n",
      "Loss decreases to  72.32673668124151\n",
      "The norm of grad vector is  34178.01041252993\n",
      "Loss decreases to  72.31243241289467\n",
      "The norm of grad vector is  34135.95535023654\n",
      "Loss decreases to  72.29816329704516\n",
      "The norm of grad vector is  34094.06759392317\n",
      "Loss decreases to  72.28392915115475\n",
      "The norm of grad vector is  34052.34622189426\n",
      "Loss decreases to  72.26972979394743\n",
      "The norm of grad vector is  34010.79031986077\n",
      "Loss decreases to  72.25556504539944\n",
      "The norm of grad vector is  33969.39898079327\n",
      "Loss decreases to  72.24143472672785\n",
      "The norm of grad vector is  33928.171304782656\n",
      "Loss decreases to  72.22733866038142\n",
      "The norm of grad vector is  33887.10639890809\n",
      "Loss decreases to  72.21327667002902\n",
      "The norm of grad vector is  33846.203377111415\n",
      "Loss decreases to  72.19924858055064\n",
      "The norm of grad vector is  33805.46136007774\n",
      "Loss decreases to  72.1852542180256\n",
      "The norm of grad vector is  33764.87947512237\n",
      "Loss decreases to  72.17129340972478\n",
      "The norm of grad vector is  33724.45685608218\n",
      "Loss decreases to  72.15736598409882\n",
      "The norm of grad vector is  33684.19264321298\n",
      "Loss decreases to  72.14347177076914\n",
      "The norm of grad vector is  33644.085983091\n",
      "Loss decreases to  72.1296106005179\n",
      "The norm of grad vector is  33604.136028518806\n",
      "Loss decreases to  72.11578230527894\n",
      "The norm of grad vector is  33564.34193843584\n",
      "Loss decreases to  72.10198671812734\n",
      "The norm of grad vector is  33524.702877832206\n",
      "Loss decreases to  72.08822367327055\n",
      "The norm of grad vector is  33485.21801766621\n",
      "Loss decreases to  72.0744930060391\n",
      "The norm of grad vector is  33445.886534785415\n",
      "Loss decreases to  72.06079455287711\n",
      "The norm of grad vector is  33406.7076118507\n",
      "Loss decreases to  72.04712815133256\n",
      "The norm of grad vector is  33367.6804372633\n",
      "Loss decreases to  72.03349364004941\n",
      "The norm of grad vector is  33328.80420509474\n",
      "Loss decreases to  72.01989085875755\n",
      "The norm of grad vector is  33290.07811501873\n",
      "Loss decreases to  72.00631964826422\n",
      "The norm of grad vector is  33251.50137224647\n",
      "Loss decreases to  71.99277985044513\n",
      "The norm of grad vector is  33213.07318746321\n",
      "Loss decreases to  71.97927130823564\n",
      "The norm of grad vector is  33174.79277676763\n",
      "Loss decreases to  71.96579386562237\n",
      "The norm of grad vector is  33136.65936161282\n",
      "Loss decreases to  71.95234736763412\n",
      "The norm of grad vector is  33098.67216874932\n",
      "Loss decreases to  71.93893166033351\n",
      "The norm of grad vector is  33060.83043016971\n",
      "Loss decreases to  71.92554659080866\n",
      "The norm of grad vector is  33023.133383055036\n",
      "Loss decreases to  71.91219200716472\n",
      "The norm of grad vector is  32985.5802697226\n",
      "Loss decreases to  71.8988677585157\n",
      "The norm of grad vector is  32948.17033757541\n",
      "Loss decreases to  71.88557369497602\n",
      "The norm of grad vector is  32910.902839052746\n",
      "Loss decreases to  71.87230966765266\n",
      "The norm of grad vector is  32873.77703158206\n",
      "Loss decreases to  71.85907552863645\n",
      "The norm of grad vector is  32836.79217753234\n",
      "Loss decreases to  71.84587113099518\n",
      "The norm of grad vector is  32799.94754416825\n",
      "Loss decreases to  71.83269632876474\n",
      "The norm of grad vector is  32763.24240360543\n",
      "Loss decreases to  71.81955097694139\n",
      "The norm of grad vector is  32726.676032767078\n",
      "Loss decreases to  71.80643493147475\n",
      "The norm of grad vector is  32690.247713341127\n",
      "Loss decreases to  71.79334804925895\n",
      "The norm of grad vector is  32653.956731738494\n",
      "Loss decreases to  71.78029018812612\n",
      "The norm of grad vector is  32617.802379052067\n",
      "Loss decreases to  71.76726120683803\n",
      "The norm of grad vector is  32581.783951016892\n",
      "Loss decreases to  71.75426096507879\n",
      "The norm of grad vector is  32545.90074797036\n",
      "Loss decreases to  71.74128932344766\n",
      "The norm of grad vector is  32510.152074814006\n",
      "Loss decreases to  71.72834614345157\n",
      "The norm of grad vector is  32474.53724097533\n",
      "Loss decreases to  71.71543128749785\n",
      "The norm of grad vector is  32439.05556037058\n",
      "Loss decreases to  71.70254461888682\n",
      "The norm of grad vector is  32403.706351368044\n",
      "Loss decreases to  71.68968600180506\n",
      "The norm of grad vector is  32368.48893675188\n",
      "Loss decreases to  71.67685530131777\n",
      "The norm of grad vector is  32333.402643686968\n",
      "Loss decreases to  71.66405238336229\n",
      "The norm of grad vector is  32298.446803683426\n",
      "Loss decreases to  71.65127711474094\n",
      "The norm of grad vector is  32263.620752562492\n",
      "Loss decreases to  71.63852936311383\n",
      "The norm of grad vector is  32228.92383042243\n",
      "Loss decreases to  71.62580899699257\n",
      "The norm of grad vector is  32194.35538160505\n",
      "Loss decreases to  71.61311588573295\n",
      "The norm of grad vector is  32159.91475466259\n",
      "Loss decreases to  71.6004498995289\n",
      "The norm of grad vector is  32125.601302325125\n",
      "Loss decreases to  71.58781090940508\n",
      "The norm of grad vector is  32091.41438146838\n",
      "Loss decreases to  71.57519878721078\n",
      "The norm of grad vector is  32057.353353081995\n",
      "Loss decreases to  71.56261340561343\n",
      "The norm of grad vector is  32023.417582237733\n",
      "Loss decreases to  71.55005463809196\n",
      "The norm of grad vector is  31989.606438058876\n",
      "Loss decreases to  71.53752235893036\n",
      "The norm of grad vector is  31955.919293689138\n",
      "Loss decreases to  71.52501644321133\n",
      "The norm of grad vector is  31922.35552626266\n",
      "Loss decreases to  71.51253676681029\n",
      "The norm of grad vector is  31888.914516873483\n",
      "Loss decreases to  71.50008320638881\n",
      "The norm of grad vector is  31855.595650546384\n",
      "Loss decreases to  71.48765563938854\n",
      "The norm of grad vector is  31822.39831620699\n",
      "Loss decreases to  71.47525394402504\n",
      "The norm of grad vector is  31789.321906652945\n",
      "Loss decreases to  71.46287799928203\n",
      "The norm of grad vector is  31756.365818525068\n",
      "Loss decreases to  71.45052768490483\n",
      "The norm of grad vector is  31723.529452278704\n",
      "Loss decreases to  71.43820288139484\n",
      "The norm of grad vector is  31690.812212155568\n",
      "Loss decreases to  71.42590347000362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  31658.213506155767\n",
      "Loss decreases to  71.41362933272659\n",
      "The norm of grad vector is  31625.732746009926\n",
      "Loss decreases to  71.40138035229798\n",
      "The norm of grad vector is  31593.369347152067\n",
      "Loss decreases to  71.3891564121844\n",
      "The norm of grad vector is  31561.122728691822\n",
      "Loss decreases to  71.37695739657961\n",
      "The norm of grad vector is  31528.99231338809\n",
      "Loss decreases to  71.36478319039844\n",
      "The norm of grad vector is  31496.977527621788\n",
      "Loss decreases to  71.35263367927186\n",
      "The norm of grad vector is  31465.077801369644\n",
      "Loss decreases to  71.34050874954046\n",
      "The norm of grad vector is  31433.29256817783\n",
      "Loss decreases to  71.32840828825027\n",
      "The norm of grad vector is  31401.621265135916\n",
      "Loss decreases to  71.3163321831461\n",
      "The norm of grad vector is  31370.063332850885\n",
      "Loss decreases to  71.30428032266654\n",
      "The norm of grad vector is  31338.618215421946\n",
      "Loss decreases to  71.29225259593927\n",
      "The norm of grad vector is  31307.285360414524\n",
      "Loss decreases to  71.28024889277484\n",
      "The norm of grad vector is  31276.064218835545\n",
      "Loss decreases to  71.26826910366155\n",
      "The norm of grad vector is  31244.954245108223\n",
      "Loss decreases to  71.25631311976117\n",
      "The norm of grad vector is  31213.954897047373\n",
      "Loss decreases to  71.24438083290262\n",
      "The norm of grad vector is  31183.06563583462\n",
      "Loss decreases to  71.23247213557741\n",
      "The norm of grad vector is  31152.285925994333\n",
      "Loss decreases to  71.22058692093462\n",
      "The norm of grad vector is  31121.615235368994\n",
      "Loss decreases to  71.20872508277593\n",
      "The norm of grad vector is  31091.05303509567\n",
      "Loss decreases to  71.19688651555028\n",
      "The norm of grad vector is  31060.59879958171\n",
      "Loss decreases to  71.18507111434926\n",
      "The norm of grad vector is  31030.252006481438\n",
      "Loss decreases to  71.17327877490202\n",
      "The norm of grad vector is  31000.01213667254\n",
      "Loss decreases to  71.16150939357075\n",
      "The norm of grad vector is  30969.878674232692\n",
      "Loss decreases to  71.14976286734567\n",
      "The norm of grad vector is  30939.851106416798\n",
      "Loss decreases to  71.13803909383996\n",
      "The norm of grad vector is  30909.92892363374\n",
      "Loss decreases to  71.12633797128588\n",
      "The norm of grad vector is  30880.111619423635\n",
      "Loss decreases to  71.11465939852897\n",
      "The norm of grad vector is  30850.398690435555\n",
      "Loss decreases to  71.10300327502465\n",
      "The norm of grad vector is  30820.789636404745\n",
      "Loss decreases to  71.09136950083274\n",
      "The norm of grad vector is  30791.28396013057\n",
      "Loss decreases to  71.07975797661314\n",
      "The norm of grad vector is  30761.881167454558\n",
      "Loss decreases to  71.06816860362146\n",
      "The norm of grad vector is  30732.580767238225\n",
      "Loss decreases to  71.05660128370455\n",
      "The norm of grad vector is  30703.38227134151\n",
      "Loss decreases to  71.04505591929572\n",
      "The norm of grad vector is  30674.285194601125\n",
      "Loss decreases to  71.03353241341081\n",
      "The norm of grad vector is  30645.28905480923\n",
      "Loss decreases to  71.02203066964367\n",
      "The norm of grad vector is  30616.393372692015\n",
      "Loss decreases to  71.01055059216189\n",
      "The norm of grad vector is  30587.59767188879\n",
      "Loss decreases to  70.99909208570234\n",
      "The norm of grad vector is  30558.901478930857\n",
      "Loss decreases to  70.98765505556693\n",
      "The norm of grad vector is  30530.304323220895\n",
      "Loss decreases to  70.97623940761879\n",
      "The norm of grad vector is  30501.805737012055\n",
      "Loss decreases to  70.96484504827758\n",
      "The norm of grad vector is  30473.405255387876\n",
      "Loss decreases to  70.95347188451582\n",
      "The norm of grad vector is  30445.1024162416\n",
      "Loss decreases to  70.94211982385472\n",
      "The norm of grad vector is  30416.89676025616\n",
      "Loss decreases to  70.93078877435968\n",
      "The norm of grad vector is  30388.787830884095\n",
      "Loss decreases to  70.91947864463694\n",
      "The norm of grad vector is  30360.775174327686\n",
      "Loss decreases to  70.9081893438289\n",
      "The norm of grad vector is  30332.858339519236\n",
      "Loss decreases to  70.89692078161089\n",
      "The norm of grad vector is  30305.036878101517\n",
      "Loss decreases to  70.88567286818684\n",
      "The norm of grad vector is  30277.310344408208\n",
      "Loss decreases to  70.87444551428518\n",
      "The norm of grad vector is  30249.678295444664\n",
      "Loss decreases to  70.86323863115564\n",
      "The norm of grad vector is  30222.14029086878\n",
      "Loss decreases to  70.85205213056457\n",
      "The norm of grad vector is  30194.69589297195\n",
      "Loss decreases to  70.84088592479229\n",
      "The norm of grad vector is  30167.34466666018\n",
      "Loss decreases to  70.82973992662804\n",
      "The norm of grad vector is  30140.08617943527\n",
      "Loss decreases to  70.81861404936767\n",
      "The norm of grad vector is  30112.920001376508\n",
      "Loss decreases to  70.80750820680842\n",
      "The norm of grad vector is  30085.84570512187\n",
      "Loss decreases to  70.79642231324681\n",
      "The norm of grad vector is  30058.862865849784\n",
      "Loss decreases to  70.78535628347355\n",
      "The norm of grad vector is  30031.971061261174\n",
      "Loss decreases to  70.77431003277111\n",
      "The norm of grad vector is  30005.169871561106\n",
      "Loss decreases to  70.76328347690995\n",
      "The norm of grad vector is  29978.458879440997\n",
      "Loss decreases to  70.75227653214408\n",
      "The norm of grad vector is  29951.837670060762\n",
      "Loss decreases to  70.74128911520901\n",
      "The norm of grad vector is  29925.305831031426\n",
      "Loss decreases to  70.73032114331691\n",
      "The norm of grad vector is  29898.862952397125\n",
      "Loss decreases to  70.719372534154\n",
      "The norm of grad vector is  29872.50862661822\n",
      "Loss decreases to  70.70844320587723\n",
      "The norm of grad vector is  29846.2424485536\n",
      "Loss decreases to  70.69753307711\n",
      "The norm of grad vector is  29820.06401544387\n",
      "Loss decreases to  70.68664206694024\n",
      "The norm of grad vector is  29793.97292689418\n",
      "Loss decreases to  70.67577009491528\n",
      "The norm of grad vector is  29767.968784857338\n",
      "Loss decreases to  70.66491708104041\n",
      "The norm of grad vector is  29742.051193617175\n",
      "Loss decreases to  70.65408294577412\n",
      "The norm of grad vector is  29716.219759771702\n",
      "Loss decreases to  70.643267610026\n",
      "The norm of grad vector is  29690.474092216893\n",
      "Loss decreases to  70.63247099515264\n",
      "The norm of grad vector is  29664.81380212997\n",
      "Loss decreases to  70.62169302295486\n",
      "The norm of grad vector is  29639.238502953547\n",
      "Loss decreases to  70.61093361567495\n",
      "The norm of grad vector is  29613.747810379045\n",
      "Loss decreases to  70.60019269599255\n",
      "The norm of grad vector is  29588.341342330918\n",
      "Loss decreases to  70.58947018702253\n",
      "The norm of grad vector is  29563.018718950832\n",
      "Loss decreases to  70.57876601231125\n",
      "The norm of grad vector is  29537.77956258172\n",
      "Loss decreases to  70.56808009583399\n",
      "The norm of grad vector is  29512.623497752087\n",
      "Loss decreases to  70.55741236199147\n",
      "The norm of grad vector is  29487.550151160736\n",
      "Loss decreases to  70.54676273560735\n",
      "The norm of grad vector is  29462.55915166101\n",
      "Loss decreases to  70.53613114192477\n",
      "The norm of grad vector is  29437.6501302456\n",
      "Loss decreases to  70.52551750660369\n",
      "The norm of grad vector is  29412.82272003155\n",
      "Loss decreases to  70.51492175571809\n",
      "The norm of grad vector is  29388.07655624495\n",
      "Loss decreases to  70.50434381575256\n",
      "The norm of grad vector is  29363.41127620596\n",
      "Loss decreases to  70.49378361360021\n",
      "The norm of grad vector is  29338.82651931392\n",
      "Loss decreases to  70.48324107655885\n",
      "The norm of grad vector is  29314.321927032972\n",
      "Loss decreases to  70.47271613232898\n",
      "The norm of grad vector is  29289.897142876973\n",
      "Loss decreases to  70.46220870901075\n",
      "The norm of grad vector is  29265.551812395188\n",
      "Loss decreases to  70.45171873510117\n",
      "The norm of grad vector is  29241.285583157776\n",
      "Loss decreases to  70.44124613949094\n",
      "The norm of grad vector is  29217.098104741737\n",
      "Loss decreases to  70.43079085146252\n",
      "The norm of grad vector is  29192.989028716387\n",
      "Loss decreases to  70.42035280068669\n",
      "The norm of grad vector is  29168.958008629434\n",
      "Loss decreases to  70.40993191722056\n",
      "The norm of grad vector is  29145.004699993173\n",
      "Loss decreases to  70.39952813150396\n",
      "The norm of grad vector is  29121.12876027008\n",
      "Loss decreases to  70.3891413743579\n",
      "The norm of grad vector is  29097.329848859634\n",
      "Loss decreases to  70.3787715769812\n",
      "The norm of grad vector is  29073.60762708434\n",
      "Loss decreases to  70.36841867094797\n",
      "The norm of grad vector is  29049.96175817627\n",
      "Loss decreases to  70.35808258820538\n",
      "The norm of grad vector is  29026.391907263438\n",
      "Loss decreases to  70.34776326107074\n",
      "The norm of grad vector is  29002.897741356755\n",
      "Loss decreases to  70.33746062222916\n",
      "The norm of grad vector is  28979.478929336357\n",
      "Loss decreases to  70.32717460473121\n",
      "The norm of grad vector is  28956.135141938827\n",
      "Loss decreases to  70.31690514198986\n",
      "The norm of grad vector is  28932.866051744077\n",
      "Loss decreases to  70.30665216777861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  28909.671333162063\n",
      "Loss decreases to  70.29641561622896\n",
      "The norm of grad vector is  28886.550662420505\n",
      "Loss decreases to  70.28619542182712\n",
      "The norm of grad vector is  28863.503717551543\n",
      "Loss decreases to  70.2759915194133\n",
      "The norm of grad vector is  28840.53017837942\n",
      "Loss decreases to  70.26580384417758\n",
      "The norm of grad vector is  28817.62972650785\n",
      "Loss decreases to  70.25563233165843\n",
      "The norm of grad vector is  28794.80204530756\n",
      "Loss decreases to  70.24547691774039\n",
      "The norm of grad vector is  28772.046819903873\n",
      "Loss decreases to  70.23533753865149\n",
      "The norm of grad vector is  28749.363737164458\n",
      "Loss decreases to  70.22521413096096\n",
      "The norm of grad vector is  28726.752485687153\n",
      "Loss decreases to  70.21510663157703\n",
      "The norm of grad vector is  28704.212755787874\n",
      "Loss decreases to  70.2050149777446\n",
      "The norm of grad vector is  28681.744239488587\n",
      "Loss decreases to  70.19493910704284\n",
      "The norm of grad vector is  28659.346630505537\n",
      "Loss decreases to  70.18487895738322\n",
      "The norm of grad vector is  28637.019624237084\n",
      "Loss decreases to  70.17483446700747\n",
      "The norm of grad vector is  28614.76291775254\n",
      "Loss decreases to  70.16480557448457\n",
      "The norm of grad vector is  28592.57620977993\n",
      "Loss decreases to  70.15479221870952\n",
      "The norm of grad vector is  28570.459200694964\n",
      "Loss decreases to  70.14479433890041\n",
      "The norm of grad vector is  28548.411592509256\n",
      "Loss decreases to  70.13481187459679\n",
      "The norm of grad vector is  28526.433088859005\n",
      "Loss decreases to  70.12484476565737\n",
      "The norm of grad vector is  28504.523394993863\n",
      "Loss decreases to  70.11489295225762\n",
      "The norm of grad vector is  28482.682217765687\n",
      "Loss decreases to  70.10495637488833\n",
      "The norm of grad vector is  28460.909265617203\n",
      "Loss decreases to  70.09503497435276\n",
      "The norm of grad vector is  28439.204248571466\n",
      "Loss decreases to  70.08512869176542\n",
      "The norm of grad vector is  28417.566878220343\n",
      "Loss decreases to  70.07523746854898\n",
      "The norm of grad vector is  28395.996867714133\n",
      "Loss decreases to  70.06536124643318\n",
      "The norm of grad vector is  28374.49393175063\n",
      "Loss decreases to  70.05549996745262\n",
      "The norm of grad vector is  28353.05778656439\n",
      "Loss decreases to  70.04565357394412\n",
      "The norm of grad vector is  28331.68814991618\n",
      "Loss decreases to  70.03582200854564\n",
      "The norm of grad vector is  28310.38474108238\n",
      "Loss decreases to  70.0260052141938\n",
      "The norm of grad vector is  28289.14728084476\n",
      "Loss decreases to  70.01620313412175\n",
      "The norm of grad vector is  28267.97549147994\n",
      "Loss decreases to  70.00641571185795\n",
      "The norm of grad vector is  28246.869096748964\n",
      "Loss decreases to  69.99664289122352\n",
      "The norm of grad vector is  28225.827821887706\n",
      "Loss decreases to  69.98688461633083\n",
      "The norm of grad vector is  28204.851393595865\n",
      "Loss decreases to  69.97714083158104\n",
      "The norm of grad vector is  28183.93954002765\n",
      "Loss decreases to  69.9674114816631\n",
      "The norm of grad vector is  28163.09199078161\n",
      "Loss decreases to  69.95769651155098\n",
      "The norm of grad vector is  28142.30847689053\n",
      "Loss decreases to  69.94799586650275\n",
      "The norm of grad vector is  28121.58873081188\n",
      "Loss decreases to  69.93830949205763\n",
      "The norm of grad vector is  28100.93248641805\n",
      "Loss decreases to  69.92863733403517\n",
      "The norm of grad vector is  28080.339478986545\n",
      "Loss decreases to  69.91897933853286\n",
      "The norm of grad vector is  28059.80944519052\n",
      "Loss decreases to  69.90933545192478\n",
      "The norm of grad vector is  28039.342123089322\n",
      "Loss decreases to  69.89970562085963\n",
      "The norm of grad vector is  28018.937252118834\n",
      "Loss decreases to  69.89008979225852\n",
      "The norm of grad vector is  27998.59457308239\n",
      "Loss decreases to  69.8804879133141\n",
      "The norm of grad vector is  27978.313828141403\n",
      "Loss decreases to  69.87089993148838\n",
      "The norm of grad vector is  27958.09476080589\n",
      "Loss decreases to  69.86132579451058\n",
      "The norm of grad vector is  27937.93711592574\n",
      "Loss decreases to  69.85176545037625\n",
      "The norm of grad vector is  27917.840639681337\n",
      "Loss decreases to  69.84221884734526\n",
      "The norm of grad vector is  27897.805079574766\n",
      "Loss decreases to  69.83268593393976\n",
      "The norm of grad vector is  27877.83018442068\n",
      "Loss decreases to  69.82316665894291\n",
      "The norm of grad vector is  27857.91570433748\n",
      "Loss decreases to  69.81366097139727\n",
      "The norm of grad vector is  27838.06139073872\n",
      "Loss decreases to  69.80416882060291\n",
      "The norm of grad vector is  27818.266996324182\n",
      "Loss decreases to  69.7946901561159\n",
      "The norm of grad vector is  27798.532275071317\n",
      "Loss decreases to  69.78522492774678\n",
      "The norm of grad vector is  27778.85698222646\n",
      "Loss decreases to  69.77577308555881\n",
      "The norm of grad vector is  27759.240874296556\n",
      "Loss decreases to  69.76633457986648\n",
      "The norm of grad vector is  27739.68370904061\n",
      "Loss decreases to  69.75690936123402\n",
      "The norm of grad vector is  27720.185245461194\n",
      "Loss decreases to  69.74749738047348\n",
      "The norm of grad vector is  27700.74524379637\n",
      "Loss decreases to  69.73809858864377\n",
      "The norm of grad vector is  27681.36346551116\n",
      "Loss decreases to  69.72871293704821\n",
      "The norm of grad vector is  27662.039673289404\n",
      "Loss decreases to  69.71934037723445\n",
      "The norm of grad vector is  27642.77363102588\n",
      "Loss decreases to  69.70998086099134\n",
      "The norm of grad vector is  27623.56510381776\n",
      "Loss decreases to  69.70063434034847\n",
      "The norm of grad vector is  27604.413857957126\n",
      "Loss decreases to  69.69130076757422\n",
      "The norm of grad vector is  27585.31966092266\n",
      "Loss decreases to  69.68198009517462\n",
      "The norm of grad vector is  27566.282281371878\n",
      "Loss decreases to  69.67267227589159\n",
      "The norm of grad vector is  27547.3014891333\n",
      "Loss decreases to  69.66337726270172\n",
      "The norm of grad vector is  27528.377055198827\n",
      "Loss decreases to  69.6540950088145\n",
      "The norm of grad vector is  27509.50875171569\n",
      "Loss decreases to  69.64482546767115\n",
      "The norm of grad vector is  27490.69635197917\n",
      "Loss decreases to  69.63556859294323\n",
      "The norm of grad vector is  27471.93963042493\n",
      "Loss decreases to  69.62632433853084\n",
      "The norm of grad vector is  27453.23836262131\n",
      "Loss decreases to  69.61709265856192\n",
      "The norm of grad vector is  27434.59232526212\n",
      "Loss decreases to  69.6078735073902\n",
      "The norm of grad vector is  27416.001296158975\n",
      "Loss decreases to  69.59866683959393\n",
      "The norm of grad vector is  27397.465054234206\n",
      "Loss decreases to  69.58947260997478\n",
      "The norm of grad vector is  27378.983379513407\n",
      "Loss decreases to  69.58029077355647\n",
      "The norm of grad vector is  27360.556053118205\n",
      "Loss decreases to  69.57112128558289\n",
      "The norm of grad vector is  27342.182857259235\n",
      "Loss decreases to  69.5619641015178\n",
      "The norm of grad vector is  27323.86357522878\n",
      "Loss decreases to  69.55281917704212\n",
      "The norm of grad vector is  27305.597991393923\n",
      "Loss decreases to  69.54368646805364\n",
      "The norm of grad vector is  27287.385891189355\n",
      "Loss decreases to  69.53456593066565\n",
      "The norm of grad vector is  27269.227061110607\n",
      "Loss decreases to  69.5254575212051\n",
      "The norm of grad vector is  27251.12128870702\n",
      "Loss decreases to  69.51636119621173\n",
      "The norm of grad vector is  27233.068362574897\n",
      "Loss decreases to  69.5072769124367\n",
      "The norm of grad vector is  27215.068072350892\n",
      "Loss decreases to  69.49820462684106\n",
      "The norm of grad vector is  27197.12020870481\n",
      "Loss decreases to  69.48914429659511\n",
      "The norm of grad vector is  27179.224563333537\n",
      "Loss decreases to  69.4800958790765\n",
      "The norm of grad vector is  27161.380928953935\n",
      "Loss decreases to  69.47105933186926\n",
      "The norm of grad vector is  27143.58909929652\n",
      "Loss decreases to  69.46203461276244\n",
      "The norm of grad vector is  27125.84886909862\n",
      "Loss decreases to  69.4530216797492\n",
      "The norm of grad vector is  27108.160034098368\n",
      "Loss decreases to  69.44402049102544\n",
      "The norm of grad vector is  27090.52239102786\n",
      "Loss decreases to  69.43503100498833\n",
      "The norm of grad vector is  27072.93573760681\n",
      "Loss decreases to  69.4260531802354\n",
      "The norm of grad vector is  27055.399872536433\n",
      "Loss decreases to  69.41708697556336\n",
      "The norm of grad vector is  27037.91459549311\n",
      "Loss decreases to  69.40813234996659\n",
      "The norm of grad vector is  27020.47970712197\n",
      "Loss decreases to  69.39918926263667\n",
      "The norm of grad vector is  27003.095009030832\n",
      "Loss decreases to  69.39025767296\n",
      "The norm of grad vector is  26985.760303784118\n",
      "Loss decreases to  69.38133754051829\n",
      "The norm of grad vector is  26968.475394896683\n",
      "Loss decreases to  69.37242882508606\n",
      "The norm of grad vector is  26951.240086827736\n",
      "Loss decreases to  69.36353148662994\n",
      "The norm of grad vector is  26934.054184975\n",
      "Loss decreases to  69.35464548530767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  26916.91749566845\n",
      "Loss decreases to  69.34577078146692\n",
      "The norm of grad vector is  26899.829826164976\n",
      "Loss decreases to  69.33690733564399\n",
      "The norm of grad vector is  26882.790984641735\n",
      "Loss decreases to  69.32805510856313\n",
      "The norm of grad vector is  26865.800780191166\n",
      "Loss decreases to  69.31921406113493\n",
      "The norm of grad vector is  26848.85902281461\n",
      "Loss decreases to  69.31038415445558\n",
      "The norm of grad vector is  26831.96552341681\n",
      "Loss decreases to  69.30156534980557\n",
      "The norm of grad vector is  26815.120093800237\n",
      "Loss decreases to  69.29275760864887\n",
      "The norm of grad vector is  26798.32254665962\n",
      "Loss decreases to  69.28396089263174\n",
      "The norm of grad vector is  26781.572695575906\n",
      "Loss decreases to  69.27517516358131\n",
      "The norm of grad vector is  26764.870355011222\n",
      "Loss decreases to  69.26640038350541\n",
      "The norm of grad vector is  26748.215340303\n",
      "Loss decreases to  69.2576365145906\n",
      "The norm of grad vector is  26731.607467658712\n",
      "Loss decreases to  69.24888351920177\n",
      "The norm of grad vector is  26715.046554150344\n",
      "Loss decreases to  69.24014135988052\n",
      "The norm of grad vector is  26698.53241770916\n",
      "Loss decreases to  69.23140999934498\n",
      "The norm of grad vector is  26682.064877120105\n",
      "Loss decreases to  69.2226894004879\n",
      "The norm of grad vector is  26665.643752016927\n",
      "Loss decreases to  69.21397952637646\n",
      "The norm of grad vector is  26649.268862876426\n",
      "Loss decreases to  69.20528034025025\n",
      "The norm of grad vector is  26632.940031013644\n",
      "Loss decreases to  69.1965918055214\n",
      "The norm of grad vector is  26616.657078576452\n",
      "Loss decreases to  69.18791388577316\n",
      "The norm of grad vector is  26600.419828540562\n",
      "Loss decreases to  69.17924654475837\n",
      "The norm of grad vector is  26584.228104704318\n",
      "Loss decreases to  69.1705897463994\n",
      "The norm of grad vector is  26568.081731683727\n",
      "Loss decreases to  69.16194345478655\n",
      "The norm of grad vector is  26551.980534907412\n",
      "Loss decreases to  69.15330763417755\n",
      "The norm of grad vector is  26535.924340611647\n",
      "Loss decreases to  69.14468224899613\n",
      "The norm of grad vector is  26519.91297583532\n",
      "Loss decreases to  69.13606726383104\n",
      "The norm of grad vector is  26503.94626841528\n",
      "Loss decreases to  69.12746264343632\n",
      "The norm of grad vector is  26488.02404698114\n",
      "Loss decreases to  69.1188683527283\n",
      "The norm of grad vector is  26472.146140950743\n",
      "Loss decreases to  69.11028435678648\n",
      "The norm of grad vector is  26456.3123805252\n",
      "Loss decreases to  69.10171062085206\n",
      "The norm of grad vector is  26440.52259668421\n",
      "Loss decreases to  69.09314711032641\n",
      "The norm of grad vector is  26424.776621181285\n",
      "Loss decreases to  69.08459379077117\n",
      "The norm of grad vector is  26409.07428653924\n",
      "Loss decreases to  69.07605062790645\n",
      "The norm of grad vector is  26393.415426045205\n",
      "Loss decreases to  69.06751758761064\n",
      "The norm of grad vector is  26377.799873746488\n",
      "Loss decreases to  69.05899463591902\n",
      "The norm of grad vector is  26362.22746444557\n",
      "Loss decreases to  69.05048173902327\n",
      "The norm of grad vector is  26346.69803369586\n",
      "Loss decreases to  69.04197886327027\n",
      "The norm of grad vector is  26331.211417796938\n",
      "Loss decreases to  69.03348597516158\n",
      "The norm of grad vector is  26315.767453790315\n",
      "Loss decreases to  69.02500304135212\n",
      "The norm of grad vector is  26300.365979454986\n",
      "Loss decreases to  69.01653002864971\n",
      "The norm of grad vector is  26285.006833302836\n",
      "Loss decreases to  69.00806690401424\n",
      "The norm of grad vector is  26269.68985457436\n",
      "Loss decreases to  68.9996136345564\n",
      "The norm of grad vector is  26254.414883234578\n",
      "Loss decreases to  68.99117018753728\n",
      "The norm of grad vector is  26239.18175996828\n",
      "Loss decreases to  68.98273653036738\n",
      "The norm of grad vector is  26223.990326176092\n",
      "Loss decreases to  68.97431263060581\n",
      "The norm of grad vector is  26208.840423970167\n",
      "Loss decreases to  68.96589845595928\n",
      "The norm of grad vector is  26193.731896169982\n",
      "Loss decreases to  68.95749397428159\n",
      "The norm of grad vector is  26178.66458629802\n",
      "Loss decreases to  68.94909915357249\n",
      "The norm of grad vector is  26163.638338575798\n",
      "Loss decreases to  68.94071396197755\n",
      "The norm of grad vector is  26148.652997919908\n",
      "Loss decreases to  68.93233836778634\n",
      "The norm of grad vector is  26133.708409937426\n",
      "Loss decreases to  68.9239723394325\n",
      "The norm of grad vector is  26118.80442092242\n",
      "Loss decreases to  68.91561584549223\n",
      "The norm of grad vector is  26103.940877851637\n",
      "Loss decreases to  68.90726885468467\n",
      "The norm of grad vector is  26089.117628380558\n",
      "Loss decreases to  68.89893133586942\n",
      "The norm of grad vector is  26074.334520839504\n",
      "Loss decreases to  68.89060325804749\n",
      "The norm of grad vector is  26059.591404229606\n",
      "Loss decreases to  68.88228459035936\n",
      "The norm of grad vector is  26044.888128219052\n",
      "Loss decreases to  68.87397530208473\n",
      "The norm of grad vector is  26030.224543139168\n",
      "Loss decreases to  68.86567536264158\n",
      "The norm of grad vector is  26015.600499980468\n",
      "Loss decreases to  68.85738474158556\n",
      "The norm of grad vector is  26001.01585038895\n",
      "Loss decreases to  68.8491034086092\n",
      "The norm of grad vector is  25986.470446662363\n",
      "Loss decreases to  68.84083133354105\n",
      "The norm of grad vector is  25971.964141746357\n",
      "Loss decreases to  68.83256848634511\n",
      "The norm of grad vector is  25957.49678923083\n",
      "Loss decreases to  68.82431483711984\n",
      "The norm of grad vector is  25943.068243346177\n",
      "Loss decreases to  68.81607035609807\n",
      "The norm of grad vector is  25928.678358959613\n",
      "Loss decreases to  68.80783501364543\n",
      "The norm of grad vector is  25914.32699157168\n",
      "Loss decreases to  68.79960878026007\n",
      "The norm of grad vector is  25900.013997312424\n",
      "Loss decreases to  68.79139162657201\n",
      "The norm of grad vector is  25885.739232937845\n",
      "Loss decreases to  68.78318352334252\n",
      "The norm of grad vector is  25871.50255582657\n",
      "Loss decreases to  68.77498444146288\n",
      "The norm of grad vector is  25857.30382397606\n",
      "Loss decreases to  68.76679435195418\n",
      "The norm of grad vector is  25843.142895999263\n",
      "Loss decreases to  68.75861322596676\n",
      "The norm of grad vector is  25829.01963112088\n",
      "Loss decreases to  68.75044103477889\n",
      "The norm of grad vector is  25814.933889174332\n",
      "Loss decreases to  68.74227774979647\n",
      "The norm of grad vector is  25800.88553059793\n",
      "Loss decreases to  68.73412334255248\n",
      "The norm of grad vector is  25786.87441643177\n",
      "Loss decreases to  68.7259777847062\n",
      "The norm of grad vector is  25772.900408314123\n",
      "Loss decreases to  68.71784104804233\n",
      "The norm of grad vector is  25758.96336847825\n",
      "Loss decreases to  68.70971310447054\n",
      "The norm of grad vector is  25745.06315974894\n",
      "Loss decreases to  68.70159392602477\n",
      "The norm of grad vector is  25731.199645539375\n",
      "Loss decreases to  68.69348348486272\n",
      "The norm of grad vector is  25717.372689847667\n",
      "Loss decreases to  68.68538175326468\n",
      "The norm of grad vector is  25703.58215725378\n",
      "Loss decreases to  68.67728870363365\n",
      "The norm of grad vector is  25689.82791291602\n",
      "Loss decreases to  68.66920430849392\n",
      "The norm of grad vector is  25676.109822568287\n",
      "Loss decreases to  68.661128540491\n",
      "The norm of grad vector is  25662.42775251647\n",
      "Loss decreases to  68.65306137239097\n",
      "The norm of grad vector is  25648.781569635405\n",
      "Loss decreases to  68.64500277707923\n",
      "The norm of grad vector is  25635.17114136591\n",
      "Loss decreases to  68.63695272756024\n",
      "The norm of grad vector is  25621.59633571139\n",
      "Loss decreases to  68.62891119695779\n",
      "The norm of grad vector is  25608.05702123496\n",
      "Loss decreases to  68.62087815851265\n",
      "The norm of grad vector is  25594.55306705643\n",
      "Loss decreases to  68.61285358558325\n",
      "The norm of grad vector is  25581.0843428489\n",
      "Loss decreases to  68.60483745164457\n",
      "The norm of grad vector is  25567.650718836165\n",
      "Loss decreases to  68.59682973028778\n",
      "The norm of grad vector is  25554.25206578944\n",
      "Loss decreases to  68.58883039521935\n",
      "The norm of grad vector is  25540.88825502445\n",
      "Loss decreases to  68.58083942026074\n",
      "The norm of grad vector is  25527.559158398693\n",
      "Loss decreases to  68.5728567793474\n",
      "The norm of grad vector is  25514.26464830805\n",
      "Loss decreases to  68.5648824465285\n",
      "The norm of grad vector is  25501.004597684183\n",
      "Loss decreases to  68.55691639596655\n",
      "The norm of grad vector is  25487.778879991627\n",
      "Loss decreases to  68.5489586019364\n",
      "The norm of grad vector is  25474.58736922491\n",
      "Loss decreases to  68.54100903882458\n",
      "The norm of grad vector is  25461.429939905545\n",
      "Loss decreases to  68.53306768112914\n",
      "The norm of grad vector is  25448.306467079463\n",
      "Loss decreases to  68.52513450345907\n",
      "The norm of grad vector is  25435.21682631397\n",
      "Loss decreases to  68.51720948053303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  25422.160893694992\n",
      "Loss decreases to  68.50929258717974\n",
      "The norm of grad vector is  25409.138545824477\n",
      "Loss decreases to  68.50138379833685\n",
      "The norm of grad vector is  25396.14965981749\n",
      "Loss decreases to  68.49348308905047\n",
      "The norm of grad vector is  25383.194113299418\n",
      "Loss decreases to  68.48559043447449\n",
      "The norm of grad vector is  25370.271784403503\n",
      "Loss decreases to  68.4777058098705\n",
      "The norm of grad vector is  25357.38255176791\n",
      "Loss decreases to  68.46982919060683\n",
      "The norm of grad vector is  25344.526294533232\n",
      "Loss decreases to  68.46196055215789\n",
      "The norm of grad vector is  25331.702892339646\n",
      "Loss decreases to  68.45409987010406\n",
      "The norm of grad vector is  25318.912225324555\n",
      "Loss decreases to  68.4462471201308\n",
      "The norm of grad vector is  25306.1541741195\n",
      "Loss decreases to  68.43840227802853\n",
      "The norm of grad vector is  25293.42861984833\n",
      "Loss decreases to  68.43056531969118\n",
      "The norm of grad vector is  25280.735444123708\n",
      "Loss decreases to  68.42273622111709\n",
      "The norm of grad vector is  25268.074529045352\n",
      "Loss decreases to  68.41491495840708\n",
      "The norm of grad vector is  25255.445757197012\n",
      "Loss decreases to  68.40710150776496\n",
      "The norm of grad vector is  25242.84901164393\n",
      "Loss decreases to  68.39929584549611\n",
      "The norm of grad vector is  25230.284175930847\n",
      "Loss decreases to  68.39149794800797\n",
      "The norm of grad vector is  25217.751134078888\n",
      "Loss decreases to  68.38370779180863\n",
      "The norm of grad vector is  25205.249770583294\n",
      "Loss decreases to  68.37592535350697\n",
      "The norm of grad vector is  25192.77997041136\n",
      "Loss decreases to  68.36815060981152\n",
      "The norm of grad vector is  25180.341618999515\n",
      "Loss decreases to  68.36038353753074\n",
      "The norm of grad vector is  25167.9346022508\n",
      "Loss decreases to  68.35262411357152\n",
      "The norm of grad vector is  25155.558806533216\n",
      "Loss decreases to  68.34487231493998\n",
      "The norm of grad vector is  25143.214118676486\n",
      "Loss decreases to  68.33712811873974\n",
      "The norm of grad vector is  25130.900425970212\n",
      "Loss decreases to  68.32939150217217\n",
      "The norm of grad vector is  25118.6176161613\n",
      "Loss decreases to  68.32166244253536\n",
      "The norm of grad vector is  25106.36557745179\n",
      "Loss decreases to  68.31394091722458\n",
      "The norm of grad vector is  25094.144198496342\n",
      "Loss decreases to  68.3062269037304\n",
      "The norm of grad vector is  25081.95336840011\n",
      "Loss decreases to  68.29852037963964\n",
      "The norm of grad vector is  25069.79297671626\n",
      "Loss decreases to  68.29082132263363\n",
      "The norm of grad vector is  25057.66291344389\n",
      "Loss decreases to  68.28312971048898\n",
      "The norm of grad vector is  25045.563069025735\n",
      "Loss decreases to  68.27544552107592\n",
      "The norm of grad vector is  25033.493334345883\n",
      "Loss decreases to  68.26776873235876\n",
      "The norm of grad vector is  25021.45360072748\n",
      "Loss decreases to  68.2600993223949\n",
      "The norm of grad vector is  25009.44375993074\n",
      "Loss decreases to  68.25243726933451\n",
      "The norm of grad vector is  24997.46370415045\n",
      "Loss decreases to  68.24478255141996\n",
      "The norm of grad vector is  24985.51332601409\n",
      "Loss decreases to  68.23713514698578\n",
      "The norm of grad vector is  24973.592518579466\n",
      "Loss decreases to  68.2294950344577\n",
      "The norm of grad vector is  24961.7011753327\n",
      "Loss decreases to  68.22186219235242\n",
      "The norm of grad vector is  24949.839190185896\n",
      "Loss decreases to  68.21423659927733\n",
      "The norm of grad vector is  24938.00645747535\n",
      "Loss decreases to  68.20661823392955\n",
      "The norm of grad vector is  24926.20287195909\n",
      "Loss decreases to  68.19900707509602\n",
      "The norm of grad vector is  24914.428328814985\n",
      "Loss decreases to  68.19140310165292\n",
      "The norm of grad vector is  24902.682723638685\n",
      "Loss decreases to  68.18380629256517\n",
      "The norm of grad vector is  24890.96595244138\n",
      "Loss decreases to  68.17621662688596\n",
      "The norm of grad vector is  24879.27791164809\n",
      "Loss decreases to  68.16863408375615\n",
      "The norm of grad vector is  24867.618498095148\n",
      "Loss decreases to  68.16105864240431\n",
      "The norm of grad vector is  24855.98760902863\n",
      "Loss decreases to  68.15349028214618\n",
      "The norm of grad vector is  24844.385142102085\n",
      "Loss decreases to  68.1459289823836\n",
      "The norm of grad vector is  24832.810995374708\n",
      "Loss decreases to  68.13837472260496\n",
      "The norm of grad vector is  24821.26506730921\n",
      "Loss decreases to  68.13082748238429\n",
      "The norm of grad vector is  24809.747256769835\n",
      "Loss decreases to  68.12328724138119\n",
      "The norm of grad vector is  24798.257463020713\n",
      "Loss decreases to  68.11575397933962\n",
      "The norm of grad vector is  24786.795585723452\n",
      "Loss decreases to  68.10822767608848\n",
      "The norm of grad vector is  24775.361524935637\n",
      "Loss decreases to  68.10070831154091\n",
      "The norm of grad vector is  24763.95518110859\n",
      "Loss decreases to  68.09319586569329\n",
      "The norm of grad vector is  24752.576455085775\n",
      "Loss decreases to  68.08569031862565\n",
      "The norm of grad vector is  24741.225248100545\n",
      "Loss decreases to  68.07819165050078\n",
      "The norm of grad vector is  24729.901461774607\n",
      "Loss decreases to  68.07069984156391\n",
      "The norm of grad vector is  24718.604998116032\n",
      "Loss decreases to  68.06321487214234\n",
      "The norm of grad vector is  24707.335759517275\n",
      "Loss decreases to  68.05573672264521\n",
      "The norm of grad vector is  24696.093648753544\n",
      "Loss decreases to  68.04826537356284\n",
      "The norm of grad vector is  24684.87856898074\n",
      "Loss decreases to  68.04080080546663\n",
      "The norm of grad vector is  24673.690423734024\n",
      "Loss decreases to  68.03334299900803\n",
      "The norm of grad vector is  24662.529116925547\n",
      "Loss decreases to  68.02589193491926\n",
      "The norm of grad vector is  24651.394552842936\n",
      "Loss decreases to  68.01844759401187\n",
      "The norm of grad vector is  24640.286636147528\n",
      "Loss decreases to  68.01100995717681\n",
      "The norm of grad vector is  24629.205271872437\n",
      "Loss decreases to  68.00357900538417\n",
      "The norm of grad vector is  24618.150365420966\n",
      "Loss decreases to  67.99615471968255\n",
      "The norm of grad vector is  24607.12182256474\n",
      "Loss decreases to  67.98873708119862\n",
      "The norm of grad vector is  24596.119549442086\n",
      "Loss decreases to  67.98132607113747\n",
      "The norm of grad vector is  24585.143452556073\n",
      "Loss decreases to  67.97392167078087\n",
      "The norm of grad vector is  24574.193438773233\n",
      "Loss decreases to  67.96652386148835\n",
      "The norm of grad vector is  24563.269415321374\n",
      "Loss decreases to  67.95913262469593\n",
      "The norm of grad vector is  24552.37128978825\n",
      "Loss decreases to  67.95174794191594\n",
      "The norm of grad vector is  24541.49897011966\n",
      "Loss decreases to  67.94436979473701\n",
      "The norm of grad vector is  24530.65236461796\n",
      "Loss decreases to  67.93699816482315\n",
      "The norm of grad vector is  24519.831381940206\n",
      "Loss decreases to  67.92963303391382\n",
      "The norm of grad vector is  24509.03593109671\n",
      "Loss decreases to  67.92227438382331\n",
      "The norm of grad vector is  24498.26592144925\n",
      "Loss decreases to  67.91492219644063\n",
      "The norm of grad vector is  24487.52126270956\n",
      "Loss decreases to  67.90757645372898\n",
      "The norm of grad vector is  24476.801864937635\n",
      "Loss decreases to  67.90023713772543\n",
      "The norm of grad vector is  24466.10763854007\n",
      "Loss decreases to  67.8929042305406\n",
      "The norm of grad vector is  24455.4384942687\n",
      "Loss decreases to  67.88557771435836\n",
      "The norm of grad vector is  24444.794343218717\n",
      "Loss decreases to  67.87825757143526\n",
      "The norm of grad vector is  24434.175096827297\n",
      "Loss decreases to  67.87094378410045\n",
      "The norm of grad vector is  24423.580666872007\n",
      "Loss decreases to  67.8636363347553\n",
      "The norm of grad vector is  24413.010965469217\n",
      "Loss decreases to  67.85633520587288\n",
      "The norm of grad vector is  24402.46590507252\n",
      "Loss decreases to  67.84904037999792\n",
      "The norm of grad vector is  24391.94539847119\n",
      "Loss decreases to  67.84175183974612\n",
      "The norm of grad vector is  24381.449358788846\n",
      "Loss decreases to  67.83446956780416\n",
      "The norm of grad vector is  24370.977699481697\n",
      "Loss decreases to  67.82719354692884\n",
      "The norm of grad vector is  24360.530334337076\n",
      "Loss decreases to  67.81992375994767\n",
      "The norm of grad vector is  24350.10717747209\n",
      "Loss decreases to  67.81266018975764\n",
      "The norm of grad vector is  24339.708143332107\n",
      "Loss decreases to  67.80540281932521\n",
      "The norm of grad vector is  24329.333146688932\n",
      "Loss decreases to  67.79815163168621\n",
      "The norm of grad vector is  24318.982102639922\n",
      "Loss decreases to  67.79090660994517\n",
      "The norm of grad vector is  24308.654926606025\n",
      "Loss decreases to  67.78366773727505\n",
      "The norm of grad vector is  24298.35153433058\n",
      "Loss decreases to  67.77643499691749\n",
      "The norm of grad vector is  24288.071841877856\n",
      "Loss decreases to  67.76920837218142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  24277.815765631472\n",
      "Loss decreases to  67.76198784644386\n",
      "The norm of grad vector is  24267.583222293193\n",
      "Loss decreases to  67.75477340314872\n",
      "The norm of grad vector is  24257.374128881245\n",
      "Loss decreases to  67.74756502580706\n",
      "The norm of grad vector is  24247.188402729258\n",
      "Loss decreases to  67.74036269799664\n",
      "The norm of grad vector is  24237.025961484484\n",
      "Loss decreases to  67.73316640336127\n",
      "The norm of grad vector is  24226.88672310675\n",
      "Loss decreases to  67.72597612561106\n",
      "The norm of grad vector is  24216.770605866714\n",
      "Loss decreases to  67.71879184852187\n",
      "The norm of grad vector is  24206.677528344895\n",
      "Loss decreases to  67.71161355593455\n",
      "The norm of grad vector is  24196.60740943002\n",
      "Loss decreases to  67.70444123175587\n",
      "The norm of grad vector is  24186.56016831767\n",
      "Loss decreases to  67.69727485995642\n",
      "The norm of grad vector is  24176.535724509216\n",
      "Loss decreases to  67.69011442457212\n",
      "The norm of grad vector is  24166.53399780998\n",
      "Loss decreases to  67.68295990970273\n",
      "The norm of grad vector is  24156.55490832849\n",
      "Loss decreases to  67.67581129951202\n",
      "The norm of grad vector is  24146.59837647474\n",
      "Loss decreases to  67.6686685782273\n",
      "The norm of grad vector is  24136.66432295893\n",
      "Loss decreases to  67.66153173013936\n",
      "The norm of grad vector is  24126.752668790275\n",
      "Loss decreases to  67.65440073960201\n",
      "The norm of grad vector is  24116.86333527562\n",
      "Loss decreases to  67.64727559103166\n",
      "The norm of grad vector is  24106.996244018297\n",
      "Loss decreases to  67.64015626890753\n",
      "The norm of grad vector is  24097.151316916505\n",
      "Loss decreases to  67.63304275777057\n",
      "The norm of grad vector is  24087.32847616242\n",
      "Loss decreases to  67.62593504222399\n",
      "The norm of grad vector is  24077.527644240654\n",
      "Loss decreases to  67.61883310693247\n",
      "The norm of grad vector is  24067.748743927103\n",
      "Loss decreases to  67.611736936622\n",
      "The norm of grad vector is  24057.991698287722\n",
      "Loss decreases to  67.60464651607981\n",
      "The norm of grad vector is  24048.25643067709\n",
      "Loss decreases to  67.59756183015361\n",
      "The norm of grad vector is  24038.542864737436\n",
      "Loss decreases to  67.59048286375187\n",
      "The norm of grad vector is  24028.850924397208\n",
      "Loss decreases to  67.5834096018431\n",
      "The norm of grad vector is  24019.180533869967\n",
      "Loss decreases to  67.57634202945589\n",
      "The norm of grad vector is  24009.53161765295\n",
      "Loss decreases to  67.5692801316785\n",
      "The norm of grad vector is  23999.904100526208\n",
      "Loss decreases to  67.56222389365836\n",
      "The norm of grad vector is  23990.297907551056\n",
      "Loss decreases to  67.55517330060262\n",
      "The norm of grad vector is  23980.712964069193\n",
      "Loss decreases to  67.54812833777642\n",
      "The norm of grad vector is  23971.14919570113\n",
      "Loss decreases to  67.54108899050439\n",
      "The norm of grad vector is  23961.606528345383\n",
      "Loss decreases to  67.53405524416881\n",
      "The norm of grad vector is  23952.08488817705\n",
      "Loss decreases to  67.52702708421049\n",
      "The norm of grad vector is  23942.58420164674\n",
      "Loss decreases to  67.52000449612777\n",
      "The norm of grad vector is  23933.10439547937\n",
      "Loss decreases to  67.51298746547673\n",
      "The norm of grad vector is  23923.645396673004\n",
      "Loss decreases to  67.50597597787059\n",
      "The norm of grad vector is  23914.20713249772\n",
      "Loss decreases to  67.4989700189795\n",
      "The norm of grad vector is  23904.789530494527\n",
      "Loss decreases to  67.49196957453077\n",
      "The norm of grad vector is  23895.392518474062\n",
      "Loss decreases to  67.48497463030783\n",
      "The norm of grad vector is  23886.01602451561\n",
      "Loss decreases to  67.4779851721507\n",
      "The norm of grad vector is  23876.659976965944\n",
      "Loss decreases to  67.471001185955\n",
      "The norm of grad vector is  23867.32430443811\n",
      "Loss decreases to  67.46402265767254\n",
      "The norm of grad vector is  23858.008935810427\n",
      "Loss decreases to  67.45704957331017\n",
      "The norm of grad vector is  23848.713800225352\n",
      "Loss decreases to  67.45008191893045\n",
      "The norm of grad vector is  23839.438827088405\n",
      "Loss decreases to  67.44311968065082\n",
      "The norm of grad vector is  23830.183946066845\n",
      "Loss decreases to  67.4361628446432\n",
      "The norm of grad vector is  23820.949087089048\n",
      "Loss decreases to  67.42921139713442\n",
      "The norm of grad vector is  23811.734180342883\n",
      "Loss decreases to  67.4222653244053\n",
      "The norm of grad vector is  23802.539156275132\n",
      "Loss decreases to  67.41532461279071\n",
      "The norm of grad vector is  23793.363945589983\n",
      "Loss decreases to  67.40838924867953\n",
      "The norm of grad vector is  23784.208479248296\n",
      "Loss decreases to  67.40145921851386\n",
      "The norm of grad vector is  23775.07268846638\n",
      "Loss decreases to  67.39453450878948\n",
      "The norm of grad vector is  23765.95650471487\n",
      "Loss decreases to  67.38761510605484\n",
      "The norm of grad vector is  23756.859859717988\n",
      "Loss decreases to  67.38070099691151\n",
      "The norm of grad vector is  23747.78268545218\n",
      "Loss decreases to  67.37379216801367\n",
      "The norm of grad vector is  23738.724914145132\n",
      "Loss decreases to  67.36688860606745\n",
      "The norm of grad vector is  23729.68647827485\n",
      "Loss decreases to  67.35999029783149\n",
      "The norm of grad vector is  23720.667310568664\n",
      "Loss decreases to  67.35309723011662\n",
      "The norm of grad vector is  23711.667344002002\n",
      "Loss decreases to  67.34620938978453\n",
      "The norm of grad vector is  23702.686511797678\n",
      "Loss decreases to  67.33932676374887\n",
      "The norm of grad vector is  23693.724747424454\n",
      "Loss decreases to  67.33244933897447\n",
      "The norm of grad vector is  23684.781984596542\n",
      "Loss decreases to  67.32557710247703\n",
      "The norm of grad vector is  23675.85815727222\n",
      "Loss decreases to  67.31871004132314\n",
      "The norm of grad vector is  23666.953199652966\n",
      "Loss decreases to  67.31184814262969\n",
      "The norm of grad vector is  23658.067046182558\n",
      "Loss decreases to  67.30499139356402\n",
      "The norm of grad vector is  23649.199631545936\n",
      "Loss decreases to  67.29813978134372\n",
      "The norm of grad vector is  23640.35089066838\n",
      "Loss decreases to  67.2912932932357\n",
      "The norm of grad vector is  23631.52075871436\n",
      "Loss decreases to  67.28445191655724\n",
      "The norm of grad vector is  23622.709171086786\n",
      "Loss decreases to  67.2776156386745\n",
      "The norm of grad vector is  23613.916063425793\n",
      "Loss decreases to  67.270784447003\n",
      "The norm of grad vector is  23605.14137160805\n",
      "Loss decreases to  67.26395832900734\n",
      "The norm of grad vector is  23596.38503174566\n",
      "Loss decreases to  67.25713727220071\n",
      "The norm of grad vector is  23587.646980185084\n",
      "Loss decreases to  67.25032126414496\n",
      "The norm of grad vector is  23578.92715350658\n",
      "Loss decreases to  67.24351029245028\n",
      "The norm of grad vector is  23570.225488522872\n",
      "Loss decreases to  67.23670434477509\n",
      "The norm of grad vector is  23561.541922278397\n",
      "Loss decreases to  67.22990340882548\n",
      "The norm of grad vector is  23552.87639204841\n",
      "Loss decreases to  67.22310747235561\n",
      "The norm of grad vector is  23544.228835337995\n",
      "Loss decreases to  67.21631652316657\n",
      "The norm of grad vector is  23535.59918988114\n",
      "Loss decreases to  67.20953054910726\n",
      "The norm of grad vector is  23526.98739363993\n",
      "Loss decreases to  67.20274953807342\n",
      "The norm of grad vector is  23518.3933848035\n",
      "Loss decreases to  67.19597347800779\n",
      "The norm of grad vector is  23509.817101787135\n",
      "Loss decreases to  67.18920235689978\n",
      "The norm of grad vector is  23501.258483231515\n",
      "Loss decreases to  67.18243616278501\n",
      "The norm of grad vector is  23492.71746800172\n",
      "Loss decreases to  67.17567488374553\n",
      "The norm of grad vector is  23484.19399518633\n",
      "Loss decreases to  67.1689185079096\n",
      "The norm of grad vector is  23475.6880040966\n",
      "Loss decreases to  67.16216702345113\n",
      "The norm of grad vector is  23467.199434265414\n",
      "Loss decreases to  67.15542041858976\n",
      "The norm of grad vector is  23458.728225446797\n",
      "Loss decreases to  67.14867868159051\n",
      "The norm of grad vector is  23450.274317614512\n",
      "Loss decreases to  67.14194180076399\n",
      "The norm of grad vector is  23441.837650961636\n",
      "Loss decreases to  67.13520976446507\n",
      "The norm of grad vector is  23433.41816589936\n",
      "Loss decreases to  67.1284825610945\n",
      "The norm of grad vector is  23425.01580305653\n",
      "Loss decreases to  67.12176017909705\n",
      "The norm of grad vector is  23416.630503278415\n",
      "Loss decreases to  67.11504260696205\n",
      "The norm of grad vector is  23408.262207625965\n",
      "Loss decreases to  67.10832983322344\n",
      "The norm of grad vector is  23399.910857375093\n",
      "Loss decreases to  67.10162184645877\n",
      "The norm of grad vector is  23391.57639401571\n",
      "Loss decreases to  67.09491863528974\n",
      "The norm of grad vector is  23383.25875925089\n",
      "Loss decreases to  67.08822018838183\n",
      "The norm of grad vector is  23374.95789499607\n",
      "Loss decreases to  67.08152649444384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  23366.673743378302\n",
      "Loss decreases to  67.07483754222795\n",
      "The norm of grad vector is  23358.40624673523\n",
      "Loss decreases to  67.06815332052976\n",
      "The norm of grad vector is  23350.15534761445\n",
      "Loss decreases to  67.06147381818728\n",
      "The norm of grad vector is  23341.92098877259\n",
      "Loss decreases to  67.05479902408185\n",
      "The norm of grad vector is  23333.7031131745\n",
      "Loss decreases to  67.04812892713686\n",
      "The norm of grad vector is  23325.501663992552\n",
      "Loss decreases to  67.0414635163186\n",
      "The norm of grad vector is  23317.316584605844\n",
      "Loss decreases to  67.03480278063518\n",
      "The norm of grad vector is  23309.147818598976\n",
      "Loss decreases to  67.02814670913703\n",
      "The norm of grad vector is  23300.99530976192\n",
      "Loss decreases to  67.02149529091633\n",
      "The norm of grad vector is  23292.859002088768\n",
      "Loss decreases to  67.01484851510666\n",
      "The norm of grad vector is  23284.738839777103\n",
      "Loss decreases to  67.00820637088357\n",
      "The norm of grad vector is  23276.634767227144\n",
      "Loss decreases to  67.00156884746336\n",
      "The norm of grad vector is  23268.546729040987\n",
      "Loss decreases to  66.9949359341041\n",
      "The norm of grad vector is  23260.47467002193\n",
      "Loss decreases to  66.98830762010427\n",
      "The norm of grad vector is  23252.418535173485\n",
      "Loss decreases to  66.98168389480351\n",
      "The norm of grad vector is  23244.378269698827\n",
      "Loss decreases to  66.97506474758177\n",
      "The norm of grad vector is  23236.35381899988\n",
      "Loss decreases to  66.9684501678595\n",
      "The norm of grad vector is  23228.34512867665\n",
      "Loss decreases to  66.96184014509765\n",
      "The norm of grad vector is  23220.352144526376\n",
      "Loss decreases to  66.95523466879685\n",
      "The norm of grad vector is  23212.374812542865\n",
      "Loss decreases to  66.94863372849787\n",
      "The norm of grad vector is  23204.413078915586\n",
      "Loss decreases to  66.94203731378136\n",
      "The norm of grad vector is  23196.466890029114\n",
      "Loss decreases to  66.93544541426716\n",
      "The norm of grad vector is  23188.536192462347\n",
      "Loss decreases to  66.9288580196147\n",
      "The norm of grad vector is  23180.62093298762\n",
      "Loss decreases to  66.92227511952285\n",
      "The norm of grad vector is  23172.72105857004\n",
      "Loss decreases to  66.91569670372903\n",
      "The norm of grad vector is  23164.836516366897\n",
      "Loss decreases to  66.90912276201014\n",
      "The norm of grad vector is  23156.967253726638\n",
      "Loss decreases to  66.90255328418138\n",
      "The norm of grad vector is  23149.113218188493\n",
      "Loss decreases to  66.89598826009663\n",
      "The norm of grad vector is  23141.274357481416\n",
      "Loss decreases to  66.88942767964812\n",
      "The norm of grad vector is  23133.45061952356\n",
      "Loss decreases to  66.8828715327665\n",
      "The norm of grad vector is  23125.64195242153\n",
      "Loss decreases to  66.87631980942035\n",
      "The norm of grad vector is  23117.84830446964\n",
      "Loss decreases to  66.8697724996161\n",
      "The norm of grad vector is  23110.069624149208\n",
      "Loss decreases to  66.86322959339796\n",
      "The norm of grad vector is  23102.30586012782\n",
      "Loss decreases to  66.85669108084778\n",
      "The norm of grad vector is  23094.55696125871\n",
      "Loss decreases to  66.85015695208483\n",
      "The norm of grad vector is  23086.82287657995\n",
      "Loss decreases to  66.8436271972655\n",
      "The norm of grad vector is  23079.10355531384\n",
      "Loss decreases to  66.8371018065834\n",
      "The norm of grad vector is  23071.398946866146\n",
      "Loss decreases to  66.83058077026928\n",
      "The norm of grad vector is  23063.709000825478\n",
      "Loss decreases to  66.82406407859011\n",
      "The norm of grad vector is  23056.033666962496\n",
      "Loss decreases to  66.81755172185018\n",
      "The norm of grad vector is  23048.37289522941\n",
      "Loss decreases to  66.81104369038975\n",
      "The norm of grad vector is  23040.726635759016\n",
      "Loss decreases to  66.80453997458551\n",
      "The norm of grad vector is  23033.09483886434\n",
      "Loss decreases to  66.79804056485041\n",
      "The norm of grad vector is  23025.47745503762\n",
      "Loss decreases to  66.79154545163354\n",
      "The norm of grad vector is  23017.87443494999\n",
      "Loss decreases to  66.78505462541956\n",
      "The norm of grad vector is  23010.285729450512\n",
      "Loss decreases to  66.77856807672904\n",
      "The norm of grad vector is  23002.711289565694\n",
      "Loss decreases to  66.7720857961178\n",
      "The norm of grad vector is  22995.15106649866\n",
      "Loss decreases to  66.7656077741777\n",
      "The norm of grad vector is  22987.605011628704\n",
      "Loss decreases to  66.75913400153499\n",
      "The norm of grad vector is  22980.073076510434\n",
      "Loss decreases to  66.75266446885166\n",
      "The norm of grad vector is  22972.555212873245\n",
      "Loss decreases to  66.74619916682465\n",
      "The norm of grad vector is  22965.05137262058\n",
      "Loss decreases to  66.73973808618526\n",
      "The norm of grad vector is  22957.561507829338\n",
      "Loss decreases to  66.73328121769995\n",
      "The norm of grad vector is  22950.08557074913\n",
      "Loss decreases to  66.72682855216935\n",
      "The norm of grad vector is  22942.62351380181\n",
      "Loss decreases to  66.72038008042863\n",
      "The norm of grad vector is  22935.175289580773\n",
      "Loss decreases to  66.71393579334698\n",
      "The norm of grad vector is  22927.74085085009\n",
      "Loss decreases to  66.70749568182808\n",
      "The norm of grad vector is  22920.320150544263\n",
      "Loss decreases to  66.70105973680911\n",
      "The norm of grad vector is  22912.913141767203\n",
      "Loss decreases to  66.6946279492612\n",
      "The norm of grad vector is  22905.519777791946\n",
      "Loss decreases to  66.68820031018919\n",
      "The norm of grad vector is  22898.14001205972\n",
      "Loss decreases to  66.68177681063142\n",
      "The norm of grad vector is  22890.773798179634\n",
      "Loss decreases to  66.67535744165953\n",
      "The norm of grad vector is  22883.42108992774\n",
      "Loss decreases to  66.66894219437837\n",
      "The norm of grad vector is  22876.081841246596\n",
      "Loss decreases to  66.66253105992597\n",
      "The norm of grad vector is  22868.75600624462\n",
      "Loss decreases to  66.6561240294733\n",
      "The norm of grad vector is  22861.443539195512\n",
      "Loss decreases to  66.64972109422395\n",
      "The norm of grad vector is  22854.14439453755\n",
      "Loss decreases to  66.64332224541442\n",
      "The norm of grad vector is  22846.858526873028\n",
      "Loss decreases to  66.63692747431362\n",
      "The norm of grad vector is  22839.58589096775\n",
      "Loss decreases to  66.63053677222291\n",
      "The norm of grad vector is  22832.326441750072\n",
      "Loss decreases to  66.6241501304759\n",
      "The norm of grad vector is  22825.0801343109\n",
      "Loss decreases to  66.61776754043817\n",
      "The norm of grad vector is  22817.8469239025\n",
      "Loss decreases to  66.61138899350762\n",
      "The norm of grad vector is  22810.626765938247\n",
      "Loss decreases to  66.60501448111364\n",
      "The norm of grad vector is  22803.41961599193\n",
      "Loss decreases to  66.59864399471749\n",
      "The norm of grad vector is  22796.225429797192\n",
      "Loss decreases to  66.59227752581225\n",
      "The norm of grad vector is  22789.044163246846\n",
      "Loss decreases to  66.58591506592197\n",
      "The norm of grad vector is  22781.87577239243\n",
      "Loss decreases to  66.57955660660248\n",
      "The norm of grad vector is  22774.720213443503\n",
      "Loss decreases to  66.57320213944053\n",
      "The norm of grad vector is  22767.57744276715\n",
      "Loss decreases to  66.56685165605404\n",
      "The norm of grad vector is  22760.44741688736\n",
      "Loss decreases to  66.56050514809193\n",
      "The norm of grad vector is  22753.330092484488\n",
      "Loss decreases to  66.55416260723376\n",
      "The norm of grad vector is  22746.225426394572\n",
      "Loss decreases to  66.54782402518983\n",
      "The norm of grad vector is  22739.13337560892\n",
      "Loss decreases to  66.54148939370096\n",
      "The norm of grad vector is  22732.0538972734\n",
      "Loss decreases to  66.53515870453856\n",
      "The norm of grad vector is  22724.986948688053\n",
      "Loss decreases to  66.52883194950411\n",
      "The norm of grad vector is  22717.932487306276\n",
      "Loss decreases to  66.5225091204293\n",
      "The norm of grad vector is  22710.89047073445\n",
      "Loss decreases to  66.51619020917592\n",
      "The norm of grad vector is  22703.860856731382\n",
      "Loss decreases to  66.50987520763584\n",
      "The norm of grad vector is  22696.843603207642\n",
      "Loss decreases to  66.50356410773028\n",
      "The norm of grad vector is  22689.838668225017\n",
      "Loss decreases to  66.49725690141048\n",
      "The norm of grad vector is  22682.846009996065\n",
      "Loss decreases to  66.49095358065705\n",
      "The norm of grad vector is  22675.865586883585\n",
      "Loss decreases to  66.48465413748023\n",
      "The norm of grad vector is  22668.89735739972\n",
      "Loss decreases to  66.47835856391929\n",
      "The norm of grad vector is  22661.94128020595\n",
      "Loss decreases to  66.47206685204291\n",
      "The norm of grad vector is  22654.9973141122\n",
      "Loss decreases to  66.46577899394865\n",
      "The norm of grad vector is  22648.06541807632\n",
      "Loss decreases to  66.45949498176302\n",
      "The norm of grad vector is  22641.145551203575\n",
      "Loss decreases to  66.45321480764129\n",
      "The norm of grad vector is  22634.237672746265\n",
      "Loss decreases to  66.44693846376778\n",
      "The norm of grad vector is  22627.341742102937\n",
      "Loss decreases to  66.44066594235497\n",
      "The norm of grad vector is  22620.45771881807\n",
      "Loss decreases to  66.4343972356439\n",
      "The norm of grad vector is  22613.585562581346\n",
      "Loss decreases to  66.42813233590407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  22606.72523322735\n",
      "Loss decreases to  66.42187123543303\n",
      "The norm of grad vector is  22599.876690734767\n",
      "Loss decreases to  66.41561392655662\n",
      "The norm of grad vector is  22593.0398952262\n",
      "Loss decreases to  66.40936040162848\n",
      "The norm of grad vector is  22586.21480696728\n",
      "Loss decreases to  66.40311065303011\n",
      "The norm of grad vector is  22579.40138636646\n",
      "Loss decreases to  66.39686467317102\n",
      "The norm of grad vector is  22572.599593974282\n",
      "Loss decreases to  66.39062245448812\n",
      "The norm of grad vector is  22565.809390482973\n",
      "Loss decreases to  66.38438398944595\n",
      "The norm of grad vector is  22559.030736725872\n",
      "Loss decreases to  66.37814927053633\n",
      "The norm of grad vector is  22552.263593677006\n",
      "Loss decreases to  66.37191829027853\n",
      "The norm of grad vector is  22545.50792245054\n",
      "Loss decreases to  66.3656910412188\n",
      "The norm of grad vector is  22538.763684300135\n",
      "Loss decreases to  66.35946751593082\n",
      "The norm of grad vector is  22532.03084061864\n",
      "Loss decreases to  66.3532477070149\n",
      "The norm of grad vector is  22525.309352937533\n",
      "Loss decreases to  66.34703160709824\n",
      "The norm of grad vector is  22518.59918292639\n",
      "Loss decreases to  66.34081920883492\n",
      "The norm of grad vector is  22511.900292392354\n",
      "Loss decreases to  66.33461050490554\n",
      "The norm of grad vector is  22505.212643279654\n",
      "Loss decreases to  66.32840548801728\n",
      "The norm of grad vector is  22498.53619766925\n",
      "Loss decreases to  66.3222041509036\n",
      "The norm of grad vector is  22491.870917778102\n",
      "Loss decreases to  66.3160064863244\n",
      "The norm of grad vector is  22485.216765958834\n",
      "Loss decreases to  66.30981248706577\n",
      "The norm of grad vector is  22478.57370469923\n",
      "Loss decreases to  66.30362214593976\n",
      "The norm of grad vector is  22471.941696621765\n",
      "Loss decreases to  66.29743545578447\n",
      "The norm of grad vector is  22465.32070448297\n",
      "Loss decreases to  66.29125240946382\n",
      "The norm of grad vector is  22458.710691173146\n",
      "Loss decreases to  66.2850729998675\n",
      "The norm of grad vector is  22452.111619715764\n",
      "Loss decreases to  66.27889721991097\n",
      "The norm of grad vector is  22445.523453267022\n",
      "Loss decreases to  66.2727250625351\n",
      "The norm of grad vector is  22438.946155115398\n",
      "Loss decreases to  66.26655652070623\n",
      "The norm of grad vector is  22432.379688681078\n",
      "Loss decreases to  66.26039158741607\n",
      "The norm of grad vector is  22425.824017515613\n",
      "Loss decreases to  66.25423025568165\n",
      "The norm of grad vector is  22419.279105301277\n",
      "Loss decreases to  66.24807251854494\n",
      "The norm of grad vector is  22412.744915850773\n",
      "Loss decreases to  66.24191836907279\n",
      "The norm of grad vector is  22406.22141310663\n",
      "Loss decreases to  66.23576780035764\n",
      "The norm of grad vector is  22399.70856114092\n",
      "Loss decreases to  66.22962080551606\n",
      "The norm of grad vector is  22393.206324154497\n",
      "Loss decreases to  66.22347737768962\n",
      "The norm of grad vector is  22386.71466647677\n",
      "Loss decreases to  66.21733751004456\n",
      "The norm of grad vector is  22380.23355256522\n",
      "Loss decreases to  66.21120119577162\n",
      "The norm of grad vector is  22373.762947004812\n",
      "Loss decreases to  66.20506842808578\n",
      "The norm of grad vector is  22367.30281450774\n",
      "Loss decreases to  66.19893920022643\n",
      "The norm of grad vector is  22360.853119912706\n",
      "Loss decreases to  66.19281350545735\n",
      "The norm of grad vector is  22354.41382818468\n",
      "Loss decreases to  66.1866913370662\n",
      "The norm of grad vector is  22347.984904414425\n",
      "Loss decreases to  66.18057268836476\n",
      "The norm of grad vector is  22341.56631381791\n",
      "Loss decreases to  66.17445755268874\n",
      "The norm of grad vector is  22335.158021735984\n",
      "Loss decreases to  66.16834592339764\n",
      "The norm of grad vector is  22328.759993633925\n",
      "Loss decreases to  66.16223779387481\n",
      "The norm of grad vector is  22322.37219510089\n",
      "Loss decreases to  66.15613315752697\n",
      "The norm of grad vector is  22315.99459184964\n",
      "Loss decreases to  66.15003200778446\n",
      "The norm of grad vector is  22309.627149715892\n",
      "Loss decreases to  66.14393433810127\n",
      "The norm of grad vector is  22303.269834658113\n",
      "Loss decreases to  66.13784014195457\n",
      "The norm of grad vector is  22296.922612756836\n",
      "Loss decreases to  66.13174941284461\n",
      "The norm of grad vector is  22290.585450214447\n",
      "Loss decreases to  66.12566214429506\n",
      "The norm of grad vector is  22284.25831335454\n",
      "Loss decreases to  66.11957832985257\n",
      "The norm of grad vector is  22277.9411686217\n",
      "Loss decreases to  66.1134979630867\n",
      "The norm of grad vector is  22271.63398258086\n",
      "Loss decreases to  66.10742103758976\n",
      "The norm of grad vector is  22265.336721917036\n",
      "Loss decreases to  66.10134754697715\n",
      "The norm of grad vector is  22259.04935343479\n",
      "Loss decreases to  66.09527748488672\n",
      "The norm of grad vector is  22252.771844057876\n",
      "Loss decreases to  66.08921084497881\n",
      "The norm of grad vector is  22246.504160828725\n",
      "Loss decreases to  66.08314762093643\n",
      "The norm of grad vector is  22240.246270908232\n",
      "Loss decreases to  66.0770878064651\n",
      "The norm of grad vector is  22233.998141574917\n",
      "Loss decreases to  66.07103139529237\n",
      "The norm of grad vector is  22227.759740225007\n",
      "Loss decreases to  66.06497838116822\n",
      "The norm of grad vector is  22221.531034371696\n",
      "Loss decreases to  66.0589287578646\n",
      "The norm of grad vector is  22215.31199164472\n",
      "Loss decreases to  66.05288251917578\n",
      "The norm of grad vector is  22209.102579790168\n",
      "Loss decreases to  66.04683965891743\n",
      "The norm of grad vector is  22202.902766669846\n",
      "Loss decreases to  66.04080017092787\n",
      "The norm of grad vector is  22196.71252026094\n",
      "Loss decreases to  66.03476404906647\n",
      "The norm of grad vector is  22190.53180865567\n",
      "Loss decreases to  66.0287312872146\n",
      "The norm of grad vector is  22184.360600060725\n",
      "Loss decreases to  66.02270187927522\n",
      "The norm of grad vector is  22178.198862797028\n",
      "Loss decreases to  66.0166758191728\n",
      "The norm of grad vector is  22172.046565299217\n",
      "Loss decreases to  66.01065310085323\n",
      "The norm of grad vector is  22165.903676115275\n",
      "Loss decreases to  66.00463371828374\n",
      "The norm of grad vector is  22159.770163906134\n",
      "Loss decreases to  65.99861766545266\n",
      "The norm of grad vector is  22153.645997445226\n",
      "Loss decreases to  65.99260493636959\n",
      "The norm of grad vector is  22147.531145618173\n",
      "Loss decreases to  65.98659552506538\n",
      "The norm of grad vector is  22141.42557742223\n",
      "Loss decreases to  65.98058942559149\n",
      "The norm of grad vector is  22135.329261966188\n",
      "Loss decreases to  65.97458663202055\n",
      "The norm of grad vector is  22129.242168469533\n",
      "Loss decreases to  65.96858713844604\n",
      "The norm of grad vector is  22123.164266262433\n",
      "Loss decreases to  65.96259093898182\n",
      "The norm of grad vector is  22117.095524785218\n",
      "Loss decreases to  65.95659802776305\n",
      "The norm of grad vector is  22111.035913587864\n",
      "Loss decreases to  65.95060839894472\n",
      "The norm of grad vector is  22104.985402329876\n",
      "Loss decreases to  65.94462204670288\n",
      "The norm of grad vector is  22098.943960779656\n",
      "Loss decreases to  65.93863896523344\n",
      "The norm of grad vector is  22092.911558814187\n",
      "Loss decreases to  65.93265914875334\n",
      "The norm of grad vector is  22086.888166418583\n",
      "Loss decreases to  65.92668259149892\n",
      "The norm of grad vector is  22080.8737536859\n",
      "Loss decreases to  65.92070928772752\n",
      "The norm of grad vector is  22074.86829081667\n",
      "Loss decreases to  65.91473923171577\n",
      "The norm of grad vector is  22068.87174811828\n",
      "Loss decreases to  65.90877241776077\n",
      "The norm of grad vector is  22062.884096004975\n",
      "Loss decreases to  65.90280884017953\n",
      "The norm of grad vector is  22056.90530499718\n",
      "Loss decreases to  65.89684849330864\n",
      "The norm of grad vector is  22050.935345721307\n",
      "Loss decreases to  65.89089137150447\n",
      "The norm of grad vector is  22044.97418890927\n",
      "Loss decreases to  65.88493746914332\n",
      "The norm of grad vector is  22039.02180539817\n",
      "Loss decreases to  65.87898678062089\n",
      "The norm of grad vector is  22033.078166129828\n",
      "Loss decreases to  65.87303930035237\n",
      "The norm of grad vector is  22027.143242150632\n",
      "Loss decreases to  65.86709502277233\n",
      "The norm of grad vector is  22021.217004610822\n",
      "Loss decreases to  65.86115394233498\n",
      "The norm of grad vector is  22015.299424764347\n",
      "Loss decreases to  65.85521605351327\n",
      "The norm of grad vector is  22009.390473968717\n",
      "Loss decreases to  65.84928135080006\n",
      "The norm of grad vector is  22003.490123684034\n",
      "Loss decreases to  65.84334982870672\n",
      "The norm of grad vector is  21997.598345473194\n",
      "Loss decreases to  65.83742148176395\n",
      "The norm of grad vector is  21991.71511100123\n",
      "Loss decreases to  65.83149630452125\n",
      "The norm of grad vector is  21985.840392035007\n",
      "Loss decreases to  65.82557429154734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  21979.97416044288\n",
      "Loss decreases to  65.81965543742932\n",
      "The norm of grad vector is  21974.11638819441\n",
      "Loss decreases to  65.81373973677324\n",
      "The norm of grad vector is  21968.26704735978\n",
      "Loss decreases to  65.80782718420394\n",
      "The norm of grad vector is  21962.4261101097\n",
      "Loss decreases to  65.80191777436458\n",
      "The norm of grad vector is  21956.593548714874\n",
      "Loss decreases to  65.79601150191692\n",
      "The norm of grad vector is  21950.769335545712\n",
      "Loss decreases to  65.7901083615414\n",
      "The norm of grad vector is  21944.95344307195\n",
      "Loss decreases to  65.78420834793633\n",
      "The norm of grad vector is  21939.145843862378\n",
      "Loss decreases to  65.77831145581872\n",
      "The norm of grad vector is  21933.346510584295\n",
      "Loss decreases to  65.77241767992372\n",
      "The norm of grad vector is  21927.5554160034\n",
      "Loss decreases to  65.76652701500447\n",
      "The norm of grad vector is  21921.772532983287\n",
      "Loss decreases to  65.76063945583228\n",
      "The norm of grad vector is  21915.997834485137\n",
      "Loss decreases to  65.75475499719647\n",
      "The norm of grad vector is  21910.2312935674\n",
      "Loss decreases to  65.74887363390421\n",
      "The norm of grad vector is  21904.472883385384\n",
      "Loss decreases to  65.74299536078072\n",
      "The norm of grad vector is  21898.72257719098\n",
      "Loss decreases to  65.73712017266863\n",
      "The norm of grad vector is  21892.980348332258\n",
      "Loss decreases to  65.73124806442857\n",
      "The norm of grad vector is  21887.24617025321\n",
      "Loss decreases to  65.72537903093873\n",
      "The norm of grad vector is  21881.520016493254\n",
      "Loss decreases to  65.71951306709491\n",
      "The norm of grad vector is  21875.80186068716\n",
      "Loss decreases to  65.71365016781029\n",
      "The norm of grad vector is  21870.09167656441\n",
      "Loss decreases to  65.70779032801563\n",
      "The norm of grad vector is  21864.38943794908\n",
      "Loss decreases to  65.70193354265892\n",
      "The norm of grad vector is  21858.695118759326\n",
      "Loss decreases to  65.69607980670538\n",
      "The norm of grad vector is  21853.008693007316\n",
      "Loss decreases to  65.69022911513767\n",
      "The norm of grad vector is  21847.33013479855\n",
      "Loss decreases to  65.68438146295529\n",
      "The norm of grad vector is  21841.659418331867\n",
      "Loss decreases to  65.67853684517509\n",
      "The norm of grad vector is  21835.996517898773\n",
      "Loss decreases to  65.67269525683098\n",
      "The norm of grad vector is  21830.341407883552\n",
      "Loss decreases to  65.66685669297348\n",
      "The norm of grad vector is  21824.694062762417\n",
      "Loss decreases to  65.66102114867006\n",
      "The norm of grad vector is  21819.054457103553\n",
      "Loss decreases to  65.65518861900526\n",
      "The norm of grad vector is  21813.422565566765\n",
      "Loss decreases to  65.6493590990803\n",
      "The norm of grad vector is  21807.798362902933\n",
      "Loss decreases to  65.64353258401275\n",
      "The norm of grad vector is  21802.181823953906\n",
      "Loss decreases to  65.6377090689371\n",
      "The norm of grad vector is  21796.572923652144\n",
      "Loss decreases to  65.6318885490042\n",
      "The norm of grad vector is  21790.971637020204\n",
      "Loss decreases to  65.62607101938148\n",
      "The norm of grad vector is  21785.377939170678\n",
      "Loss decreases to  65.62025647525294\n",
      "The norm of grad vector is  21779.791805305795\n",
      "Loss decreases to  65.61444491181842\n",
      "The norm of grad vector is  21774.213210717007\n",
      "Loss decreases to  65.60863632429454\n",
      "The norm of grad vector is  21768.642130784694\n",
      "Loss decreases to  65.60283070791387\n",
      "The norm of grad vector is  21763.07854097801\n",
      "Loss decreases to  65.59702805792512\n",
      "The norm of grad vector is  21757.52241685444\n",
      "Loss decreases to  65.59122836959322\n",
      "The norm of grad vector is  21751.973734059367\n",
      "Loss decreases to  65.58543163819903\n",
      "The norm of grad vector is  21746.432468325984\n",
      "Loss decreases to  65.57963785903932\n",
      "The norm of grad vector is  21740.89859547492\n",
      "Loss decreases to  65.57384702742686\n",
      "The norm of grad vector is  21735.37209141383\n",
      "Loss decreases to  65.56805913869027\n",
      "The norm of grad vector is  21729.85293213718\n",
      "Loss decreases to  65.56227418817355\n",
      "The norm of grad vector is  21724.341093725947\n",
      "Loss decreases to  65.55649217123698\n",
      "The norm of grad vector is  21718.836552347166\n",
      "Loss decreases to  65.55071308325604\n",
      "The norm of grad vector is  21713.339284253867\n",
      "Loss decreases to  65.54493691962199\n",
      "The norm of grad vector is  21707.84926578461\n",
      "Loss decreases to  65.53916367574155\n",
      "The norm of grad vector is  21702.3664733632\n",
      "Loss decreases to  65.53339334703688\n",
      "The norm of grad vector is  21696.89088349828\n",
      "Loss decreases to  65.52762592894543\n",
      "The norm of grad vector is  21691.422472783364\n",
      "Loss decreases to  65.5218614169202\n",
      "The norm of grad vector is  21685.96121789614\n",
      "Loss decreases to  65.51609980642931\n",
      "The norm of grad vector is  21680.507095598405\n",
      "Loss decreases to  65.51034109295611\n",
      "The norm of grad vector is  21675.060082735778\n",
      "Loss decreases to  65.50458527199893\n",
      "The norm of grad vector is  21669.620156237183\n",
      "Loss decreases to  65.49883233907151\n",
      "The norm of grad vector is  21664.187293114876\n",
      "Loss decreases to  65.49308228970253\n",
      "The norm of grad vector is  21658.761470463854\n",
      "Loss decreases to  65.48733511943536\n",
      "The norm of grad vector is  21653.34266546171\n",
      "Loss decreases to  65.48159082382857\n",
      "The norm of grad vector is  21647.930855368384\n",
      "Loss decreases to  65.47584939845534\n",
      "The norm of grad vector is  21642.526017525648\n",
      "Loss decreases to  65.47011083890396\n",
      "The norm of grad vector is  21637.128129357174\n",
      "Loss decreases to  65.46437514077714\n",
      "The norm of grad vector is  21631.737168367843\n",
      "Loss decreases to  65.4586422996923\n",
      "The norm of grad vector is  21626.353112143835\n",
      "Loss decreases to  65.45291231128164\n",
      "The norm of grad vector is  21620.975938351938\n",
      "Loss decreases to  65.44718517119175\n",
      "The norm of grad vector is  21615.60562473971\n",
      "Loss decreases to  65.44146087508392\n",
      "The norm of grad vector is  21610.24214913481\n",
      "Loss decreases to  65.43573941863357\n",
      "The norm of grad vector is  21604.88548944489\n",
      "Loss decreases to  65.43002079753067\n",
      "The norm of grad vector is  21599.535623657346\n",
      "Loss decreases to  65.42430500747955\n",
      "The norm of grad vector is  21594.192529838925\n",
      "Loss decreases to  65.41859204419873\n",
      "The norm of grad vector is  21588.856186135505\n",
      "Loss decreases to  65.412881903421\n",
      "The norm of grad vector is  21583.526570771795\n",
      "Loss decreases to  65.40717458089318\n",
      "The norm of grad vector is  21578.203662051103\n",
      "Loss decreases to  65.4014700723763\n",
      "The norm of grad vector is  21572.887438354934\n",
      "Loss decreases to  65.39576837364557\n",
      "The norm of grad vector is  21567.577878142896\n",
      "Loss decreases to  65.39006948048973\n",
      "The norm of grad vector is  21562.274959952225\n",
      "Loss decreases to  65.3843733887119\n",
      "The norm of grad vector is  21556.978662397723\n",
      "Loss decreases to  65.37868009412888\n",
      "The norm of grad vector is  21551.688964171208\n",
      "Loss decreases to  65.37298959257127\n",
      "The norm of grad vector is  21546.405844041514\n",
      "Loss decreases to  65.3673018798835\n",
      "The norm of grad vector is  21541.12928085413\n",
      "Loss decreases to  65.36161695192374\n",
      "The norm of grad vector is  21535.859253530754\n",
      "Loss decreases to  65.35593480456363\n",
      "The norm of grad vector is  21530.595741069268\n",
      "Loss decreases to  65.35025543368866\n",
      "The norm of grad vector is  21525.338722543373\n",
      "Loss decreases to  65.34457883519771\n",
      "The norm of grad vector is  21520.088177102247\n",
      "Loss decreases to  65.33890500500321\n",
      "The norm of grad vector is  21514.844083970405\n",
      "Loss decreases to  65.33323393903096\n",
      "The norm of grad vector is  21509.606422447338\n",
      "Loss decreases to  65.32756563322012\n",
      "The norm of grad vector is  21504.375171907264\n",
      "Loss decreases to  65.3219000835233\n",
      "The norm of grad vector is  21499.150311798952\n",
      "Loss decreases to  65.31623728590647\n",
      "The norm of grad vector is  21493.93182164522\n",
      "Loss decreases to  65.3105772363486\n",
      "The norm of grad vector is  21488.71968104298\n",
      "Loss decreases to  65.30491993084189\n",
      "The norm of grad vector is  21483.51386966277\n",
      "Loss decreases to  65.29926536539183\n",
      "The norm of grad vector is  21478.31436724855\n",
      "Loss decreases to  65.29361353601668\n",
      "The norm of grad vector is  21473.12115361743\n",
      "Loss decreases to  65.2879644387481\n",
      "The norm of grad vector is  21467.93420865942\n",
      "Loss decreases to  65.28231806963045\n",
      "The norm of grad vector is  21462.753512337178\n",
      "Loss decreases to  65.27667442472101\n",
      "The norm of grad vector is  21457.579044685757\n",
      "Loss decreases to  65.27103350009\n",
      "The norm of grad vector is  21452.410785812317\n",
      "Loss decreases to  65.26539529182054\n",
      "The norm of grad vector is  21447.248715895865\n",
      "Loss decreases to  65.25975979600832\n",
      "The norm of grad vector is  21442.092815186978\n",
      "Loss decreases to  65.25412700876184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  21436.943064007784\n",
      "Loss decreases to  65.24849692620242\n",
      "The norm of grad vector is  21431.799442751282\n",
      "Loss decreases to  65.24286954446356\n",
      "The norm of grad vector is  21426.661931881397\n",
      "Loss decreases to  65.23724485969194\n",
      "The norm of grad vector is  21421.53051193272\n",
      "Loss decreases to  65.23162286804612\n",
      "The norm of grad vector is  21416.405163510073\n",
      "Loss decreases to  65.2260035656976\n",
      "The norm of grad vector is  21411.2858672885\n",
      "Loss decreases to  65.22038694883004\n",
      "The norm of grad vector is  21406.172604012743\n",
      "Loss decreases to  65.21477301363957\n",
      "The norm of grad vector is  21401.06535449728\n",
      "Loss decreases to  65.2091617563345\n",
      "The norm of grad vector is  21395.96409962584\n",
      "Loss decreases to  65.20355317313579\n",
      "The norm of grad vector is  21390.86882035133\n",
      "Loss decreases to  65.19794726027607\n",
      "The norm of grad vector is  21385.779497695406\n",
      "Loss decreases to  65.19234401400051\n",
      "The norm of grad vector is  21380.696112748446\n",
      "Loss decreases to  65.18674343056645\n",
      "The norm of grad vector is  21375.618646669125\n",
      "Loss decreases to  65.18114550624294\n",
      "The norm of grad vector is  21370.547080684297\n",
      "Loss decreases to  65.1755502373115\n",
      "The norm of grad vector is  21365.481396088606\n",
      "Loss decreases to  65.16995762006533\n",
      "The norm of grad vector is  21360.421574244378\n",
      "Loss decreases to  65.16436765080974\n",
      "The norm of grad vector is  21355.36759658144\n",
      "Loss decreases to  65.15878032586167\n",
      "The norm of grad vector is  21350.31944459658\n",
      "Loss decreases to  65.15319564155021\n",
      "The norm of grad vector is  21345.277099853636\n",
      "Loss decreases to  65.14761359421611\n",
      "The norm of grad vector is  21340.24054398311\n",
      "Loss decreases to  65.14203418021181\n",
      "The norm of grad vector is  21335.209758681853\n",
      "Loss decreases to  65.13645739590162\n",
      "The norm of grad vector is  21330.184725713098\n",
      "Loss decreases to  65.13088323766128\n",
      "The norm of grad vector is  21325.165426905794\n",
      "Loss decreases to  65.12531170187825\n",
      "The norm of grad vector is  21320.151844154905\n",
      "Loss decreases to  65.11974278495171\n",
      "The norm of grad vector is  21315.143959420668\n",
      "Loss decreases to  65.1141764832921\n",
      "The norm of grad vector is  21310.14175472866\n",
      "Loss decreases to  65.10861279332134\n",
      "The norm of grad vector is  21305.145212169584\n",
      "Loss decreases to  65.10305171147317\n",
      "The norm of grad vector is  21300.154313898787\n",
      "Loss decreases to  65.09749323419211\n",
      "The norm of grad vector is  21295.16904213627\n",
      "Loss decreases to  65.09193735793457\n",
      "The norm of grad vector is  21290.189379166317\n",
      "Loss decreases to  65.08638407916777\n",
      "The norm of grad vector is  21285.21530733744\n",
      "Loss decreases to  65.08083339437059\n",
      "The norm of grad vector is  21280.246809061853\n",
      "Loss decreases to  65.07528530003309\n",
      "The norm of grad vector is  21275.283866815607\n",
      "Loss decreases to  65.06973979265605\n",
      "The norm of grad vector is  21270.326463137993\n",
      "Loss decreases to  65.06419686875189\n",
      "The norm of grad vector is  21265.374580631677\n",
      "Loss decreases to  65.05865652484374\n",
      "The norm of grad vector is  21260.42820196215\n",
      "Loss decreases to  65.05311875746612\n",
      "The norm of grad vector is  21255.48730985773\n",
      "Loss decreases to  65.04758356316401\n",
      "The norm of grad vector is  21250.55188710929\n",
      "Loss decreases to  65.0420509384939\n",
      "The norm of grad vector is  21245.621916569875\n",
      "Loss decreases to  65.0365208800228\n",
      "The norm of grad vector is  21240.69738115473\n",
      "Loss decreases to  65.03099338432874\n",
      "The norm of grad vector is  21235.778263840853\n",
      "Loss decreases to  65.02546844800052\n",
      "The norm of grad vector is  21230.86454766703\n",
      "Loss decreases to  65.01994606763782\n",
      "The norm of grad vector is  21225.9562157333\n",
      "Loss decreases to  65.01442623985079\n",
      "The norm of grad vector is  21221.053251200963\n",
      "Loss decreases to  65.00890896126046\n",
      "The norm of grad vector is  21216.155637292322\n",
      "Loss decreases to  65.00339422849864\n",
      "The norm of grad vector is  21211.26335729036\n",
      "Loss decreases to  64.9978820382075\n",
      "The norm of grad vector is  21206.376394538715\n",
      "Loss decreases to  64.9923723870398\n",
      "The norm of grad vector is  21201.49473244118\n",
      "Loss decreases to  64.98686527165897\n",
      "The norm of grad vector is  21196.618354461887\n",
      "Loss decreases to  64.98136068873875\n",
      "The norm of grad vector is  21191.747244124657\n",
      "Loss decreases to  64.97585863496353\n",
      "The norm of grad vector is  21186.881385013112\n",
      "Loss decreases to  64.97035910702789\n",
      "The norm of grad vector is  21182.02076077023\n",
      "Loss decreases to  64.9648621016368\n",
      "The norm of grad vector is  21177.165355098372\n",
      "Loss decreases to  64.95936761550576\n",
      "The norm of grad vector is  21172.315151758834\n",
      "Loss decreases to  64.95387564536024\n",
      "The norm of grad vector is  21167.470134571846\n",
      "Loss decreases to  64.94838618793622\n",
      "The norm of grad vector is  21162.63028741612\n",
      "Loss decreases to  64.94289923997974\n",
      "The norm of grad vector is  21157.795594228915\n",
      "Loss decreases to  64.93741479824692\n",
      "The norm of grad vector is  21152.966039005605\n",
      "Loss decreases to  64.93193285950419\n",
      "The norm of grad vector is  21148.141605799538\n",
      "Loss decreases to  64.92645342052798\n",
      "The norm of grad vector is  21143.322278721997\n",
      "Loss decreases to  64.92097647810466\n",
      "The norm of grad vector is  21138.50804194164\n",
      "Loss decreases to  64.9155020290307\n",
      "The norm of grad vector is  21133.69887968464\n",
      "Loss decreases to  64.91003007011257\n",
      "The norm of grad vector is  21128.89477623431\n",
      "Loss decreases to  64.90456059816651\n",
      "The norm of grad vector is  21124.095715930827\n",
      "Loss decreases to  64.89909361001882\n",
      "The norm of grad vector is  21119.301683171252\n",
      "Loss decreases to  64.89362910250559\n",
      "The norm of grad vector is  21114.51266240916\n",
      "Loss decreases to  64.88816707247268\n",
      "The norm of grad vector is  21109.728638154447\n",
      "Loss decreases to  64.88270751677558\n",
      "The norm of grad vector is  21104.949594973186\n",
      "Loss decreases to  64.87725043227985\n",
      "The norm of grad vector is  21100.175517487398\n",
      "Loss decreases to  64.87179581586055\n",
      "The norm of grad vector is  21095.40639037484\n",
      "Loss decreases to  64.86634366440231\n",
      "The norm of grad vector is  21090.642198368834\n",
      "Loss decreases to  64.86089397479941\n",
      "The norm of grad vector is  21085.88292625802\n",
      "Loss decreases to  64.85544674395585\n",
      "The norm of grad vector is  21081.128558886194\n",
      "Loss decreases to  64.85000196878514\n",
      "The norm of grad vector is  21076.379081152187\n",
      "Loss decreases to  64.84455964621006\n",
      "The norm of grad vector is  21071.634478009466\n",
      "Loss decreases to  64.83911977316318\n",
      "The norm of grad vector is  21066.89473446613\n",
      "Loss decreases to  64.83368234658647\n",
      "The norm of grad vector is  21062.159835584604\n",
      "Loss decreases to  64.82824736343089\n",
      "The norm of grad vector is  21057.429766481462\n",
      "Loss decreases to  64.8228148206572\n",
      "The norm of grad vector is  21052.704512327364\n",
      "Loss decreases to  64.81738471523542\n",
      "The norm of grad vector is  21047.984058346654\n",
      "Loss decreases to  64.81195704414463\n",
      "The norm of grad vector is  21043.268389817273\n",
      "Loss decreases to  64.80653180437348\n",
      "The norm of grad vector is  21038.557492070544\n",
      "Loss decreases to  64.8011089929193\n",
      "The norm of grad vector is  21033.851350491062\n",
      "Loss decreases to  64.79568860678937\n",
      "The norm of grad vector is  21029.149950516367\n",
      "Loss decreases to  64.79027064299933\n",
      "The norm of grad vector is  21024.45327763685\n",
      "Loss decreases to  64.78485509857444\n",
      "The norm of grad vector is  21019.761317395532\n",
      "Loss decreases to  64.77944197054886\n",
      "The norm of grad vector is  21015.07405538785\n",
      "Loss decreases to  64.77403125596587\n",
      "The norm of grad vector is  21010.391477261594\n",
      "Loss decreases to  64.76862295187766\n",
      "The norm of grad vector is  21005.713568716445\n",
      "Loss decreases to  64.76321705534517\n",
      "The norm of grad vector is  21001.04031550412\n",
      "Loss decreases to  64.75781356343883\n",
      "The norm of grad vector is  20996.371703428027\n",
      "Loss decreases to  64.75241247323737\n",
      "The norm of grad vector is  20991.70771834293\n",
      "Loss decreases to  64.74701378182878\n",
      "The norm of grad vector is  20987.04834615512\n",
      "Loss decreases to  64.74161748630982\n",
      "The norm of grad vector is  20982.39357282184\n",
      "Loss decreases to  64.73622358378576\n",
      "The norm of grad vector is  20977.743384351437\n",
      "Loss decreases to  64.73083207137105\n",
      "The norm of grad vector is  20973.097766802905\n",
      "Loss decreases to  64.72544294618856\n",
      "The norm of grad vector is  20968.456706285935\n",
      "Loss decreases to  64.72005620536991\n",
      "The norm of grad vector is  20963.82018896057\n",
      "Loss decreases to  64.71467184605542\n",
      "The norm of grad vector is  20959.188201037086\n",
      "Loss decreases to  64.70928986539394\n",
      "The norm of grad vector is  20954.560728775785\n",
      "Loss decreases to  64.7039102605433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  20949.937758486845\n",
      "Loss decreases to  64.69853302866915\n",
      "The norm of grad vector is  20945.3192765302\n",
      "Loss decreases to  64.69315816694638\n",
      "The norm of grad vector is  20940.705269315225\n",
      "Loss decreases to  64.687785672558\n",
      "The norm of grad vector is  20936.095723300517\n",
      "Loss decreases to  64.68241554269551\n",
      "The norm of grad vector is  20931.490624994047\n",
      "Loss decreases to  64.67704777455894\n",
      "The norm of grad vector is  20926.88996095262\n",
      "Loss decreases to  64.67168236535663\n",
      "The norm of grad vector is  20922.29371778184\n",
      "Loss decreases to  64.66631931230528\n",
      "The norm of grad vector is  20917.701882135978\n",
      "Loss decreases to  64.66095861262998\n",
      "The norm of grad vector is  20913.11444071772\n",
      "Loss decreases to  64.65560026356405\n",
      "The norm of grad vector is  20908.53138027809\n",
      "Loss decreases to  64.6502442623491\n",
      "The norm of grad vector is  20903.952687616104\n",
      "Loss decreases to  64.64489060623497\n",
      "The norm of grad vector is  20899.37834957876\n",
      "Loss decreases to  64.63953929247964\n",
      "The norm of grad vector is  20894.80835306087\n",
      "Loss decreases to  64.63419031834937\n",
      "The norm of grad vector is  20890.2426850047\n",
      "Loss decreases to  64.62884368111848\n",
      "The norm of grad vector is  20885.68133240002\n",
      "Loss decreases to  64.62349937806943\n",
      "The norm of grad vector is  20881.124282283778\n",
      "Loss decreases to  64.61815740649267\n",
      "The norm of grad vector is  20876.57152174003\n",
      "Loss decreases to  64.61281776368696\n",
      "The norm of grad vector is  20872.023037899773\n",
      "Loss decreases to  64.6074804469586\n",
      "The norm of grad vector is  20867.478817940653\n",
      "Loss decreases to  64.60214545362217\n",
      "The norm of grad vector is  20862.938849086873\n",
      "Loss decreases to  64.59681278100032\n",
      "The norm of grad vector is  20858.403118609058\n",
      "Loss decreases to  64.59148242642333\n",
      "The norm of grad vector is  20853.87161382412\n",
      "Loss decreases to  64.58615438722963\n",
      "The norm of grad vector is  20849.344322094898\n",
      "Loss decreases to  64.58082866076523\n",
      "The norm of grad vector is  20844.821230830265\n",
      "Loss decreases to  64.5755052443842\n",
      "The norm of grad vector is  20840.30232748475\n",
      "Loss decreases to  64.57018413544827\n",
      "The norm of grad vector is  20835.787599558396\n",
      "Loss decreases to  64.56486533132697\n",
      "The norm of grad vector is  20831.277034596707\n",
      "Loss decreases to  64.5595488293977\n",
      "The norm of grad vector is  20826.77062019045\n",
      "Loss decreases to  64.55423462704529\n",
      "The norm of grad vector is  20822.2683439754\n",
      "Loss decreases to  64.54892272166248\n",
      "The norm of grad vector is  20817.77019363225\n",
      "Loss decreases to  64.5436131106497\n",
      "The norm of grad vector is  20813.27615688646\n",
      "Loss decreases to  64.53830579141463\n",
      "The norm of grad vector is  20808.786221508086\n",
      "Loss decreases to  64.53300076137296\n",
      "The norm of grad vector is  20804.300375311534\n",
      "Loss decreases to  64.52769801794773\n",
      "The norm of grad vector is  20799.818606155557\n",
      "Loss decreases to  64.52239755856944\n",
      "The norm of grad vector is  20795.34090194298\n",
      "Loss decreases to  64.51709938067646\n",
      "The norm of grad vector is  20790.86725062055\n",
      "Loss decreases to  64.51180348171413\n",
      "The norm of grad vector is  20786.397640178857\n",
      "Loss decreases to  64.50650985913556\n",
      "The norm of grad vector is  20781.93205865199\n",
      "Loss decreases to  64.50121851040116\n",
      "The norm of grad vector is  20777.470494117675\n",
      "Loss decreases to  64.49592943297877\n",
      "The norm of grad vector is  20773.012934696868\n",
      "Loss decreases to  64.49064262434354\n",
      "The norm of grad vector is  20768.559368553633\n",
      "Loss decreases to  64.485358081978\n",
      "The norm of grad vector is  20764.10978389513\n",
      "Loss decreases to  64.48007580337197\n",
      "The norm of grad vector is  20759.664168971223\n",
      "Loss decreases to  64.47479578602265\n",
      "The norm of grad vector is  20755.222512074684\n",
      "Loss decreases to  64.46951802743418\n",
      "The norm of grad vector is  20750.784801540616\n",
      "Loss decreases to  64.46424252511822\n",
      "The norm of grad vector is  20746.351025746564\n",
      "Loss decreases to  64.45896927659334\n",
      "The norm of grad vector is  20741.921173112314\n",
      "Loss decreases to  64.45369827938565\n",
      "The norm of grad vector is  20737.495232099758\n",
      "Loss decreases to  64.4484295310281\n",
      "The norm of grad vector is  20733.073191212654\n",
      "Loss decreases to  64.4431630290609\n",
      "The norm of grad vector is  20728.655038996516\n",
      "Loss decreases to  64.43789877103112\n",
      "The norm of grad vector is  20724.2407640386\n",
      "Loss decreases to  64.43263675449315\n",
      "The norm of grad vector is  20719.830354967402\n",
      "Loss decreases to  64.42737697700821\n",
      "The norm of grad vector is  20715.423800453045\n",
      "Loss decreases to  64.42211943614461\n",
      "The norm of grad vector is  20711.021089206497\n",
      "Loss decreases to  64.4168641294776\n",
      "The norm of grad vector is  20706.622209979974\n",
      "Loss decreases to  64.4116110545895\n",
      "The norm of grad vector is  20702.227151566494\n",
      "Loss decreases to  64.40636020906938\n",
      "The norm of grad vector is  20697.835902799772\n",
      "Loss decreases to  64.40111159051324\n",
      "The norm of grad vector is  20693.448452554137\n",
      "Loss decreases to  64.39586519652403\n",
      "The norm of grad vector is  20689.06478974427\n",
      "Loss decreases to  64.3906210247114\n",
      "The norm of grad vector is  20684.68490332526\n",
      "Loss decreases to  64.38537907269193\n",
      "The norm of grad vector is  20680.308782292195\n",
      "Loss decreases to  64.38013933808887\n",
      "The norm of grad vector is  20675.936415680255\n",
      "Loss decreases to  64.37490181853246\n",
      "The norm of grad vector is  20671.567792564467\n",
      "Loss decreases to  64.36966651165935\n",
      "The norm of grad vector is  20667.202902059453\n",
      "Loss decreases to  64.36443341511321\n",
      "The norm of grad vector is  20662.841733319477\n",
      "Loss decreases to  64.35920252654425\n",
      "The norm of grad vector is  20658.484275538187\n",
      "Loss decreases to  64.35397384360918\n",
      "The norm of grad vector is  20654.130517948583\n",
      "Loss decreases to  64.34874736397163\n",
      "The norm of grad vector is  20649.780449822647\n",
      "Loss decreases to  64.34352308530163\n",
      "The norm of grad vector is  20645.434060471413\n",
      "Loss decreases to  64.33830100527604\n",
      "The norm of grad vector is  20641.091339244842\n",
      "Loss decreases to  64.33308112157803\n",
      "The norm of grad vector is  20636.7522755315\n",
      "Loss decreases to  64.32786343189723\n",
      "The norm of grad vector is  20632.416858758537\n",
      "Loss decreases to  64.32264793393013\n",
      "The norm of grad vector is  20628.08507839148\n",
      "Loss decreases to  64.31743462537938\n",
      "The norm of grad vector is  20623.75692393426\n",
      "Loss decreases to  64.31222350395424\n",
      "The norm of grad vector is  20619.43238492886\n",
      "Loss decreases to  64.30701456737037\n",
      "The norm of grad vector is  20615.111450955275\n",
      "Loss decreases to  64.30180781334974\n",
      "The norm of grad vector is  20610.794111631378\n",
      "Loss decreases to  64.29660323962099\n",
      "The norm of grad vector is  20606.480356612843\n",
      "Loss decreases to  64.29140084391878\n",
      "The norm of grad vector is  20602.170175592793\n",
      "Loss decreases to  64.28620062398424\n",
      "The norm of grad vector is  20597.86355830196\n",
      "Loss decreases to  64.28100257756486\n",
      "The norm of grad vector is  20593.560494508274\n",
      "Loss decreases to  64.27580670241444\n",
      "The norm of grad vector is  20589.26097401692\n",
      "Loss decreases to  64.27061299629294\n",
      "The norm of grad vector is  20584.964986670107\n",
      "Loss decreases to  64.2654214569664\n",
      "The norm of grad vector is  20580.672522346955\n",
      "Loss decreases to  64.26023208220764\n",
      "The norm of grad vector is  20576.383570963455\n",
      "Loss decreases to  64.25504486979496\n",
      "The norm of grad vector is  20572.098122472056\n",
      "Loss decreases to  64.24985981751338\n",
      "The norm of grad vector is  20567.816166861892\n",
      "Loss decreases to  64.24467692315378\n",
      "The norm of grad vector is  20563.537694158404\n",
      "Loss decreases to  64.23949618451326\n",
      "The norm of grad vector is  20559.262694423367\n",
      "Loss decreases to  64.23431759939484\n",
      "The norm of grad vector is  20554.99115775453\n",
      "Loss decreases to  64.22914116560798\n",
      "The norm of grad vector is  20550.723074285692\n",
      "Loss decreases to  64.22396688096782\n",
      "The norm of grad vector is  20546.458434186552\n",
      "Loss decreases to  64.21879474329573\n",
      "The norm of grad vector is  20542.19722766249\n",
      "Loss decreases to  64.21362475041913\n",
      "The norm of grad vector is  20537.939444954518\n",
      "Loss decreases to  64.20845690017111\n",
      "The norm of grad vector is  20533.685076339032\n",
      "Loss decreases to  64.20329119039117\n",
      "The norm of grad vector is  20529.43411212789\n",
      "Loss decreases to  64.19812761892427\n",
      "The norm of grad vector is  20525.186542667972\n",
      "Loss decreases to  64.19296618362169\n",
      "The norm of grad vector is  20520.94235834144\n",
      "Loss decreases to  64.18780688234041\n",
      "The norm of grad vector is  20516.701549565325\n",
      "Loss decreases to  64.1826497129433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  20512.464106791424\n",
      "Loss decreases to  64.17749467329912\n",
      "The norm of grad vector is  20508.230020506293\n",
      "Loss decreases to  64.17234176128231\n",
      "The norm of grad vector is  20503.999281231103\n",
      "Loss decreases to  64.16719097477335\n",
      "The norm of grad vector is  20499.77187952134\n",
      "Loss decreases to  64.16204231165823\n",
      "The norm of grad vector is  20495.547805966922\n",
      "Loss decreases to  64.15689576982896\n",
      "The norm of grad vector is  20491.32705119191\n",
      "Loss decreases to  64.15175134718297\n",
      "The norm of grad vector is  20487.109605854457\n",
      "Loss decreases to  64.14660904162385\n",
      "The norm of grad vector is  20482.89546064668\n",
      "Loss decreases to  64.14146885106055\n",
      "The norm of grad vector is  20478.68460629444\n",
      "Loss decreases to  64.1363307734076\n",
      "The norm of grad vector is  20474.477033557385\n",
      "Loss decreases to  64.13119480658557\n",
      "The norm of grad vector is  20470.272733228707\n",
      "Loss decreases to  64.12606094852026\n",
      "The norm of grad vector is  20466.071696135037\n",
      "Loss decreases to  64.12092919714344\n",
      "The norm of grad vector is  20461.873913136347\n",
      "Loss decreases to  64.11579955039213\n",
      "The norm of grad vector is  20457.679375125786\n",
      "Loss decreases to  64.11067200620924\n",
      "The norm of grad vector is  20453.488073029737\n",
      "Loss decreases to  64.1055465625428\n",
      "The norm of grad vector is  20449.29999780733\n",
      "Loss decreases to  64.1004232173468\n",
      "The norm of grad vector is  20445.115140450693\n",
      "Loss decreases to  64.09530196858049\n",
      "The norm of grad vector is  20440.933491984644\n",
      "Loss decreases to  64.09018281420859\n",
      "The norm of grad vector is  20436.75504346659\n",
      "Loss decreases to  64.08506575220144\n",
      "The norm of grad vector is  20432.579785986472\n",
      "Loss decreases to  64.07995078053467\n",
      "The norm of grad vector is  20428.407710666575\n",
      "Loss decreases to  64.07483789718952\n",
      "The norm of grad vector is  20424.2388086614\n",
      "Loss decreases to  64.06972710015228\n",
      "The norm of grad vector is  20420.073071157683\n",
      "Loss decreases to  64.0646183874149\n",
      "The norm of grad vector is  20415.910489374073\n",
      "Loss decreases to  64.05951175697467\n",
      "The norm of grad vector is  20411.751054561206\n",
      "Loss decreases to  64.05440720683411\n",
      "The norm of grad vector is  20407.594758001378\n",
      "Loss decreases to  64.04930473500114\n",
      "The norm of grad vector is  20403.441591008705\n",
      "Loss decreases to  64.04420433948894\n",
      "The norm of grad vector is  20399.291544928794\n",
      "Loss decreases to  64.03910601831603\n",
      "The norm of grad vector is  20395.144611138625\n",
      "Loss decreases to  64.03400976950589\n",
      "The norm of grad vector is  20391.000781046572\n",
      "Loss decreases to  64.02891559108772\n",
      "The norm of grad vector is  20386.86004609226\n",
      "Loss decreases to  64.02382348109573\n",
      "The norm of grad vector is  20382.72239774627\n",
      "Loss decreases to  64.01873343756918\n",
      "The norm of grad vector is  20378.587827510306\n",
      "Loss decreases to  64.01364545855259\n",
      "The norm of grad vector is  20374.456326916858\n",
      "Loss decreases to  64.0085595420956\n",
      "The norm of grad vector is  20370.327887529125\n",
      "Loss decreases to  64.00347568625344\n",
      "The norm of grad vector is  20366.20250094111\n",
      "Loss decreases to  63.99839388908559\n",
      "The norm of grad vector is  20362.080158777233\n",
      "Loss decreases to  63.9933141486574\n",
      "The norm of grad vector is  20357.96085269229\n",
      "Loss decreases to  63.98823646303901\n",
      "The norm of grad vector is  20353.844574371502\n",
      "Loss decreases to  63.983160830305565\n",
      "The norm of grad vector is  20349.731315530174\n",
      "Loss decreases to  63.97808724853732\n",
      "The norm of grad vector is  20345.62106791381\n",
      "Loss decreases to  63.973015715819564\n",
      "The norm of grad vector is  20341.513823297748\n",
      "Loss decreases to  63.967946230242525\n",
      "The norm of grad vector is  20337.409573487355\n",
      "Loss decreases to  63.96287878990164\n",
      "The norm of grad vector is  20333.308310317603\n",
      "Loss decreases to  63.957813392897044\n",
      "The norm of grad vector is  20329.210025653232\n",
      "Loss decreases to  63.952750037333836\n",
      "The norm of grad vector is  20325.114711388447\n",
      "Loss decreases to  63.947688721322244\n",
      "The norm of grad vector is  20321.022359446895\n",
      "Loss decreases to  63.942629442977285\n",
      "The norm of grad vector is  20316.932961781582\n",
      "Loss decreases to  63.937572200418856\n",
      "The norm of grad vector is  20312.84651037472\n",
      "Loss decreases to  63.9325169917718\n",
      "The norm of grad vector is  20308.762997237573\n",
      "Loss decreases to  63.927463815165666\n",
      "The norm of grad vector is  20304.682414410483\n",
      "Loss decreases to  63.922412668735085\n",
      "The norm of grad vector is  20300.604753962656\n",
      "Loss decreases to  63.91736355061926\n",
      "The norm of grad vector is  20296.530007992103\n",
      "Loss decreases to  63.91231645896229\n",
      "The norm of grad vector is  20292.458168625482\n",
      "Loss decreases to  63.907271391913156\n",
      "The norm of grad vector is  20288.38922801805\n",
      "Loss decreases to  63.90222834762547\n",
      "The norm of grad vector is  20284.323178353578\n",
      "Loss decreases to  63.897187324257665\n",
      "The norm of grad vector is  20280.260011844144\n",
      "Loss decreases to  63.892148319972854\n",
      "The norm of grad vector is  20276.19972073012\n",
      "Loss decreases to  63.887111332938915\n",
      "The norm of grad vector is  20272.142297280086\n",
      "Loss decreases to  63.8820763613284\n",
      "The norm of grad vector is  20268.087733790635\n",
      "Loss decreases to  63.87704340331853\n",
      "The norm of grad vector is  20264.036022586257\n",
      "Loss decreases to  63.87201245709107\n",
      "The norm of grad vector is  20259.987156019477\n",
      "Loss decreases to  63.86698352083275\n",
      "The norm of grad vector is  20255.941126470374\n",
      "Loss decreases to  63.86195659273453\n",
      "The norm of grad vector is  20251.897926346734\n",
      "Loss decreases to  63.856931670992424\n",
      "The norm of grad vector is  20247.857548084034\n",
      "Loss decreases to  63.85190875380653\n",
      "The norm of grad vector is  20243.81998414501\n",
      "Loss decreases to  63.84688783938186\n",
      "The norm of grad vector is  20239.785227019813\n",
      "Loss decreases to  63.84186892592789\n",
      "The norm of grad vector is  20235.7532692259\n",
      "Loss decreases to  63.83685201165875\n",
      "The norm of grad vector is  20231.72410330784\n",
      "Loss decreases to  63.83183709479275\n",
      "The norm of grad vector is  20227.697721837136\n",
      "Loss decreases to  63.82682417355309\n",
      "The norm of grad vector is  20223.674117412455\n",
      "Loss decreases to  63.8218132461672\n",
      "The norm of grad vector is  20219.65328265917\n",
      "Loss decreases to  63.816804310867205\n",
      "The norm of grad vector is  20215.635210229382\n",
      "Loss decreases to  63.8117973658895\n",
      "The norm of grad vector is  20211.619892801962\n",
      "Loss decreases to  63.806792409475065\n",
      "The norm of grad vector is  20207.60732308222\n",
      "Loss decreases to  63.801789439869104\n",
      "The norm of grad vector is  20203.597493802004\n",
      "Loss decreases to  63.796788455321426\n",
      "The norm of grad vector is  20199.590397719498\n",
      "Loss decreases to  63.79178945408597\n",
      "The norm of grad vector is  20195.586027619123\n",
      "Loss decreases to  63.78679243442144\n",
      "The norm of grad vector is  20191.58437631152\n",
      "Loss decreases to  63.78179739459057\n",
      "The norm of grad vector is  20187.58543663334\n",
      "Loss decreases to  63.77680433286053\n",
      "The norm of grad vector is  20183.58920144726\n",
      "Loss decreases to  63.771813247502955\n",
      "The norm of grad vector is  20179.5956636418\n",
      "Loss decreases to  63.76682413679343\n",
      "The norm of grad vector is  20175.604816131334\n",
      "Loss decreases to  63.76183699901211\n",
      "The norm of grad vector is  20171.61665185584\n",
      "Loss decreases to  63.75685183244354\n",
      "The norm of grad vector is  20167.631163780898\n",
      "Loss decreases to  63.75186863537628\n",
      "The norm of grad vector is  20163.648344897636\n",
      "Loss decreases to  63.746887406103106\n",
      "The norm of grad vector is  20159.668188222593\n",
      "Loss decreases to  63.74190814292118\n",
      "The norm of grad vector is  20155.69068679762\n",
      "Loss decreases to  63.73693084413181\n",
      "The norm of grad vector is  20151.71583368971\n",
      "Loss decreases to  63.73195550804058\n",
      "The norm of grad vector is  20147.74362199109\n",
      "Loss decreases to  63.726982132957126\n",
      "The norm of grad vector is  20143.77404481896\n",
      "Loss decreases to  63.722010717195424\n",
      "The norm of grad vector is  20139.80709531549\n",
      "Loss decreases to  63.717041259073284\n",
      "The norm of grad vector is  20135.842766647726\n",
      "Loss decreases to  63.71207375691301\n",
      "The norm of grad vector is  20131.881052007397\n",
      "Loss decreases to  63.70710820904075\n",
      "The norm of grad vector is  20127.921944610967\n",
      "Loss decreases to  63.70214461378692\n",
      "The norm of grad vector is  20123.965437699477\n",
      "Loss decreases to  63.697182969486\n",
      "The norm of grad vector is  20120.01152453845\n",
      "Loss decreases to  63.692223274476525\n",
      "The norm of grad vector is  20116.06019841782\n",
      "Loss decreases to  63.68726552710102\n",
      "The norm of grad vector is  20112.111452651712\n",
      "Loss decreases to  63.68230972570623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  20108.165280578665\n",
      "Loss decreases to  63.67735586864269\n",
      "The norm of grad vector is  20104.22167556124\n",
      "Loss decreases to  63.6724039542651\n",
      "The norm of grad vector is  20100.28063098604\n",
      "Loss decreases to  63.667453980932144\n",
      "The norm of grad vector is  20096.342140263598\n",
      "Loss decreases to  63.66250594700648\n",
      "The norm of grad vector is  20092.40619682841\n",
      "Loss decreases to  63.6575598508548\n",
      "The norm of grad vector is  20088.472794138674\n",
      "Loss decreases to  63.65261569084753\n",
      "The norm of grad vector is  20084.54192567625\n",
      "Loss decreases to  63.64767346535927\n",
      "The norm of grad vector is  20080.61358494673\n",
      "Loss decreases to  63.642733172768445\n",
      "The norm of grad vector is  20076.687765479033\n",
      "Loss decreases to  63.637794811457404\n",
      "The norm of grad vector is  20072.764460825638\n",
      "Loss decreases to  63.63285837981237\n",
      "The norm of grad vector is  20068.843664562395\n",
      "Loss decreases to  63.627923876223484\n",
      "The norm of grad vector is  20064.92537028825\n",
      "Loss decreases to  63.62299129908477\n",
      "The norm of grad vector is  20061.00957162549\n",
      "Loss decreases to  63.61806064679404\n",
      "The norm of grad vector is  20057.096262219427\n",
      "Loss decreases to  63.61313191775289\n",
      "The norm of grad vector is  20053.185435738316\n",
      "Loss decreases to  63.60820511036701\n",
      "The norm of grad vector is  20049.27708587345\n",
      "Loss decreases to  63.603280223045665\n",
      "The norm of grad vector is  20045.371206338797\n",
      "Loss decreases to  63.598357254201794\n",
      "The norm of grad vector is  20041.467790871164\n",
      "Loss decreases to  63.593436202252484\n",
      "The norm of grad vector is  20037.566833230096\n",
      "Loss decreases to  63.5885170656184\n",
      "The norm of grad vector is  20033.668327197538\n",
      "Loss decreases to  63.5835998427238\n",
      "The norm of grad vector is  20029.772266578053\n",
      "Loss decreases to  63.578684531997055\n",
      "The norm of grad vector is  20025.878645198638\n",
      "Loss decreases to  63.573771131869904\n",
      "The norm of grad vector is  20021.987456908526\n",
      "Loss decreases to  63.5688596407779\n",
      "The norm of grad vector is  20018.09869557925\n",
      "Loss decreases to  63.56395005716037\n",
      "The norm of grad vector is  20014.212355104537\n",
      "Loss decreases to  63.559042379460394\n",
      "The norm of grad vector is  20010.328429400146\n",
      "Loss decreases to  63.55413660612462\n",
      "The norm of grad vector is  20006.44691240387\n",
      "Loss decreases to  63.5492327356033\n",
      "The norm of grad vector is  20002.567798075426\n",
      "Loss decreases to  63.54433076635032\n",
      "The norm of grad vector is  19998.691080396336\n",
      "Loss decreases to  63.53943069682331\n",
      "The norm of grad vector is  19994.816753369923\n",
      "Loss decreases to  63.53453252548364\n",
      "The norm of grad vector is  19990.944811021174\n",
      "Loss decreases to  63.52963625079588\n",
      "The norm of grad vector is  19987.07524739669\n",
      "Loss decreases to  63.52474187122859\n",
      "The norm of grad vector is  19983.20805656455\n",
      "Loss decreases to  63.519849385253686\n",
      "The norm of grad vector is  19979.34323261434\n",
      "Loss decreases to  63.51495879134666\n",
      "The norm of grad vector is  19975.480769656944\n",
      "Loss decreases to  63.510070087986726\n",
      "The norm of grad vector is  19971.62066182458\n",
      "Loss decreases to  63.50518327365651\n",
      "The norm of grad vector is  19967.76290327067\n",
      "Loss decreases to  63.50029834684209\n",
      "The norm of grad vector is  19963.907488169658\n",
      "Loss decreases to  63.49541530603313\n",
      "The norm of grad vector is  19960.0544107172\n",
      "Loss decreases to  63.49053414972292\n",
      "The norm of grad vector is  19956.203665129815\n",
      "Loss decreases to  63.48565487640803\n",
      "The norm of grad vector is  19952.355245644954\n",
      "Loss decreases to  63.4807774845887\n",
      "The norm of grad vector is  19948.509146520846\n",
      "Loss decreases to  63.47590197276832\n",
      "The norm of grad vector is  19944.665362036507\n",
      "Loss decreases to  63.47102833945414\n",
      "The norm of grad vector is  19940.823886491584\n",
      "Loss decreases to  63.46615658315661\n",
      "The norm of grad vector is  19936.984714206326\n",
      "Loss decreases to  63.46128670238963\n",
      "The norm of grad vector is  19933.147839521484\n",
      "Loss decreases to  63.45641869567046\n",
      "The norm of grad vector is  19929.313256798225\n",
      "Loss decreases to  63.45155256151977\n",
      "The norm of grad vector is  19925.48096041814\n",
      "Loss decreases to  63.44668829846185\n",
      "The norm of grad vector is  19921.650944782996\n",
      "Loss decreases to  63.441825905024054\n",
      "The norm of grad vector is  19917.823204314904\n",
      "Loss decreases to  63.43696537973732\n",
      "The norm of grad vector is  19913.997733455944\n",
      "Loss decreases to  63.432106721135675\n",
      "The norm of grad vector is  19910.174526668445\n",
      "Loss decreases to  63.427249927756705\n",
      "The norm of grad vector is  19906.353578434544\n",
      "Loss decreases to  63.42239499814142\n",
      "The norm of grad vector is  19902.534883256416\n",
      "Loss decreases to  63.41754193083379\n",
      "The norm of grad vector is  19898.71843565603\n",
      "Loss decreases to  63.412690724381385\n",
      "The norm of grad vector is  19894.904230175078\n",
      "Loss decreases to  63.40784137733491\n",
      "The norm of grad vector is  19891.09226137501\n",
      "Loss decreases to  63.402993888248446\n",
      "The norm of grad vector is  19887.28252383683\n",
      "Loss decreases to  63.39814825567919\n",
      "The norm of grad vector is  19883.475012161194\n",
      "Loss decreases to  63.39330447818788\n",
      "The norm of grad vector is  19879.669720968122\n",
      "Loss decreases to  63.38846255433811\n",
      "The norm of grad vector is  19875.866644897043\n",
      "Loss decreases to  63.38362248269695\n",
      "The norm of grad vector is  19872.065778606782\n",
      "Loss decreases to  63.37878426183478\n",
      "The norm of grad vector is  19868.267116775398\n",
      "Loss decreases to  63.37394789032478\n",
      "The norm of grad vector is  19864.470654100096\n",
      "Loss decreases to  63.369113366743846\n",
      "The norm of grad vector is  19860.67638529726\n",
      "Loss decreases to  63.364280689671645\n",
      "The norm of grad vector is  19856.884305102234\n",
      "Loss decreases to  63.35944985769118\n",
      "The norm of grad vector is  19853.09440826947\n",
      "Loss decreases to  63.3546208693887\n",
      "The norm of grad vector is  19849.306689572186\n",
      "Loss decreases to  63.34979372335341\n",
      "The norm of grad vector is  19845.521143802533\n",
      "Loss decreases to  63.34496841817782\n",
      "The norm of grad vector is  19841.73776577134\n",
      "Loss decreases to  63.34014495245742\n",
      "The norm of grad vector is  19837.956550308212\n",
      "Loss decreases to  63.335323324790814\n",
      "The norm of grad vector is  19834.177492261355\n",
      "Loss decreases to  63.33050353377995\n",
      "The norm of grad vector is  19830.40058649749\n",
      "Loss decreases to  63.325685578029585\n",
      "The norm of grad vector is  19826.625827901906\n",
      "Loss decreases to  63.3208694561478\n",
      "The norm of grad vector is  19822.853211378213\n",
      "Loss decreases to  63.316055166745386\n",
      "The norm of grad vector is  19819.082731848473\n",
      "Loss decreases to  63.3112427084366\n",
      "The norm of grad vector is  19815.31438425296\n",
      "Loss decreases to  63.30643207983846\n",
      "The norm of grad vector is  19811.54816355023\n",
      "Loss decreases to  63.30162327957115\n",
      "The norm of grad vector is  19807.784064716892\n",
      "Loss decreases to  63.29681630625785\n",
      "The norm of grad vector is  19804.022082747735\n",
      "Loss decreases to  63.29201115852488\n",
      "The norm of grad vector is  19800.262212655485\n",
      "Loss decreases to  63.28720783500114\n",
      "The norm of grad vector is  19796.50444947089\n",
      "Loss decreases to  63.28240633431909\n",
      "The norm of grad vector is  19792.748788242512\n",
      "Loss decreases to  63.27760665511382\n",
      "The norm of grad vector is  19788.995224036764\n",
      "Loss decreases to  63.27280879602336\n",
      "The norm of grad vector is  19785.24375193783\n",
      "Loss decreases to  63.26801275568901\n",
      "The norm of grad vector is  19781.494367047475\n",
      "Loss decreases to  63.26321853275468\n",
      "The norm of grad vector is  19777.74706448522\n",
      "Loss decreases to  63.258426125867345\n",
      "The norm of grad vector is  19774.00183938803\n",
      "Loss decreases to  63.25363553367695\n",
      "The norm of grad vector is  19770.258686910453\n",
      "Loss decreases to  63.2488467548364\n",
      "The norm of grad vector is  19766.51760222433\n",
      "Loss decreases to  63.244059788001344\n",
      "The norm of grad vector is  19762.778580519014\n",
      "Loss decreases to  63.23927463183033\n",
      "The norm of grad vector is  19759.041617001\n",
      "Loss decreases to  63.23449128498512\n",
      "The norm of grad vector is  19755.306706894164\n",
      "Loss decreases to  63.22970974612986\n",
      "The norm of grad vector is  19751.57384543938\n",
      "Loss decreases to  63.22493001393195\n",
      "The norm of grad vector is  19747.8430278948\n",
      "Loss decreases to  63.220152087061514\n",
      "The norm of grad vector is  19744.114249535476\n",
      "Loss decreases to  63.21537596419142\n",
      "The norm of grad vector is  19740.387505653503\n",
      "Loss decreases to  63.21060164399749\n",
      "The norm of grad vector is  19736.662791557876\n",
      "Loss decreases to  63.20582912515835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  19732.940102574412\n",
      "Loss decreases to  63.201058406355415\n",
      "The norm of grad vector is  19729.21943404577\n",
      "Loss decreases to  63.19628948627293\n",
      "The norm of grad vector is  19725.500781331266\n",
      "Loss decreases to  63.19152236359789\n",
      "The norm of grad vector is  19721.78413980693\n",
      "Loss decreases to  63.186757037020215\n",
      "The norm of grad vector is  19718.0695048654\n",
      "Loss decreases to  63.18199350523237\n",
      "The norm of grad vector is  19714.356871915814\n",
      "Loss decreases to  63.1772317669297\n",
      "The norm of grad vector is  19710.64623638378\n",
      "Loss decreases to  63.17247182081033\n",
      "The norm of grad vector is  19706.937593711395\n",
      "Loss decreases to  63.16771366557509\n",
      "The norm of grad vector is  19703.230939357043\n",
      "Loss decreases to  63.162957299927605\n",
      "The norm of grad vector is  19699.526268795405\n",
      "Loss decreases to  63.1582027225741\n",
      "The norm of grad vector is  19695.823577517436\n",
      "Loss decreases to  63.15344993222372\n",
      "The norm of grad vector is  19692.122861030253\n",
      "Loss decreases to  63.148698927588\n",
      "The norm of grad vector is  19688.424114857076\n",
      "Loss decreases to  63.14394970738146\n",
      "The norm of grad vector is  19684.72733453718\n",
      "Loss decreases to  63.139202270321256\n",
      "The norm of grad vector is  19681.032515625808\n",
      "Loss decreases to  63.13445661512706\n",
      "The norm of grad vector is  19677.339653694176\n",
      "Loss decreases to  63.12971274052125\n",
      "The norm of grad vector is  19673.648744329446\n",
      "Loss decreases to  63.12497064522905\n",
      "The norm of grad vector is  19669.95978313443\n",
      "Loss decreases to  63.12023032797824\n",
      "The norm of grad vector is  19666.272765727823\n",
      "Loss decreases to  63.11549178749905\n",
      "The norm of grad vector is  19662.587687743977\n",
      "Loss decreases to  63.11075502252467\n",
      "The norm of grad vector is  19658.904544832913\n",
      "Loss decreases to  63.10602003179055\n",
      "The norm of grad vector is  19655.22333266014\n",
      "Loss decreases to  63.10128681403492\n",
      "The norm of grad vector is  19651.544046906918\n",
      "Loss decreases to  63.09655536799882\n",
      "The norm of grad vector is  19647.866683269694\n",
      "Loss decreases to  63.091825692425545\n",
      "The norm of grad vector is  19644.191237460524\n",
      "Loss decreases to  63.08709778606101\n",
      "The norm of grad vector is  19640.517705206694\n",
      "Loss decreases to  63.08237164765413\n",
      "The norm of grad vector is  19636.846082250926\n",
      "Loss decreases to  63.077647275955584\n",
      "The norm of grad vector is  19633.176364351002\n",
      "Loss decreases to  63.07292466971942\n",
      "The norm of grad vector is  19629.508547280093\n",
      "Loss decreases to  63.06820382770182\n",
      "The norm of grad vector is  19625.842626826307\n",
      "Loss decreases to  63.06348474866138\n",
      "The norm of grad vector is  19622.1785987929\n",
      "Loss decreases to  63.05876743135967\n",
      "The norm of grad vector is  19618.51645899821\n",
      "Loss decreases to  63.054051874560336\n",
      "The norm of grad vector is  19614.856203275398\n",
      "Loss decreases to  63.04933807702977\n",
      "The norm of grad vector is  19611.19782747264\n",
      "Loss decreases to  63.04462603753689\n",
      "The norm of grad vector is  19607.54132745292\n",
      "Loss decreases to  63.039915754852885\n",
      "The norm of grad vector is  19603.886699093957\n",
      "Loss decreases to  63.03520722775154\n",
      "The norm of grad vector is  19600.23393828833\n",
      "Loss decreases to  63.030500455009296\n",
      "The norm of grad vector is  19596.58304094317\n",
      "Loss decreases to  63.02579543540479\n",
      "The norm of grad vector is  19592.934002980375\n",
      "Loss decreases to  63.02109216771924\n",
      "The norm of grad vector is  19589.286820336267\n",
      "Loss decreases to  63.01639065073628\n",
      "The norm of grad vector is  19585.64148896176\n",
      "Loss decreases to  63.01169088324209\n",
      "The norm of grad vector is  19581.998004822275\n",
      "Loss decreases to  63.006992864025136\n",
      "The norm of grad vector is  19578.356363897543\n",
      "Loss decreases to  63.00229659187629\n",
      "The norm of grad vector is  19574.71656218181\n",
      "Loss decreases to  62.997602065588914\n",
      "The norm of grad vector is  19571.078595683437\n",
      "Loss decreases to  62.99290928395895\n",
      "The norm of grad vector is  19567.442460425144\n",
      "Loss decreases to  62.988218245784346\n",
      "The norm of grad vector is  19563.808152443857\n",
      "Loss decreases to  62.983528949865715\n",
      "The norm of grad vector is  19560.175667790572\n",
      "Loss decreases to  62.97884139500595\n",
      "The norm of grad vector is  19556.545002530514\n",
      "Loss decreases to  62.97415558001039\n",
      "The norm of grad vector is  19552.916152742768\n",
      "Loss decreases to  62.96947150368667\n",
      "The norm of grad vector is  19549.28911452053\n",
      "Loss decreases to  62.96478916484482\n",
      "The norm of grad vector is  19545.663883970923\n",
      "Loss decreases to  62.96010856229699\n",
      "The norm of grad vector is  19542.04045721494\n",
      "Loss decreases to  62.95542969485822\n",
      "The norm of grad vector is  19538.418830387353\n",
      "Loss decreases to  62.95075256134533\n",
      "The norm of grad vector is  19534.798999636812\n",
      "Loss decreases to  62.946077160577616\n",
      "The norm of grad vector is  19531.180961125625\n",
      "Loss decreases to  62.94140349137689\n",
      "The norm of grad vector is  19527.564711029805\n",
      "Loss decreases to  62.93673155256694\n",
      "The norm of grad vector is  19523.950245538894\n",
      "Loss decreases to  62.93206134297415\n",
      "The norm of grad vector is  19520.337560856307\n",
      "Loss decreases to  62.92739286142692\n",
      "The norm of grad vector is  19516.726653198628\n",
      "Loss decreases to  62.92272610675624\n",
      "The norm of grad vector is  19513.11751879606\n",
      "Loss decreases to  62.918061077795116\n",
      "The norm of grad vector is  19509.51015389233\n",
      "Loss decreases to  62.913397773379025\n",
      "The norm of grad vector is  19505.9045547444\n",
      "Loss decreases to  62.908736192345266\n",
      "The norm of grad vector is  19502.300717622635\n",
      "Loss decreases to  62.904076333534015\n",
      "The norm of grad vector is  19498.698638810638\n",
      "Loss decreases to  62.89941819578724\n",
      "The norm of grad vector is  19495.098314605257\n",
      "Loss decreases to  62.89476177794942\n",
      "The norm of grad vector is  19491.49974131651\n",
      "Loss decreases to  62.89010707886688\n",
      "The norm of grad vector is  19487.90291526756\n",
      "Loss decreases to  62.885454097388674\n",
      "The norm of grad vector is  19484.30783279465\n",
      "Loss decreases to  62.880802832365546\n",
      "The norm of grad vector is  19480.714490247014\n",
      "Loss decreases to  62.876153282650854\n",
      "The norm of grad vector is  19477.12288398687\n",
      "Loss decreases to  62.87150544709984\n",
      "The norm of grad vector is  19473.533010389463\n",
      "Loss decreases to  62.86685932457024\n",
      "The norm of grad vector is  19469.9448658428\n",
      "Loss decreases to  62.86221491392157\n",
      "The norm of grad vector is  19466.358446747843\n",
      "Loss decreases to  62.85757221401608\n",
      "The norm of grad vector is  19462.77374951825\n",
      "Loss decreases to  62.85293122371764\n",
      "The norm of grad vector is  19459.1907705804\n",
      "Loss decreases to  62.84829194189248\n",
      "The norm of grad vector is  19455.60950637352\n",
      "Loss decreases to  62.843654367409066\n",
      "The norm of grad vector is  19452.029953349334\n",
      "Loss decreases to  62.83901849913796\n",
      "The norm of grad vector is  19448.452107972193\n",
      "Loss decreases to  62.83438433595179\n",
      "The norm of grad vector is  19444.875966719097\n",
      "Loss decreases to  62.82975187672525\n",
      "The norm of grad vector is  19441.301526079445\n",
      "Loss decreases to  62.82512112033533\n",
      "The norm of grad vector is  19437.728782555096\n",
      "Loss decreases to  62.82049206566098\n",
      "The norm of grad vector is  19434.157732660442\n",
      "Loss decreases to  62.8158647115835\n",
      "The norm of grad vector is  19430.5883729221\n",
      "Loss decreases to  62.811239056985926\n",
      "The norm of grad vector is  19427.020699879093\n",
      "Loss decreases to  62.80661510075376\n",
      "The norm of grad vector is  19423.454710082682\n",
      "Loss decreases to  62.80199284177424\n",
      "The norm of grad vector is  19419.890400096396\n",
      "Loss decreases to  62.79737227893688\n",
      "The norm of grad vector is  19416.327766495866\n",
      "Loss decreases to  62.79275341113313\n",
      "The norm of grad vector is  19412.766805869007\n",
      "Loss decreases to  62.78813623725676\n",
      "The norm of grad vector is  19409.207514815665\n",
      "Loss decreases to  62.783520756203295\n",
      "The norm of grad vector is  19405.649889947872\n",
      "Loss decreases to  62.778906966870416\n",
      "The norm of grad vector is  19402.093927889502\n",
      "Loss decreases to  62.77429486815791\n",
      "The norm of grad vector is  19398.539625276593\n",
      "Loss decreases to  62.76968445896761\n",
      "The norm of grad vector is  19394.986978756908\n",
      "Loss decreases to  62.76507573820316\n",
      "The norm of grad vector is  19391.435984990236\n",
      "Loss decreases to  62.76046870477055\n",
      "The norm of grad vector is  19387.886640648045\n",
      "Loss decreases to  62.75586335757746\n",
      "The norm of grad vector is  19384.338942413688\n",
      "Loss decreases to  62.751259695533705\n",
      "The norm of grad vector is  19380.79288698224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss decreases to  62.746657717551166\n",
      "The norm of grad vector is  19377.248471060393\n",
      "Loss decreases to  62.742057422543645\n",
      "The norm of grad vector is  19373.705691366587\n",
      "Loss decreases to  62.73745880942694\n",
      "The norm of grad vector is  19370.164544630858\n",
      "Loss decreases to  62.7328618771188\n",
      "The norm of grad vector is  19366.625027594742\n",
      "Loss decreases to  62.72826662453903\n",
      "The norm of grad vector is  19363.087137011295\n",
      "Loss decreases to  62.7236730506094\n",
      "The norm of grad vector is  19359.550869645147\n",
      "Loss decreases to  62.71908115425329\n",
      "The norm of grad vector is  19356.01622227225\n",
      "Loss decreases to  62.71449093439663\n",
      "The norm of grad vector is  19352.483191680032\n",
      "Loss decreases to  62.70990238996673\n",
      "The norm of grad vector is  19348.95177466721\n",
      "Loss decreases to  62.70531551989325\n",
      "The norm of grad vector is  19345.421968043865\n",
      "Loss decreases to  62.70073032310759\n",
      "The norm of grad vector is  19341.893768631246\n",
      "Loss decreases to  62.696146798543005\n",
      "The norm of grad vector is  19338.36717326194\n",
      "Loss decreases to  62.69156494513487\n",
      "The norm of grad vector is  19334.84217877965\n",
      "Loss decreases to  62.68698476182023\n",
      "The norm of grad vector is  19331.318782039256\n",
      "Loss decreases to  62.68240624753827\n",
      "The norm of grad vector is  19327.796979906627\n",
      "Loss decreases to  62.677829401229836\n",
      "The norm of grad vector is  19324.276769258824\n",
      "Loss decreases to  62.673254221837816\n",
      "The norm of grad vector is  19320.758146983906\n",
      "Loss decreases to  62.66868070830716\n",
      "The norm of grad vector is  19317.241109980827\n",
      "Loss decreases to  62.66410885958426\n",
      "The norm of grad vector is  19313.72565515948\n",
      "Loss decreases to  62.65953867461765\n",
      "The norm of grad vector is  19310.21177944074\n",
      "Loss decreases to  62.65497015235772\n",
      "The norm of grad vector is  19306.699479756233\n",
      "Loss decreases to  62.650403291756696\n",
      "The norm of grad vector is  19303.188753048507\n",
      "Loss decreases to  62.645838091768574\n",
      "The norm of grad vector is  19299.67959627074\n",
      "Loss decreases to  62.64127455134943\n",
      "The norm of grad vector is  19296.172006386943\n",
      "Loss decreases to  62.636712669456855\n",
      "The norm of grad vector is  19292.66598037178\n",
      "Loss decreases to  62.63215244505046\n",
      "The norm of grad vector is  19289.161515210562\n",
      "Loss decreases to  62.62759387709164\n",
      "The norm of grad vector is  19285.65860789925\n",
      "Loss decreases to  62.62303696454372\n",
      "The norm of grad vector is  19282.157255444312\n",
      "Loss decreases to  62.61848170637167\n",
      "The norm of grad vector is  19278.65745486277\n",
      "Loss decreases to  62.61392810154225\n",
      "The norm of grad vector is  19275.159203182186\n",
      "Loss decreases to  62.60937614902435\n",
      "The norm of grad vector is  19271.662497440528\n",
      "Loss decreases to  62.60482584778825\n",
      "The norm of grad vector is  19268.16733468615\n",
      "Loss decreases to  62.600277196806196\n",
      "The norm of grad vector is  19264.67371197788\n",
      "Loss decreases to  62.595730195052255\n",
      "The norm of grad vector is  19261.181626384754\n",
      "Loss decreases to  62.59118484150232\n",
      "The norm of grad vector is  19257.69107498619\n",
      "Loss decreases to  62.58664113513367\n",
      "The norm of grad vector is  19254.202054871883\n",
      "Loss decreases to  62.58209907492593\n",
      "The norm of grad vector is  19250.714563141653\n",
      "Loss decreases to  62.57755865986006\n",
      "The norm of grad vector is  19247.22859690559\n",
      "Loss decreases to  62.57301988891905\n",
      "The norm of grad vector is  19243.744153283857\n",
      "Loss decreases to  62.568482761087395\n",
      "The norm of grad vector is  19240.261229406817\n",
      "Loss decreases to  62.56394727535132\n",
      "The norm of grad vector is  19236.77982241483\n",
      "Loss decreases to  62.55941343069901\n",
      "The norm of grad vector is  19233.299929458277\n",
      "Loss decreases to  62.554881226120415\n",
      "The norm of grad vector is  19229.82154769763\n",
      "Loss decreases to  62.550350660606746\n",
      "The norm of grad vector is  19226.34467430319\n",
      "Loss decreases to  62.54582173315159\n",
      "The norm of grad vector is  19222.86930645521\n",
      "Loss decreases to  62.54129444274963\n",
      "The norm of grad vector is  19219.395441343913\n",
      "Loss decreases to  62.53676878839765\n",
      "The norm of grad vector is  19215.923076169267\n",
      "Loss decreases to  62.53224476909409\n",
      "The norm of grad vector is  19212.452208141116\n",
      "Loss decreases to  62.527722383838956\n",
      "The norm of grad vector is  19208.982834479015\n",
      "Loss decreases to  62.52320163163385\n",
      "The norm of grad vector is  19205.514952412283\n",
      "Loss decreases to  62.51868251148245\n",
      "The norm of grad vector is  19202.04855917998\n",
      "Loss decreases to  62.51416502238972\n",
      "The norm of grad vector is  19198.58365203076\n",
      "Loss decreases to  62.50964916336243\n",
      "The norm of grad vector is  19195.120228222888\n",
      "Loss decreases to  62.5051349334092\n",
      "The norm of grad vector is  19191.65828502432\n",
      "Loss decreases to  62.500622331539944\n",
      "The norm of grad vector is  19188.197819712474\n",
      "Loss decreases to  62.49611135676651\n",
      "The norm of grad vector is  19184.738829574388\n",
      "Loss decreases to  62.49160200810236\n",
      "The norm of grad vector is  19181.2813119064\n",
      "Loss decreases to  62.487094284562524\n",
      "The norm of grad vector is  19177.825264014522\n",
      "Loss decreases to  62.48258818516371\n",
      "The norm of grad vector is  19174.370683214016\n",
      "Loss decreases to  62.478083708924274\n",
      "The norm of grad vector is  19170.91756682957\n",
      "Loss decreases to  62.47358085486414\n",
      "The norm of grad vector is  19167.465912195203\n",
      "Loss decreases to  62.4690796220049\n",
      "The norm of grad vector is  19164.015716654318\n",
      "Loss decreases to  62.464580009369726\n",
      "The norm of grad vector is  19160.566977559476\n",
      "Loss decreases to  62.46008201598352\n",
      "The norm of grad vector is  19157.119692272598\n",
      "Loss decreases to  62.455585640872734\n",
      "The norm of grad vector is  19153.673858164646\n",
      "Loss decreases to  62.45109088306525\n",
      "The norm of grad vector is  19150.229472615905\n",
      "Loss decreases to  62.44659774159071\n",
      "The norm of grad vector is  19146.78653301565\n",
      "Loss decreases to  62.44210621548047\n",
      "The norm of grad vector is  19143.34503676247\n",
      "Loss decreases to  62.43761630376724\n",
      "The norm of grad vector is  19139.904981263808\n",
      "Loss decreases to  62.43312800548532\n",
      "The norm of grad vector is  19136.466363936244\n",
      "Loss decreases to  62.428641319670824\n",
      "The norm of grad vector is  19133.029182205326\n",
      "Loss decreases to  62.42415624536106\n",
      "The norm of grad vector is  19129.593433505583\n",
      "Loss decreases to  62.41967278159532\n",
      "The norm of grad vector is  19126.15911528046\n",
      "Loss decreases to  62.415190927414166\n",
      "The norm of grad vector is  19122.726224982325\n",
      "Loss decreases to  62.41071068185968\n",
      "The norm of grad vector is  19119.294760072367\n",
      "Loss decreases to  62.40623204397578\n",
      "The norm of grad vector is  19115.864718020654\n",
      "Loss decreases to  62.40175501280767\n",
      "The norm of grad vector is  19112.436096306043\n",
      "Loss decreases to  62.39727958740217\n",
      "The norm of grad vector is  19109.008892416125\n",
      "Loss decreases to  62.39280576680766\n",
      "The norm of grad vector is  19105.5831038473\n",
      "Loss decreases to  62.38833355007401\n",
      "The norm of grad vector is  19102.158728104496\n",
      "Loss decreases to  62.383862936252704\n",
      "The norm of grad vector is  19098.735762701595\n",
      "Loss decreases to  62.379393924396574\n",
      "The norm of grad vector is  19095.314205160816\n",
      "Loss decreases to  62.37492651356012\n",
      "The norm of grad vector is  19091.89405301313\n",
      "Loss decreases to  62.37046070279934\n",
      "The norm of grad vector is  19088.475303798084\n",
      "Loss decreases to  62.36599649117156\n",
      "The norm of grad vector is  19085.057955063778\n",
      "Loss decreases to  62.36153387773585\n",
      "The norm of grad vector is  19081.642004366717\n",
      "Loss decreases to  62.35707286155257\n",
      "The norm of grad vector is  19078.22744927198\n",
      "Loss decreases to  62.35261344168363\n",
      "The norm of grad vector is  19074.814287353045\n",
      "Loss decreases to  62.348155617192546\n",
      "The norm of grad vector is  19071.402516191833\n",
      "Loss decreases to  62.34369938714419\n",
      "The norm of grad vector is  19067.992133378615\n",
      "Loss decreases to  62.339244750604884\n",
      "The norm of grad vector is  19064.583136512003\n",
      "Loss decreases to  62.33479170664252\n",
      "The norm of grad vector is  19061.175523198952\n",
      "Loss decreases to  62.33034025432628\n",
      "The norm of grad vector is  19057.769291054712\n",
      "Loss decreases to  62.3258903927271\n",
      "The norm of grad vector is  19054.364437702763\n",
      "Loss decreases to  62.32144212091709\n",
      "The norm of grad vector is  19050.960960774803\n",
      "Loss decreases to  62.31699543796992\n",
      "The norm of grad vector is  19047.55885791078\n",
      "Loss decreases to  62.31255034296071\n",
      "The norm of grad vector is  19044.15812675875\n",
      "Loss decreases to  62.30810683496602\n",
      "The norm of grad vector is  19040.75876497489\n",
      "Loss decreases to  62.3036649130639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  19037.360770223546\n",
      "Loss decreases to  62.299224576333614\n",
      "The norm of grad vector is  19033.964140177057\n",
      "Loss decreases to  62.29478582385602\n",
      "The norm of grad vector is  19030.56887251594\n",
      "Loss decreases to  62.29034865471357\n",
      "The norm of grad vector is  19027.174964928487\n",
      "Loss decreases to  62.285913067989775\n",
      "The norm of grad vector is  19023.782415111218\n",
      "Loss decreases to  62.28147906276978\n",
      "The norm of grad vector is  19020.391220768473\n",
      "Loss decreases to  62.277046638140085\n",
      "The norm of grad vector is  19017.001379612524\n",
      "Loss decreases to  62.2726157931885\n",
      "The norm of grad vector is  19013.61288936357\n",
      "Loss decreases to  62.268186527004545\n",
      "The norm of grad vector is  19010.22574774964\n",
      "Loss decreases to  62.26375883867869\n",
      "The norm of grad vector is  19006.839952506638\n",
      "Loss decreases to  62.25933272730306\n",
      "The norm of grad vector is  19003.455501378234\n",
      "Loss decreases to  62.25490819197124\n",
      "The norm of grad vector is  19000.07239211589\n",
      "Loss decreases to  62.25048523177805\n",
      "The norm of grad vector is  18996.690622478793\n",
      "Loss decreases to  62.246063845819556\n",
      "The norm of grad vector is  18993.310190233875\n",
      "Loss decreases to  62.24164403319363\n",
      "The norm of grad vector is  18989.931093155756\n",
      "Loss decreases to  62.23722579299896\n",
      "The norm of grad vector is  18986.55332902668\n",
      "Loss decreases to  62.23280912433599\n",
      "The norm of grad vector is  18983.17689563654\n",
      "Loss decreases to  62.228394026306425\n",
      "The norm of grad vector is  18979.801790782905\n",
      "Loss decreases to  62.22398049801335\n",
      "The norm of grad vector is  18976.428012270797\n",
      "Loss decreases to  62.219568538561035\n",
      "The norm of grad vector is  18973.05555791282\n",
      "Loss decreases to  62.21515814705517\n",
      "The norm of grad vector is  18969.68442552914\n",
      "Loss decreases to  62.21074932260298\n",
      "The norm of grad vector is  18966.314612947383\n",
      "Loss decreases to  62.2063420643128\n",
      "The norm of grad vector is  18962.946118002605\n",
      "Loss decreases to  62.20193637129439\n",
      "The norm of grad vector is  18959.578938537365\n",
      "Loss decreases to  62.19753224265877\n",
      "The norm of grad vector is  18956.213072401584\n",
      "Loss decreases to  62.19312967751841\n",
      "The norm of grad vector is  18952.848517452538\n",
      "Loss decreases to  62.18872867498698\n",
      "The norm of grad vector is  18949.485271554906\n",
      "Loss decreases to  62.18432923417945\n",
      "The norm of grad vector is  18946.12333258063\n",
      "Loss decreases to  62.17993135421218\n",
      "The norm of grad vector is  18942.762698409028\n",
      "Loss decreases to  62.17553503420299\n",
      "The norm of grad vector is  18939.403366926577\n",
      "Loss decreases to  62.17114027327048\n",
      "The norm of grad vector is  18936.045336027124\n",
      "Loss decreases to  62.166747070535216\n",
      "The norm of grad vector is  18932.68860361161\n",
      "Loss decreases to  62.16235542511859\n",
      "The norm of grad vector is  18929.333167588225\n",
      "Loss decreases to  62.157965336143434\n",
      "The norm of grad vector is  18925.979025872286\n",
      "Loss decreases to  62.153576802733866\n",
      "The norm of grad vector is  18922.626176386286\n",
      "Loss decreases to  62.149189824015295\n",
      "The norm of grad vector is  18919.274617059833\n",
      "Loss decreases to  62.14480439911431\n",
      "The norm of grad vector is  18915.924345829502\n",
      "Loss decreases to  62.14042052715903\n",
      "The norm of grad vector is  18912.575360639064\n",
      "Loss decreases to  62.13603820727843\n",
      "The norm of grad vector is  18909.227659439206\n",
      "Loss decreases to  62.13165743860314\n",
      "The norm of grad vector is  18905.881240187737\n",
      "Loss decreases to  62.12727822026497\n",
      "The norm of grad vector is  18902.536100849262\n",
      "Loss decreases to  62.12290055139685\n",
      "The norm of grad vector is  18899.192239395492\n",
      "Loss decreases to  62.11852443113294\n",
      "The norm of grad vector is  18895.849653805024\n",
      "Loss decreases to  62.114149858608776\n",
      "The norm of grad vector is  18892.508342063255\n",
      "Loss decreases to  62.109776832961145\n",
      "The norm of grad vector is  18889.16830216263\n",
      "Loss decreases to  62.10540535332807\n",
      "The norm of grad vector is  18885.82953210225\n",
      "Loss decreases to  62.10103541884862\n",
      "The norm of grad vector is  18882.492029888163\n",
      "Loss decreases to  62.09666702866349\n",
      "The norm of grad vector is  18879.155793533162\n",
      "Loss decreases to  62.092300181914155\n",
      "The norm of grad vector is  18875.82082105679\n",
      "Loss decreases to  62.087934877743564\n",
      "The norm of grad vector is  18872.487110485392\n",
      "Loss decreases to  62.08357111529604\n",
      "The norm of grad vector is  18869.154659851953\n",
      "Loss decreases to  62.07920889371661\n",
      "The norm of grad vector is  18865.823467196242\n",
      "Loss decreases to  62.074848212152084\n",
      "The norm of grad vector is  18862.49353056457\n",
      "Loss decreases to  62.070489069750195\n",
      "The norm of grad vector is  18859.16484801005\n",
      "Loss decreases to  62.06613146565984\n",
      "The norm of grad vector is  18855.83741759224\n",
      "Loss decreases to  62.06177539903129\n",
      "The norm of grad vector is  18852.51123737745\n",
      "Loss decreases to  62.057420869016006\n",
      "The norm of grad vector is  18849.186305438445\n",
      "Loss decreases to  62.05306787476634\n",
      "The norm of grad vector is  18845.862619854568\n",
      "Loss decreases to  62.04871641543613\n",
      "The norm of grad vector is  18842.540178711704\n",
      "Loss decreases to  62.044366490180465\n",
      "The norm of grad vector is  18839.21898010216\n",
      "Loss decreases to  62.04001809815534\n",
      "The norm of grad vector is  18835.899022124864\n",
      "Loss decreases to  62.0356712385182\n",
      "The norm of grad vector is  18832.580302885\n",
      "Loss decreases to  62.03132591042747\n",
      "The norm of grad vector is  18829.26282049431\n",
      "Loss decreases to  62.02698211304278\n",
      "The norm of grad vector is  18825.946573070854\n",
      "Loss decreases to  62.02263984552506\n",
      "The norm of grad vector is  18822.63155873913\n",
      "Loss decreases to  62.01829910703625\n",
      "The norm of grad vector is  18819.31777562993\n",
      "Loss decreases to  62.013959896739564\n",
      "The norm of grad vector is  18816.005221880412\n",
      "Loss decreases to  62.0096222137992\n",
      "The norm of grad vector is  18812.693895634013\n",
      "Loss decreases to  62.00528605738085\n",
      "The norm of grad vector is  18809.383795040445\n",
      "Loss decreases to  62.00095142665103\n",
      "The norm of grad vector is  18806.07491825571\n",
      "Loss decreases to  61.996618320777415\n",
      "The norm of grad vector is  18802.767263441983\n",
      "Loss decreases to  61.99228673892914\n",
      "The norm of grad vector is  18799.460828767733\n",
      "Loss decreases to  61.98795668027605\n",
      "The norm of grad vector is  18796.155612407514\n",
      "Loss decreases to  61.983628143989456\n",
      "The norm of grad vector is  18792.851612542072\n",
      "Loss decreases to  61.97930112924169\n",
      "The norm of grad vector is  18789.54882735841\n",
      "Loss decreases to  61.97497563520617\n",
      "The norm of grad vector is  18786.247255049424\n",
      "Loss decreases to  61.97065166105749\n",
      "The norm of grad vector is  18782.946893814325\n",
      "Loss decreases to  61.966329205971356\n",
      "The norm of grad vector is  18779.647741858204\n",
      "Loss decreases to  61.96200826912465\n",
      "The norm of grad vector is  18776.349797392384\n",
      "Loss decreases to  61.95768884969524\n",
      "The norm of grad vector is  18773.05305863402\n",
      "Loss decreases to  61.95337094686213\n",
      "The norm of grad vector is  18769.757523806446\n",
      "Loss decreases to  61.94905455980562\n",
      "The norm of grad vector is  18766.463191138857\n",
      "Loss decreases to  61.944739687706935\n",
      "The norm of grad vector is  18763.17005886645\n",
      "Loss decreases to  61.9404263297484\n",
      "The norm of grad vector is  18759.878125230374\n",
      "Loss decreases to  61.936114485113464\n",
      "The norm of grad vector is  18756.587388477637\n",
      "Loss decreases to  61.93180415298681\n",
      "The norm of grad vector is  18753.297846861213\n",
      "Loss decreases to  61.927495332553924\n",
      "The norm of grad vector is  18750.009498639833\n",
      "Loss decreases to  61.923188023001735\n",
      "The norm of grad vector is  18746.722342078214\n",
      "Loss decreases to  61.91888222351788\n",
      "The norm of grad vector is  18743.436375446752\n",
      "Loss decreases to  61.914577933291454\n",
      "The norm of grad vector is  18740.15159702179\n",
      "Loss decreases to  61.91027515151231\n",
      "The norm of grad vector is  18736.86800508535\n",
      "Loss decreases to  61.90597387737144\n",
      "The norm of grad vector is  18733.58559792527\n",
      "Loss decreases to  61.901674110061194\n",
      "The norm of grad vector is  18730.304373835046\n",
      "Loss decreases to  61.89737584877463\n",
      "The norm of grad vector is  18727.02433111399\n",
      "Loss decreases to  61.89307909270608\n",
      "The norm of grad vector is  18723.74546806705\n",
      "Loss decreases to  61.88878384105082\n",
      "The norm of grad vector is  18720.467783004828\n",
      "Loss decreases to  61.884490093005304\n",
      "The norm of grad vector is  18717.191274243723\n",
      "Loss decreases to  61.880197847766915\n",
      "The norm of grad vector is  18713.91594010555\n",
      "Loss decreases to  61.875907104534114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  18710.641778917863\n",
      "Loss decreases to  61.871617862506604\n",
      "The norm of grad vector is  18707.368789013766\n",
      "Loss decreases to  61.86733012088473\n",
      "The norm of grad vector is  18704.09696873201\n",
      "Loss decreases to  61.8630438788703\n",
      "The norm of grad vector is  18700.826316416766\n",
      "Loss decreases to  61.85875913566591\n",
      "The norm of grad vector is  18697.55683041781\n",
      "Loss decreases to  61.85447589047536\n",
      "The norm of grad vector is  18694.28850909045\n",
      "Loss decreases to  61.85019414250328\n",
      "The norm of grad vector is  18691.021350795392\n",
      "Loss decreases to  61.84591389095557\n",
      "The norm of grad vector is  18687.755353898832\n",
      "Loss decreases to  61.8416351350389\n",
      "The norm of grad vector is  18684.490516772486\n",
      "Loss decreases to  61.83735787396113\n",
      "The norm of grad vector is  18681.22683779342\n",
      "Loss decreases to  61.83308210693112\n",
      "The norm of grad vector is  18677.964315344132\n",
      "Loss decreases to  61.82880783315866\n",
      "The norm of grad vector is  18674.702947812453\n",
      "Loss decreases to  61.82453505185481\n",
      "The norm of grad vector is  18671.442733591655\n",
      "Loss decreases to  61.820263762231214\n",
      "The norm of grad vector is  18668.18367108032\n",
      "Loss decreases to  61.81599396350106\n",
      "The norm of grad vector is  18664.925758682322\n",
      "Loss decreases to  61.811725654877854\n",
      "The norm of grad vector is  18661.66899480691\n",
      "Loss decreases to  61.80745883557682\n",
      "The norm of grad vector is  18658.41337786853\n",
      "Loss decreases to  61.803193504813606\n",
      "The norm of grad vector is  18655.158906286957\n",
      "Loss decreases to  61.798929661805396\n",
      "The norm of grad vector is  18651.905578487138\n",
      "Loss decreases to  61.79466730576988\n",
      "The norm of grad vector is  18648.653392899378\n",
      "Loss decreases to  61.790406435925966\n",
      "The norm of grad vector is  18645.40234795906\n",
      "Loss decreases to  61.78614705149359\n",
      "The norm of grad vector is  18642.152442106744\n",
      "Loss decreases to  61.78188915169349\n",
      "The norm of grad vector is  18638.903673788285\n",
      "Loss decreases to  61.777632735747545\n",
      "The norm of grad vector is  18635.656041454542\n",
      "Loss decreases to  61.77337780287862\n",
      "The norm of grad vector is  18632.40954356161\n",
      "Loss decreases to  61.76912435231053\n",
      "The norm of grad vector is  18629.164178570612\n",
      "Loss decreases to  61.76487238326794\n",
      "The norm of grad vector is  18625.91994494779\n",
      "Loss decreases to  61.76062189497656\n",
      "The norm of grad vector is  18622.67684116451\n",
      "Loss decreases to  61.75637288666305\n",
      "The norm of grad vector is  18619.43486569708\n",
      "Loss decreases to  61.75212535755526\n",
      "The norm of grad vector is  18616.19401702693\n",
      "Loss decreases to  61.74787930688168\n",
      "The norm of grad vector is  18612.95429364046\n",
      "Loss decreases to  61.74363473387194\n",
      "The norm of grad vector is  18609.715694029062\n",
      "Loss decreases to  61.73939163775648\n",
      "The norm of grad vector is  18606.478216689153\n",
      "Loss decreases to  61.73515001776682\n",
      "The norm of grad vector is  18603.241860121994\n",
      "Loss decreases to  61.730909873135545\n",
      "The norm of grad vector is  18600.006622833964\n",
      "Loss decreases to  61.7266712030958\n",
      "The norm of grad vector is  18596.772503336208\n",
      "Loss decreases to  61.722434006882054\n",
      "The norm of grad vector is  18593.53950014481\n",
      "Loss decreases to  61.71819828372946\n",
      "The norm of grad vector is  18590.30761178075\n",
      "Loss decreases to  61.71396403287438\n",
      "The norm of grad vector is  18587.0768367699\n",
      "Loss decreases to  61.70973125355389\n",
      "The norm of grad vector is  18583.84717364291\n",
      "Loss decreases to  61.70549994500603\n",
      "The norm of grad vector is  18580.61862093537\n",
      "Loss decreases to  61.70127010646987\n",
      "The norm of grad vector is  18577.391177187517\n",
      "Loss decreases to  61.69704173718528\n",
      "The norm of grad vector is  18574.164840944548\n",
      "Loss decreases to  61.69281483639317\n",
      "The norm of grad vector is  18570.939610756323\n",
      "Loss decreases to  61.6885894033352\n",
      "The norm of grad vector is  18567.715485177512\n",
      "Loss decreases to  61.68436543725425\n",
      "The norm of grad vector is  18564.49246276749\n",
      "Loss decreases to  61.680142937393846\n",
      "The norm of grad vector is  18561.27054209041\n",
      "Loss decreases to  61.67592190299844\n",
      "The norm of grad vector is  18558.049721715044\n",
      "Loss decreases to  61.67170233331354\n",
      "The norm of grad vector is  18554.83000021492\n",
      "Loss decreases to  61.667484227585554\n",
      "The norm of grad vector is  18551.611376168188\n",
      "Loss decreases to  61.663267585061604\n",
      "The norm of grad vector is  18548.393848157702\n",
      "Loss decreases to  61.659052404989936\n",
      "The norm of grad vector is  18545.177414770864\n",
      "Loss decreases to  61.65483868661961\n",
      "The norm of grad vector is  18541.962074599807\n",
      "Loss decreases to  61.65062642920061\n",
      "The norm of grad vector is  18538.747826241153\n",
      "Loss decreases to  61.646415631983665\n",
      "The norm of grad vector is  18535.534668296183\n",
      "Loss decreases to  61.642206294220664\n",
      "The norm of grad vector is  18532.32259937069\n",
      "Loss decreases to  61.637998415164056\n",
      "The norm of grad vector is  18529.111618075043\n",
      "Loss decreases to  61.63379199406756\n",
      "The norm of grad vector is  18525.90172302415\n",
      "Loss decreases to  61.62958703018543\n",
      "The norm of grad vector is  18522.69291283739\n",
      "Loss decreases to  61.62538352277311\n",
      "The norm of grad vector is  18519.485186138685\n",
      "Loss decreases to  61.62118147108664\n",
      "The norm of grad vector is  18516.278541556436\n",
      "Loss decreases to  61.61698087438313\n",
      "The norm of grad vector is  18513.072977723437\n",
      "Loss decreases to  61.61278173192036\n",
      "The norm of grad vector is  18509.868493277005\n",
      "Loss decreases to  61.60858404295733\n",
      "The norm of grad vector is  18506.66508685887\n",
      "Loss decreases to  61.60438780675354\n",
      "The norm of grad vector is  18503.46275711516\n",
      "Loss decreases to  61.60019302256969\n",
      "The norm of grad vector is  18500.26150269639\n",
      "Loss decreases to  61.59599968966709\n",
      "The norm of grad vector is  18497.06132225749\n",
      "Loss decreases to  61.591807807307845\n",
      "The norm of grad vector is  18493.862214457735\n",
      "Loss decreases to  61.58761737475531\n",
      "The norm of grad vector is  18490.664177960713\n",
      "Loss decreases to  61.583428391273316\n",
      "The norm of grad vector is  18487.4672114344\n",
      "Loss decreases to  61.5792408561267\n",
      "The norm of grad vector is  18484.27131355107\n",
      "Loss decreases to  61.5750547685812\n",
      "The norm of grad vector is  18481.07648298727\n",
      "Loss decreases to  61.57087012790328\n",
      "The norm of grad vector is  18477.88271842387\n",
      "Loss decreases to  61.56668693336035\n",
      "The norm of grad vector is  18474.69001854597\n",
      "Loss decreases to  61.562505184220626\n",
      "The norm of grad vector is  18471.49838204291\n",
      "Loss decreases to  61.55832487975321\n",
      "The norm of grad vector is  18468.307807608315\n",
      "Loss decreases to  61.554146019228014\n",
      "The norm of grad vector is  18465.118293940002\n",
      "Loss decreases to  61.54996860191564\n",
      "The norm of grad vector is  18461.929839739987\n",
      "Loss decreases to  61.545792627087785\n",
      "The norm of grad vector is  18458.742443714473\n",
      "Loss decreases to  61.541618094016926\n",
      "The norm of grad vector is  18455.556104573865\n",
      "Loss decreases to  61.53744500197615\n",
      "The norm of grad vector is  18452.370821032673\n",
      "Loss decreases to  61.53327335023967\n",
      "The norm of grad vector is  18449.18659180961\n",
      "Loss decreases to  61.529103138082256\n",
      "The norm of grad vector is  18446.003415627394\n",
      "Loss decreases to  61.52493436477968\n",
      "The norm of grad vector is  18442.821291213033\n",
      "Loss decreases to  61.52076702960844\n",
      "The norm of grad vector is  18439.640217297463\n",
      "Loss decreases to  61.51660113184604\n",
      "The norm of grad vector is  18436.460192615814\n",
      "Loss decreases to  61.51243667077056\n",
      "The norm of grad vector is  18433.28121590718\n",
      "Loss decreases to  61.50827364566091\n",
      "The norm of grad vector is  18430.10328591481\n",
      "Loss decreases to  61.50411205579698\n",
      "The norm of grad vector is  18426.926401385885\n",
      "Loss decreases to  61.49995190045944\n",
      "The norm of grad vector is  18423.750561071713\n",
      "Loss decreases to  61.495793178929524\n",
      "The norm of grad vector is  18420.575763727484\n",
      "Loss decreases to  61.49163589048963\n",
      "The norm of grad vector is  18417.402008112484\n",
      "Loss decreases to  61.4874800344227\n",
      "The norm of grad vector is  18414.229292989898\n",
      "Loss decreases to  61.48332561001273\n",
      "The norm of grad vector is  18411.057617126895\n",
      "Loss decreases to  61.479172616544\n",
      "The norm of grad vector is  18407.886979294613\n",
      "Loss decreases to  61.47502105330226\n",
      "The norm of grad vector is  18404.717378268055\n",
      "Loss decreases to  61.47087091957356\n",
      "The norm of grad vector is  18401.548812826204\n",
      "Loss decreases to  61.466722214645024\n",
      "The norm of grad vector is  18398.3812817519\n",
      "Loss decreases to  61.46257493780432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  18395.214783831914\n",
      "Loss decreases to  61.458429088340125\n",
      "The norm of grad vector is  18392.049317856836\n",
      "Loss decreases to  61.45428466554177\n",
      "The norm of grad vector is  18388.884882621147\n",
      "Loss decreases to  61.45014166869939\n",
      "The norm of grad vector is  18385.721476923132\n",
      "Loss decreases to  61.44600009710402\n",
      "The norm of grad vector is  18382.559099564947\n",
      "Loss decreases to  61.441859950047196\n",
      "The norm of grad vector is  18379.39774935252\n",
      "Loss decreases to  61.437721226821544\n",
      "The norm of grad vector is  18376.237425095638\n",
      "Loss decreases to  61.43358392672022\n",
      "The norm of grad vector is  18373.078125607815\n",
      "Loss decreases to  61.42944804903739\n",
      "The norm of grad vector is  18369.91984970632\n",
      "Loss decreases to  61.42531359306789\n",
      "The norm of grad vector is  18366.76259621228\n",
      "Loss decreases to  61.42118055810707\n",
      "The norm of grad vector is  18363.606363950417\n",
      "Loss decreases to  61.41704894345143\n",
      "The norm of grad vector is  18360.45115174932\n",
      "Loss decreases to  61.41291874839804\n",
      "The norm of grad vector is  18357.296958441195\n",
      "Loss decreases to  61.40878997224475\n",
      "The norm of grad vector is  18354.14378286201\n",
      "Loss decreases to  61.40466261429021\n",
      "The norm of grad vector is  18350.991623851398\n",
      "Loss decreases to  61.40053667383372\n",
      "The norm of grad vector is  18347.84048025265\n",
      "Loss decreases to  61.396412150175564\n",
      "The norm of grad vector is  18344.690350912722\n",
      "Loss decreases to  61.39228904261648\n",
      "The norm of grad vector is  18341.54123468222\n",
      "Loss decreases to  61.38816735045824\n",
      "The norm of grad vector is  18338.393130415403\n",
      "Loss decreases to  61.38404707300308\n",
      "The norm of grad vector is  18335.24603697011\n",
      "Loss decreases to  61.379928209554194\n",
      "The norm of grad vector is  18332.09995320777\n",
      "Loss decreases to  61.37581075941555\n",
      "The norm of grad vector is  18328.95487799352\n",
      "Loss decreases to  61.37169472189162\n",
      "The norm of grad vector is  18325.810810195915\n",
      "Loss decreases to  61.36758009628787\n",
      "The norm of grad vector is  18322.667748687225\n",
      "Loss decreases to  61.36346688191042\n",
      "The norm of grad vector is  18319.525692343086\n",
      "Loss decreases to  61.359355078066095\n",
      "The norm of grad vector is  18316.38464004288\n",
      "Loss decreases to  61.35524468406235\n",
      "The norm of grad vector is  18313.244590669365\n",
      "Loss decreases to  61.351135699207646\n",
      "The norm of grad vector is  18310.105543108886\n",
      "Loss decreases to  61.34702812281099\n",
      "The norm of grad vector is  18306.967496251236\n",
      "Loss decreases to  61.34292195418201\n",
      "The norm of grad vector is  18303.830448989745\n",
      "Loss decreases to  61.33881719263139\n",
      "The norm of grad vector is  18300.694400221168\n",
      "Loss decreases to  61.33471383747021\n",
      "The norm of grad vector is  18297.559348845727\n",
      "Loss decreases to  61.3306118880104\n",
      "The norm of grad vector is  18294.42529376716\n",
      "Loss decreases to  61.326511343564796\n",
      "The norm of grad vector is  18291.292233892484\n",
      "Loss decreases to  61.322412203446596\n",
      "The norm of grad vector is  18288.160168132305\n",
      "Loss decreases to  61.31831446696993\n",
      "The norm of grad vector is  18285.029095400572\n",
      "Loss decreases to  61.314218133449685\n",
      "The norm of grad vector is  18281.899014614577\n",
      "Loss decreases to  61.31012320220129\n",
      "The norm of grad vector is  18278.76992469507\n",
      "Loss decreases to  61.30602967254116\n",
      "The norm of grad vector is  18275.64182456611\n",
      "Loss decreases to  61.301937543785975\n",
      "The norm of grad vector is  18272.514713155204\n",
      "Loss decreases to  61.29784681525354\n",
      "The norm of grad vector is  18269.388589393093\n",
      "Loss decreases to  61.293757486262216\n",
      "The norm of grad vector is  18266.263452213883\n",
      "Loss decreases to  61.28966955613108\n",
      "The norm of grad vector is  18263.139300555096\n",
      "Loss decreases to  61.2855830241799\n",
      "The norm of grad vector is  18260.016133357385\n",
      "Loss decreases to  61.28149788972899\n",
      "The norm of grad vector is  18256.893949564877\n",
      "Loss decreases to  61.27741415209967\n",
      "The norm of grad vector is  18253.772748124833\n",
      "Loss decreases to  61.2733318106137\n",
      "The norm of grad vector is  18250.652527987917\n",
      "Loss decreases to  61.269250864593765\n",
      "The norm of grad vector is  18247.533288107945\n",
      "Loss decreases to  61.265171313363034\n",
      "The norm of grad vector is  18244.415027442003\n",
      "Loss decreases to  61.26109315624544\n",
      "The norm of grad vector is  18241.297744950472\n",
      "Loss decreases to  61.25701639256554\n",
      "The norm of grad vector is  18238.181439596887\n",
      "Loss decreases to  61.252941021648695\n",
      "The norm of grad vector is  18235.06611034801\n",
      "Loss decreases to  61.24886704282097\n",
      "The norm of grad vector is  18231.951756173832\n",
      "Loss decreases to  61.244794455408936\n",
      "The norm of grad vector is  18228.838376047504\n",
      "Loss decreases to  61.24072325874009\n",
      "The norm of grad vector is  18225.725968945277\n",
      "Loss decreases to  61.23665345214224\n",
      "The norm of grad vector is  18222.61453384675\n",
      "Loss decreases to  61.23258503494442\n",
      "The norm of grad vector is  18219.504069734507\n",
      "Loss decreases to  61.22851800647572\n",
      "The norm of grad vector is  18216.3945755943\n",
      "Loss decreases to  61.22445236606645\n",
      "The norm of grad vector is  18213.286050415078\n",
      "Loss decreases to  61.220388113047264\n",
      "The norm of grad vector is  18210.178493188847\n",
      "Loss decreases to  61.21632524674946\n",
      "The norm of grad vector is  18207.071902910728\n",
      "Loss decreases to  61.21226376650525\n",
      "The norm of grad vector is  18203.96627857893\n",
      "Loss decreases to  61.2082036716474\n",
      "The norm of grad vector is  18200.861619194773\n",
      "Loss decreases to  61.20414496150916\n",
      "The norm of grad vector is  18197.757923762565\n",
      "Loss decreases to  61.20008763542483\n",
      "The norm of grad vector is  18194.655191289785\n",
      "Loss decreases to  61.196031692729015\n",
      "The norm of grad vector is  18191.553420786877\n",
      "Loss decreases to  61.191977132757096\n",
      "The norm of grad vector is  18188.452611267337\n",
      "Loss decreases to  61.1879239548453\n",
      "The norm of grad vector is  18185.352761747727\n",
      "Loss decreases to  61.18387215833005\n",
      "The norm of grad vector is  18182.253871247507\n",
      "Loss decreases to  61.179821742549\n",
      "The norm of grad vector is  18179.155938789325\n",
      "Loss decreases to  61.17577270684002\n",
      "The norm of grad vector is  18176.058963398624\n",
      "Loss decreases to  61.1717250505419\n",
      "The norm of grad vector is  18172.96294410397\n",
      "Loss decreases to  61.16767877299385\n",
      "The norm of grad vector is  18169.867879936795\n",
      "Loss decreases to  61.16363387353592\n",
      "The norm of grad vector is  18166.77376993153\n",
      "Loss decreases to  61.15959035150864\n",
      "The norm of grad vector is  18163.680613125576\n",
      "Loss decreases to  61.15554820625338\n",
      "The norm of grad vector is  18160.58840855925\n",
      "Loss decreases to  61.151507437112\n",
      "The norm of grad vector is  18157.49715527577\n",
      "Loss decreases to  61.14746804342713\n",
      "The norm of grad vector is  18154.40685232128\n",
      "Loss decreases to  61.14343002454185\n",
      "The norm of grad vector is  18151.317498744847\n",
      "Loss decreases to  61.139393379800055\n",
      "The norm of grad vector is  18148.229093598402\n",
      "Loss decreases to  61.13535810854618\n",
      "The norm of grad vector is  18145.141635936754\n",
      "Loss decreases to  61.131324210125314\n",
      "The norm of grad vector is  18142.05512481761\n",
      "Loss decreases to  61.12729168388319\n",
      "The norm of grad vector is  18138.969559301535\n",
      "Loss decreases to  61.123260529166274\n",
      "The norm of grad vector is  18135.884938451858\n",
      "Loss decreases to  61.119230745321545\n",
      "The norm of grad vector is  18132.801261334876\n",
      "Loss decreases to  61.11520233169652\n",
      "The norm of grad vector is  18129.71852701963\n",
      "Loss decreases to  61.111175287639604\n",
      "The norm of grad vector is  18126.636734578013\n",
      "Loss decreases to  61.10714961249944\n",
      "The norm of grad vector is  18123.555883084697\n",
      "Loss decreases to  61.10312530562583\n",
      "The norm of grad vector is  18120.47597161715\n",
      "Loss decreases to  61.09910236636868\n",
      "The norm of grad vector is  18117.396999255656\n",
      "Loss decreases to  61.095080794078875\n",
      "The norm of grad vector is  18114.31896508321\n",
      "Loss decreases to  61.091060588107744\n",
      "The norm of grad vector is  18111.241868185687\n",
      "Loss decreases to  61.087041747807106\n",
      "The norm of grad vector is  18108.16570765157\n",
      "Loss decreases to  61.083024272529805\n",
      "The norm of grad vector is  18105.0904825722\n",
      "Loss decreases to  61.07900816162901\n",
      "The norm of grad vector is  18102.0161920416\n",
      "Loss decreases to  61.07499341445837\n",
      "The norm of grad vector is  18098.942835156522\n",
      "Loss decreases to  61.07098003037255\n",
      "The norm of grad vector is  18095.870411016458\n",
      "Loss decreases to  61.06696800872646\n",
      "The norm of grad vector is  18092.798918723554\n",
      "Loss decreases to  61.062957348875784\n",
      "The norm of grad vector is  18089.72835738267\n",
      "Loss decreases to  61.05894805017683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  18086.658726101374\n",
      "Loss decreases to  61.05494011198649\n",
      "The norm of grad vector is  18083.590023989924\n",
      "Loss decreases to  61.0509335336621\n",
      "The norm of grad vector is  18080.522250161157\n",
      "Loss decreases to  61.046928314561804\n",
      "The norm of grad vector is  18077.45540373057\n",
      "Loss decreases to  61.04292445404438\n",
      "The norm of grad vector is  18074.38948381644\n",
      "Loss decreases to  61.03892195146899\n",
      "The norm of grad vector is  18071.32448953953\n",
      "Loss decreases to  61.03492080619552\n",
      "The norm of grad vector is  18068.26042002327\n",
      "Loss decreases to  61.030921017584426\n",
      "The norm of grad vector is  18065.19727439374\n",
      "Loss decreases to  61.026922584996825\n",
      "The norm of grad vector is  18062.135051779547\n",
      "Loss decreases to  61.022925507794476\n",
      "The norm of grad vector is  18059.07375131203\n",
      "Loss decreases to  61.018929785339395\n",
      "The norm of grad vector is  18056.013372124948\n",
      "Loss decreases to  61.01493541699461\n",
      "The norm of grad vector is  18052.95391335472\n",
      "Loss decreases to  61.010942402123476\n",
      "The norm of grad vector is  18049.89537414034\n",
      "Loss decreases to  61.00695074008999\n",
      "The norm of grad vector is  18046.83775362334\n",
      "Loss decreases to  61.002960430258696\n",
      "The norm of grad vector is  18043.781050947815\n",
      "Loss decreases to  60.99897147199506\n",
      "The norm of grad vector is  18040.72526526031\n",
      "Loss decreases to  60.99498386466447\n",
      "The norm of grad vector is  18037.67039571004\n",
      "Loss decreases to  60.99099760763358\n",
      "The norm of grad vector is  18034.61644144862\n",
      "Loss decreases to  60.987012700269204\n",
      "The norm of grad vector is  18031.563401630294\n",
      "Loss decreases to  60.9830291419389\n",
      "The norm of grad vector is  18028.511275411645\n",
      "Loss decreases to  60.979046932010554\n",
      "The norm of grad vector is  18025.460061951868\n",
      "Loss decreases to  60.97506606985319\n",
      "The norm of grad vector is  18022.40976041257\n",
      "Loss decreases to  60.97108655483581\n",
      "The norm of grad vector is  18019.360369957943\n",
      "Loss decreases to  60.96710838632816\n",
      "The norm of grad vector is  18016.311889754503\n",
      "Loss decreases to  60.963131563700834\n",
      "The norm of grad vector is  18013.264318971265\n",
      "Loss decreases to  60.959156086324725\n",
      "The norm of grad vector is  18010.217656779736\n",
      "Loss decreases to  60.95518195357133\n",
      "The norm of grad vector is  18007.171902353843\n",
      "Loss decreases to  60.951209164812745\n",
      "The norm of grad vector is  18004.127054869834\n",
      "Loss decreases to  60.94723771942157\n",
      "The norm of grad vector is  18001.083113506513\n",
      "Loss decreases to  60.94326761677111\n",
      "The norm of grad vector is  17998.04007744505\n",
      "Loss decreases to  60.93929885623515\n",
      "The norm of grad vector is  17994.99794586901\n",
      "Loss decreases to  60.93533143718796\n",
      "The norm of grad vector is  17991.95671796429\n",
      "Loss decreases to  60.93136535900454\n",
      "The norm of grad vector is  17988.916392919265\n",
      "Loss decreases to  60.92740062106041\n",
      "The norm of grad vector is  17985.87696992459\n",
      "Loss decreases to  60.92343722273137\n",
      "The norm of grad vector is  17982.83844817338\n",
      "Loss decreases to  60.919475163394246\n",
      "The norm of grad vector is  17979.800826861047\n",
      "Loss decreases to  60.91551444242611\n",
      "The norm of grad vector is  17976.76410518531\n",
      "Loss decreases to  60.91155505920456\n",
      "The norm of grad vector is  17973.72828234632\n",
      "Loss decreases to  60.907597013107974\n",
      "The norm of grad vector is  17970.693357546494\n",
      "Loss decreases to  60.90364030351508\n",
      "The norm of grad vector is  17967.659329990573\n",
      "Loss decreases to  60.89968492980528\n",
      "The norm of grad vector is  17964.626198885628\n",
      "Loss decreases to  60.8957308913583\n",
      "The norm of grad vector is  17961.593963441035\n",
      "Loss decreases to  60.89177818755485\n",
      "The norm of grad vector is  17958.562622868456\n",
      "Loss decreases to  60.88782681777582\n",
      "The norm of grad vector is  17955.532176381817\n",
      "Loss decreases to  60.88387678140266\n",
      "The norm of grad vector is  17952.502623197382\n",
      "Loss decreases to  60.87992807781743\n",
      "The norm of grad vector is  17949.473962533564\n",
      "Loss decreases to  60.875980706403034\n",
      "The norm of grad vector is  17946.446193611195\n",
      "Loss decreases to  60.87203466654232\n",
      "The norm of grad vector is  17943.419315653227\n",
      "Loss decreases to  60.86808995761909\n",
      "The norm of grad vector is  17940.39332788496\n",
      "Loss decreases to  60.86414657901768\n",
      "The norm of grad vector is  17937.36822953384\n",
      "Loss decreases to  60.860204530122786\n",
      "The norm of grad vector is  17934.344019829576\n",
      "Loss decreases to  60.85626381031969\n",
      "The norm of grad vector is  17931.320698004107\n",
      "Loss decreases to  60.852324418994314\n",
      "The norm of grad vector is  17928.298263291585\n",
      "Loss decreases to  60.84838635553297\n",
      "The norm of grad vector is  17925.276714928354\n",
      "Loss decreases to  60.84444961932273\n",
      "The norm of grad vector is  17922.256052152898\n",
      "Loss decreases to  60.84051420975087\n",
      "The norm of grad vector is  17919.23627420603\n",
      "Loss decreases to  60.83658012620547\n",
      "The norm of grad vector is  17916.217380330556\n",
      "Loss decreases to  60.832647368075\n",
      "The norm of grad vector is  17913.199369771624\n",
      "Loss decreases to  60.82871593474843\n",
      "The norm of grad vector is  17910.18224177642\n",
      "Loss decreases to  60.82478582561545\n",
      "The norm of grad vector is  17907.165995594314\n",
      "Loss decreases to  60.82085704006604\n",
      "The norm of grad vector is  17904.150630476914\n",
      "Loss decreases to  60.816929577490896\n",
      "The norm of grad vector is  17901.136145677807\n",
      "Loss decreases to  60.81300343728101\n",
      "The norm of grad vector is  17898.12254045282\n",
      "Loss decreases to  60.80907861882824\n",
      "The norm of grad vector is  17895.109814059877\n",
      "Loss decreases to  60.80515512152454\n",
      "The norm of grad vector is  17892.09796575902\n",
      "Loss decreases to  60.801232944762816\n",
      "The norm of grad vector is  17889.08699481239\n",
      "Loss decreases to  60.797312087936085\n",
      "The norm of grad vector is  17886.076900484244\n",
      "Loss decreases to  60.793392550438185\n",
      "The norm of grad vector is  17883.067682040877\n",
      "Loss decreases to  60.789474331663236\n",
      "The norm of grad vector is  17880.059338750707\n",
      "Loss decreases to  60.785557431006104\n",
      "The norm of grad vector is  17877.05186988423\n",
      "Loss decreases to  60.78164184786207\n",
      "The norm of grad vector is  17874.04527471402\n",
      "Loss decreases to  60.7777275816269\n",
      "The norm of grad vector is  17871.039552514667\n",
      "Loss decreases to  60.77381463169678\n",
      "The norm of grad vector is  17868.034702562843\n",
      "Loss decreases to  60.76990299746855\n",
      "The norm of grad vector is  17865.030724137276\n",
      "Loss decreases to  60.76599267833965\n",
      "The norm of grad vector is  17862.02761651871\n",
      "Loss decreases to  60.762083673707785\n",
      "The norm of grad vector is  17859.025378989933\n",
      "Loss decreases to  60.758175982971096\n",
      "The norm of grad vector is  17856.0240108357\n",
      "Loss decreases to  60.7542696055287\n",
      "The norm of grad vector is  17853.02351134288\n",
      "Loss decreases to  60.7503645407799\n",
      "The norm of grad vector is  17850.023879800272\n",
      "Loss decreases to  60.74646078812441\n",
      "The norm of grad vector is  17847.025115498705\n",
      "Loss decreases to  60.74255834696261\n",
      "The norm of grad vector is  17844.027217730993\n",
      "Loss decreases to  60.73865721669515\n",
      "The norm of grad vector is  17841.03018579195\n",
      "Loss decreases to  60.73475739672372\n",
      "The norm of grad vector is  17838.034018978316\n",
      "Loss decreases to  60.73085888644993\n",
      "The norm of grad vector is  17835.038716588868\n",
      "Loss decreases to  60.72696168527604\n",
      "The norm of grad vector is  17832.044277924295\n",
      "Loss decreases to  60.72306579260493\n",
      "The norm of grad vector is  17829.05070228729\n",
      "Loss decreases to  60.71917120783998\n",
      "The norm of grad vector is  17826.057988982455\n",
      "Loss decreases to  60.71527793038501\n",
      "The norm of grad vector is  17823.066137316313\n",
      "Loss decreases to  60.7113859596443\n",
      "The norm of grad vector is  17820.075146597406\n",
      "Loss decreases to  60.70749529502249\n",
      "The norm of grad vector is  17817.08501613612\n",
      "Loss decreases to  60.70360593592512\n",
      "The norm of grad vector is  17814.09574524481\n",
      "Loss decreases to  60.69971788175779\n",
      "The norm of grad vector is  17811.107333237684\n",
      "Loss decreases to  60.69583113192674\n",
      "The norm of grad vector is  17808.11977943091\n",
      "Loss decreases to  60.69194568583884\n",
      "The norm of grad vector is  17805.13308314258\n",
      "Loss decreases to  60.688061542901174\n",
      "The norm of grad vector is  17802.147243692576\n",
      "Loss decreases to  60.68417870252165\n",
      "The norm of grad vector is  17799.162260402787\n",
      "Loss decreases to  60.680297164108325\n",
      "The norm of grad vector is  17796.17813259685\n",
      "Loss decreases to  60.67641692706983\n",
      "The norm of grad vector is  17793.194859600404\n",
      "Loss decreases to  60.67253799081554\n",
      "The norm of grad vector is  17790.212440740823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss decreases to  60.66866035475483\n",
      "The norm of grad vector is  17787.230875347486\n",
      "Loss decreases to  60.664784018298164\n",
      "The norm of grad vector is  17784.250162751476\n",
      "Loss decreases to  60.66090898085578\n",
      "The norm of grad vector is  17781.270302285768\n",
      "Loss decreases to  60.65703524183903\n",
      "The norm of grad vector is  17778.291293285278\n",
      "Loss decreases to  60.65316280065936\n",
      "The norm of grad vector is  17775.313135086566\n",
      "Loss decreases to  60.64929165672875\n",
      "The norm of grad vector is  17772.335827028117\n",
      "Loss decreases to  60.645421809459855\n",
      "The norm of grad vector is  17769.35936845029\n",
      "Loss decreases to  60.641553258265525\n",
      "The norm of grad vector is  17766.383758695116\n",
      "Loss decreases to  60.63768600255921\n",
      "The norm of grad vector is  17763.408997106533\n",
      "Loss decreases to  60.63382004175482\n",
      "The norm of grad vector is  17760.435083030232\n",
      "Loss decreases to  60.62995537526698\n",
      "The norm of grad vector is  17757.462015813737\n",
      "Loss decreases to  60.62609200251029\n",
      "The norm of grad vector is  17754.489794806264\n",
      "Loss decreases to  60.62222992290009\n",
      "The norm of grad vector is  17751.518419358872\n",
      "Loss decreases to  60.618369135852305\n",
      "The norm of grad vector is  17748.547888824403\n",
      "Loss decreases to  60.61450964078303\n",
      "The norm of grad vector is  17745.578202557455\n",
      "Loss decreases to  60.610651437109226\n",
      "The norm of grad vector is  17742.6093599143\n",
      "Loss decreases to  60.60679452424783\n",
      "The norm of grad vector is  17739.641360253067\n",
      "Loss decreases to  60.602938901616774\n",
      "The norm of grad vector is  17736.67420293358\n",
      "Loss decreases to  60.59908456863391\n",
      "The norm of grad vector is  17733.707887317425\n",
      "Loss decreases to  60.59523152471799\n",
      "The norm of grad vector is  17730.742412767864\n",
      "Loss decreases to  60.591379769287876\n",
      "The norm of grad vector is  17727.77777864994\n",
      "Loss decreases to  60.58752930176318\n",
      "The norm of grad vector is  17724.813984330376\n",
      "Loss decreases to  60.58368012156394\n",
      "The norm of grad vector is  17721.85102917764\n",
      "Loss decreases to  60.5798322281104\n",
      "The norm of grad vector is  17718.888912561924\n",
      "Loss decreases to  60.57598562082358\n",
      "The norm of grad vector is  17715.92763385499\n",
      "Loss decreases to  60.57214029912462\n",
      "The norm of grad vector is  17712.967192430482\n",
      "Loss decreases to  60.56829626243541\n",
      "The norm of grad vector is  17710.00758766357\n",
      "Loss decreases to  60.56445351017814\n",
      "The norm of grad vector is  17707.0488189312\n",
      "Loss decreases to  60.56061204177547\n",
      "The norm of grad vector is  17704.09088561193\n",
      "Loss decreases to  60.556771856650705\n",
      "The norm of grad vector is  17701.13378708605\n",
      "Loss decreases to  60.55293295422727\n",
      "The norm of grad vector is  17698.1775227355\n",
      "Loss decreases to  60.54909533392915\n",
      "The norm of grad vector is  17695.22209194378\n",
      "Loss decreases to  60.54525899518094\n",
      "The norm of grad vector is  17692.267494096162\n",
      "Loss decreases to  60.54142393740748\n",
      "The norm of grad vector is  17689.313728579527\n",
      "Loss decreases to  60.5375901600342\n",
      "The norm of grad vector is  17686.36079478233\n",
      "Loss decreases to  60.53375766248697\n",
      "The norm of grad vector is  17683.408692094756\n",
      "Loss decreases to  60.529926444191986\n",
      "The norm of grad vector is  17680.45741990855\n",
      "Loss decreases to  60.52609650457583\n",
      "The norm of grad vector is  17677.506977617068\n",
      "Loss decreases to  60.52226784306589\n",
      "The norm of grad vector is  17674.55736461533\n",
      "Loss decreases to  60.51844045908954\n",
      "The norm of grad vector is  17671.608580299926\n",
      "Loss decreases to  60.51461435207515\n",
      "The norm of grad vector is  17668.66062406906\n",
      "Loss decreases to  60.51078952145067\n",
      "The norm of grad vector is  17665.713495322518\n",
      "Loss decreases to  60.506965966645524\n",
      "The norm of grad vector is  17662.76719346173\n",
      "Loss decreases to  60.50314368708874\n",
      "The norm of grad vector is  17659.82171788962\n",
      "Loss decreases to  60.49932268221031\n",
      "The norm of grad vector is  17656.877068010737\n",
      "Loss decreases to  60.49550295144024\n",
      "The norm of grad vector is  17653.933243231244\n",
      "Loss decreases to  60.491684494209395\n",
      "The norm of grad vector is  17650.99024295879\n",
      "Loss decreases to  60.4878673099488\n",
      "The norm of grad vector is  17648.04806660264\n",
      "Loss decreases to  60.484051398089896\n",
      "The norm of grad vector is  17645.106713573616\n",
      "Loss decreases to  60.48023675806467\n",
      "The norm of grad vector is  17642.16618328403\n",
      "Loss decreases to  60.47642338930566\n",
      "The norm of grad vector is  17639.22647514781\n",
      "Loss decreases to  60.472611291245514\n",
      "The norm of grad vector is  17636.28758858039\n",
      "Loss decreases to  60.46880046331762\n",
      "The norm of grad vector is  17633.349522998713\n",
      "Loss decreases to  60.46499090495553\n",
      "The norm of grad vector is  17630.412277821273\n",
      "Loss decreases to  60.46118261559349\n",
      "The norm of grad vector is  17627.47585246811\n",
      "Loss decreases to  60.45737559466586\n",
      "The norm of grad vector is  17624.540246360746\n",
      "Loss decreases to  60.453569841607816\n",
      "The norm of grad vector is  17621.60545892221\n",
      "Loss decreases to  60.44976535585464\n",
      "The norm of grad vector is  17618.671489577027\n",
      "Loss decreases to  60.44596213684215\n",
      "The norm of grad vector is  17615.738337751256\n",
      "Loss decreases to  60.44216018400657\n",
      "The norm of grad vector is  17612.80600287246\n",
      "Loss decreases to  60.43835949678462\n",
      "The norm of grad vector is  17609.87448436961\n",
      "Loss decreases to  60.43456007461331\n",
      "The norm of grad vector is  17606.943781673264\n",
      "Loss decreases to  60.43076191693019\n",
      "The norm of grad vector is  17604.01389421536\n",
      "Loss decreases to  60.42696502317316\n",
      "The norm of grad vector is  17601.08482142939\n",
      "Loss decreases to  60.42316939278066\n",
      "The norm of grad vector is  17598.15656275024\n",
      "Loss decreases to  60.41937502519125\n",
      "The norm of grad vector is  17595.229117614283\n",
      "Loss decreases to  60.41558191984436\n",
      "The norm of grad vector is  17592.302485459368\n",
      "Loss decreases to  60.41179007617942\n",
      "The norm of grad vector is  17589.376665724798\n",
      "Loss decreases to  60.40799949363669\n",
      "The norm of grad vector is  17586.451657851278\n",
      "Loss decreases to  60.404210171656175\n",
      "The norm of grad vector is  17583.527461280977\n",
      "Loss decreases to  60.40042210967909\n",
      "The norm of grad vector is  17580.60407545749\n",
      "Loss decreases to  60.39663530714656\n",
      "The norm of grad vector is  17577.6814998259\n",
      "Loss decreases to  60.392849763500244\n",
      "The norm of grad vector is  17574.759733832605\n",
      "Loss decreases to  60.38906547818223\n",
      "The norm of grad vector is  17571.83877692552\n",
      "Loss decreases to  60.38528245063513\n",
      "The norm of grad vector is  17568.918628553874\n",
      "Loss decreases to  60.381500680301684\n",
      "The norm of grad vector is  17565.99928816841\n",
      "Loss decreases to  60.37772016662535\n",
      "The norm of grad vector is  17563.08075522119\n",
      "Loss decreases to  60.37394090904987\n",
      "The norm of grad vector is  17560.16302916573\n",
      "Loss decreases to  60.37016290701923\n",
      "The norm of grad vector is  17557.246109456915\n",
      "Loss decreases to  60.36638615997801\n",
      "The norm of grad vector is  17554.329995551023\n",
      "Loss decreases to  60.362610667371335\n",
      "The norm of grad vector is  17551.414686905686\n",
      "Loss decreases to  60.35883642864434\n",
      "The norm of grad vector is  17548.50018297995\n",
      "Loss decreases to  60.35506344324285\n",
      "The norm of grad vector is  17545.58648323424\n",
      "Loss decreases to  60.35129171061316\n",
      "The norm of grad vector is  17542.673587130288\n",
      "Loss decreases to  60.34752123020179\n",
      "The norm of grad vector is  17539.761494131246\n",
      "Loss decreases to  60.34375200145543\n",
      "The norm of grad vector is  17536.85020370164\n",
      "Loss decreases to  60.339984023821806\n",
      "The norm of grad vector is  17533.939715307264\n",
      "Loss decreases to  60.336217296748664\n",
      "The norm of grad vector is  17531.03002841531\n",
      "Loss decreases to  60.33245181968396\n",
      "The norm of grad vector is  17528.121142494325\n",
      "Loss decreases to  60.3286875920764\n",
      "The norm of grad vector is  17525.213057014174\n",
      "Loss decreases to  60.32492461337506\n",
      "The norm of grad vector is  17522.305771446085\n",
      "Loss decreases to  60.3211628830291\n",
      "The norm of grad vector is  17519.399285262567\n",
      "Loss decreases to  60.31740240048842\n",
      "The norm of grad vector is  17516.49359793747\n",
      "Loss decreases to  60.3136431652032\n",
      "The norm of grad vector is  17513.588708945976\n",
      "Loss decreases to  60.30988517662393\n",
      "The norm of grad vector is  17510.684617764557\n",
      "Loss decreases to  60.3061284342015\n",
      "The norm of grad vector is  17507.781323871015\n",
      "Loss decreases to  60.302372937387446\n",
      "The norm of grad vector is  17504.878826744407\n",
      "Loss decreases to  60.298618685633414\n",
      "The norm of grad vector is  17501.97712586518\n",
      "Loss decreases to  60.29486567839151\n",
      "The norm of grad vector is  17499.076220714996\n",
      "Loss decreases to  60.29111391511431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  17496.176110776814\n",
      "Loss decreases to  60.287363395254694\n",
      "The norm of grad vector is  17493.276795534963\n",
      "Loss decreases to  60.28361411826608\n",
      "The norm of grad vector is  17490.378274474908\n",
      "Loss decreases to  60.27986608360192\n",
      "The norm of grad vector is  17487.480547083498\n",
      "Loss decreases to  60.27611929071657\n",
      "The norm of grad vector is  17484.583612848804\n",
      "Loss decreases to  60.27237373906434\n",
      "The norm of grad vector is  17481.687471260186\n",
      "Loss decreases to  60.268629428100255\n",
      "The norm of grad vector is  17478.792121808277\n",
      "Loss decreases to  60.26488635727947\n",
      "The norm of grad vector is  17475.897563984945\n",
      "Loss decreases to  60.26114452605747\n",
      "The norm of grad vector is  17473.003797283276\n",
      "Loss decreases to  60.25740393389057\n",
      "The norm of grad vector is  17470.11082119767\n",
      "Loss decreases to  60.25366458023503\n",
      "The norm of grad vector is  17467.218635223737\n",
      "Loss decreases to  60.249926464547556\n",
      "The norm of grad vector is  17464.327238858292\n",
      "Loss decreases to  60.24618958628542\n",
      "The norm of grad vector is  17461.436631599496\n",
      "Loss decreases to  60.242453944906195\n",
      "The norm of grad vector is  17458.546812946584\n",
      "Loss decreases to  60.2387195398678\n",
      "The norm of grad vector is  17455.657782400154\n",
      "Loss decreases to  60.234986370628505\n",
      "The norm of grad vector is  17452.769539461933\n",
      "Loss decreases to  60.23125443664703\n",
      "The norm of grad vector is  17449.882083634882\n",
      "Loss decreases to  60.22752373738253\n",
      "The norm of grad vector is  17446.99541442321\n",
      "Loss decreases to  60.22379427229428\n",
      "The norm of grad vector is  17444.10953133232\n",
      "Loss decreases to  60.220066040842404\n",
      "The norm of grad vector is  17441.224433868814\n",
      "Loss decreases to  60.216339042486894\n",
      "The norm of grad vector is  17438.340121540426\n",
      "Loss decreases to  60.212613276688366\n",
      "The norm of grad vector is  17435.456593856205\n",
      "Loss decreases to  60.208888742907824\n",
      "The norm of grad vector is  17432.573850326287\n",
      "Loss decreases to  60.20516544060657\n",
      "The norm of grad vector is  17429.691890462072\n",
      "Loss decreases to  60.20144336924644\n",
      "The norm of grad vector is  17426.810713776096\n",
      "Loss decreases to  60.19772252828949\n",
      "The norm of grad vector is  17423.93031978205\n",
      "Loss decreases to  60.194002917197984\n",
      "The norm of grad vector is  17421.05070799487\n",
      "Loss decreases to  60.19028453543498\n",
      "The norm of grad vector is  17418.171877930567\n",
      "Loss decreases to  60.18656738246355\n",
      "The norm of grad vector is  17415.29382910642\n",
      "Loss decreases to  60.18285145774733\n",
      "The norm of grad vector is  17412.41656104077\n",
      "Loss decreases to  60.179136760750374\n",
      "The norm of grad vector is  17409.540073253178\n",
      "Loss decreases to  60.175423290936834\n",
      "The norm of grad vector is  17406.664365264318\n",
      "Loss decreases to  60.17171104777152\n",
      "The norm of grad vector is  17403.78943659603\n",
      "Loss decreases to  60.16800003071938\n",
      "The norm of grad vector is  17400.915286771306\n",
      "Loss decreases to  60.16429023924589\n",
      "The norm of grad vector is  17398.04191531427\n",
      "Loss decreases to  60.16058167281693\n",
      "The norm of grad vector is  17395.169321750127\n",
      "Loss decreases to  60.156874330898525\n",
      "The norm of grad vector is  17392.29750560534\n",
      "Loss decreases to  60.15316821295733\n",
      "The norm of grad vector is  17389.42646640736\n",
      "Loss decreases to  60.149463318460164\n",
      "The norm of grad vector is  17386.556203684824\n",
      "Loss decreases to  60.14575964687422\n",
      "The norm of grad vector is  17383.686716967484\n",
      "Loss decreases to  60.1420571976672\n",
      "The norm of grad vector is  17380.81800578622\n",
      "Loss decreases to  60.13835597030714\n",
      "The norm of grad vector is  17377.950069673003\n",
      "Loss decreases to  60.134655964262244\n",
      "The norm of grad vector is  17375.082908160886\n",
      "Loss decreases to  60.13095717900133\n",
      "The norm of grad vector is  17372.21652078409\n",
      "Loss decreases to  60.12725961399346\n",
      "The norm of grad vector is  17369.350907077864\n",
      "Loss decreases to  60.12356326870792\n",
      "The norm of grad vector is  17366.486066578575\n",
      "Loss decreases to  60.11986814261468\n",
      "The norm of grad vector is  17363.62199882369\n",
      "Loss decreases to  60.11617423518393\n",
      "The norm of grad vector is  17360.75870335178\n",
      "Loss decreases to  60.11248154588602\n",
      "The norm of grad vector is  17357.89617970249\n",
      "Loss decreases to  60.10879007419181\n",
      "The norm of grad vector is  17355.03442741647\n",
      "Loss decreases to  60.10509981957264\n",
      "The norm of grad vector is  17352.17344603556\n",
      "Loss decreases to  60.10141078150004\n",
      "The norm of grad vector is  17349.31323510254\n",
      "Loss decreases to  60.09772295944592\n",
      "The norm of grad vector is  17346.4537941614\n",
      "Loss decreases to  60.09403635288264\n",
      "The norm of grad vector is  17343.595122757135\n",
      "Loss decreases to  60.09035096128287\n",
      "The norm of grad vector is  17340.73722043571\n",
      "Loss decreases to  60.08666678411946\n",
      "The norm of grad vector is  17337.880086744248\n",
      "Loss decreases to  60.082983820865934\n",
      "The norm of grad vector is  17335.023721230933\n",
      "Loss decreases to  60.07930207099595\n",
      "The norm of grad vector is  17332.168123444924\n",
      "Loss decreases to  60.07562153398347\n",
      "The norm of grad vector is  17329.31329293646\n",
      "Loss decreases to  60.071942209303025\n",
      "The norm of grad vector is  17326.45922925683\n",
      "Loss decreases to  60.068264096429395\n",
      "The norm of grad vector is  17323.605931958347\n",
      "Loss decreases to  60.064587194837586\n",
      "The norm of grad vector is  17320.753400594327\n",
      "Loss decreases to  60.06091150400315\n",
      "The norm of grad vector is  17317.90163471919\n",
      "Loss decreases to  60.05723702340192\n",
      "The norm of grad vector is  17315.05063388831\n",
      "Loss decreases to  60.05356375250998\n",
      "The norm of grad vector is  17312.200397658067\n",
      "Loss decreases to  60.04989169080399\n",
      "The norm of grad vector is  17309.35092558597\n",
      "Loss decreases to  60.04622083776063\n",
      "The norm of grad vector is  17306.50221723042\n",
      "Loss decreases to  60.04255119285727\n",
      "The norm of grad vector is  17303.654272150903\n",
      "Loss decreases to  60.038882755571265\n",
      "The norm of grad vector is  17300.80708990786\n",
      "Loss decreases to  60.03521552538082\n",
      "The norm of grad vector is  17297.960670062745\n",
      "Loss decreases to  60.03154950176389\n",
      "The norm of grad vector is  17295.11501217808\n",
      "Loss decreases to  60.02788468419925\n",
      "The norm of grad vector is  17292.27011581723\n",
      "Loss decreases to  60.024221072165716\n",
      "The norm of grad vector is  17289.425980544805\n",
      "Loss decreases to  60.02055866514264\n",
      "The norm of grad vector is  17286.582605926105\n",
      "Loss decreases to  60.016897462609656\n",
      "The norm of grad vector is  17283.739991527622\n",
      "Loss decreases to  60.013237464046775\n",
      "The norm of grad vector is  17280.898136916738\n",
      "Loss decreases to  60.00957866893408\n",
      "The norm of grad vector is  17278.057041661847\n",
      "Loss decreases to  60.00592107675248\n",
      "The norm of grad vector is  17275.216705332317\n",
      "Loss decreases to  60.002264686982805\n",
      "The norm of grad vector is  17272.377127498465\n",
      "Loss decreases to  59.99860949910653\n",
      "The norm of grad vector is  17269.53830773161\n",
      "Loss decreases to  59.99495551260519\n",
      "The norm of grad vector is  17266.700245603974\n",
      "Loss decreases to  59.99130272696078\n",
      "The norm of grad vector is  17263.86294068881\n",
      "Loss decreases to  59.98765114165578\n",
      "The norm of grad vector is  17261.0263925603\n",
      "Loss decreases to  59.9840007561728\n",
      "The norm of grad vector is  17258.190600793525\n",
      "Loss decreases to  59.980351569994795\n",
      "The norm of grad vector is  17255.355564964593\n",
      "Loss decreases to  59.97670358260537\n",
      "The norm of grad vector is  17252.521284650546\n",
      "Loss decreases to  59.97305679348784\n",
      "The norm of grad vector is  17249.68775942931\n",
      "Loss decreases to  59.96941120212664\n",
      "The norm of grad vector is  17246.854988879866\n",
      "Loss decreases to  59.96576680800578\n",
      "The norm of grad vector is  17244.022972581988\n",
      "Loss decreases to  59.96212361061017\n",
      "The norm of grad vector is  17241.19171011648\n",
      "Loss decreases to  59.95848160942483\n",
      "The norm of grad vector is  17238.361201065032\n",
      "Loss decreases to  59.954840803935085\n",
      "The norm of grad vector is  17235.53144501029\n",
      "Loss decreases to  59.951201193626645\n",
      "The norm of grad vector is  17232.70244153581\n",
      "Loss decreases to  59.947562777985496\n",
      "The norm of grad vector is  17229.874190226034\n",
      "Loss decreases to  59.94392555649818\n",
      "The norm of grad vector is  17227.04669066639\n",
      "Loss decreases to  59.940289528651114\n",
      "The norm of grad vector is  17224.21994244316\n",
      "Loss decreases to  59.936654693931594\n",
      "The norm of grad vector is  17221.39394514355\n",
      "Loss decreases to  59.9330210518269\n",
      "The norm of grad vector is  17218.568698355706\n",
      "Loss decreases to  59.92938860182456\n",
      "The norm of grad vector is  17215.744201668622\n",
      "Loss decreases to  59.92575734341281\n",
      "The norm of grad vector is  17212.9204546722\n",
      "Loss decreases to  59.92212727607988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  17210.097456957305\n",
      "Loss decreases to  59.9184983993145\n",
      "The norm of grad vector is  17207.275208115592\n",
      "Loss decreases to  59.914870712605534\n",
      "The norm of grad vector is  17204.453707739714\n",
      "Loss decreases to  59.91124421544258\n",
      "The norm of grad vector is  17201.6329554231\n",
      "Loss decreases to  59.907618907314976\n",
      "The norm of grad vector is  17198.81295076016\n",
      "Loss decreases to  59.90399478771293\n",
      "The norm of grad vector is  17195.993693346154\n",
      "Loss decreases to  59.900371856126604\n",
      "The norm of grad vector is  17193.17518277716\n",
      "Loss decreases to  59.896750112046746\n",
      "The norm of grad vector is  17190.357418650223\n",
      "Loss decreases to  59.89312955496423\n",
      "The norm of grad vector is  17187.540400563157\n",
      "Loss decreases to  59.88951018437044\n",
      "The norm of grad vector is  17184.72412811475\n",
      "Loss decreases to  59.885891999756815\n",
      "The norm of grad vector is  17181.90860090461\n",
      "Loss decreases to  59.88227500061551\n",
      "The norm of grad vector is  17179.093818533187\n",
      "Loss decreases to  59.87865918643852\n",
      "The norm of grad vector is  17176.279780601806\n",
      "Loss decreases to  59.87504455671856\n",
      "The norm of grad vector is  17173.466486712674\n",
      "Loss decreases to  59.87143111094849\n",
      "The norm of grad vector is  17170.653936468712\n",
      "Loss decreases to  59.86781884862152\n",
      "The norm of grad vector is  17167.842129473935\n",
      "Loss decreases to  59.86420776923115\n",
      "The norm of grad vector is  17165.031065333\n",
      "Loss decreases to  59.86059787227132\n",
      "The norm of grad vector is  17162.220743651505\n",
      "Loss decreases to  59.8569891572362\n",
      "The norm of grad vector is  17159.411164035853\n",
      "Loss decreases to  59.85338162362008\n",
      "The norm of grad vector is  17156.602326093256\n",
      "Loss decreases to  59.84977527091805\n",
      "The norm of grad vector is  17153.794229431867\n",
      "Loss decreases to  59.84617009862496\n",
      "The norm of grad vector is  17150.986873660524\n",
      "Loss decreases to  59.842566106236504\n",
      "The norm of grad vector is  17148.180258389028\n",
      "Loss decreases to  59.83896329324837\n",
      "The norm of grad vector is  17145.3743832279\n",
      "Loss decreases to  59.835361659156476\n",
      "The norm of grad vector is  17142.569247788564\n",
      "Loss decreases to  59.831761203457425\n",
      "The norm of grad vector is  17139.764851683205\n",
      "Loss decreases to  59.828161925647876\n",
      "The norm of grad vector is  17136.96119452487\n",
      "Loss decreases to  59.82456382522465\n",
      "The norm of grad vector is  17134.158275927362\n",
      "Loss decreases to  59.82096690168535\n",
      "The norm of grad vector is  17131.356095505347\n",
      "Loss decreases to  59.81737115452755\n",
      "The norm of grad vector is  17128.554652874263\n",
      "Loss decreases to  59.81377658324913\n",
      "The norm of grad vector is  17125.753947650403\n",
      "Loss decreases to  59.81018318734845\n",
      "The norm of grad vector is  17122.95397945081\n",
      "Loss decreases to  59.80659096632412\n",
      "The norm of grad vector is  17120.15474789336\n",
      "Loss decreases to  59.80299991967486\n",
      "The norm of grad vector is  17117.3562525967\n",
      "Loss decreases to  59.79941004690012\n",
      "The norm of grad vector is  17114.558493180255\n",
      "Loss decreases to  59.795821347499235\n",
      "The norm of grad vector is  17111.76146926431\n",
      "Loss decreases to  59.79223382097223\n",
      "The norm of grad vector is  17108.965180469884\n",
      "Loss decreases to  59.78864746681909\n",
      "The norm of grad vector is  17106.16962641878\n",
      "Loss decreases to  59.78506228454027\n",
      "The norm of grad vector is  17103.374806733587\n",
      "Loss decreases to  59.781478273636615\n",
      "The norm of grad vector is  17100.5807210377\n",
      "Loss decreases to  59.77789543360921\n",
      "The norm of grad vector is  17097.787368955243\n",
      "Loss decreases to  59.77431376395928\n",
      "The norm of grad vector is  17094.994750111186\n",
      "Loss decreases to  59.77073326418873\n",
      "The norm of grad vector is  17092.202864131174\n",
      "Loss decreases to  59.76715393379942\n",
      "The norm of grad vector is  17089.411710641725\n",
      "Loss decreases to  59.763575772293684\n",
      "The norm of grad vector is  17086.621289270035\n",
      "Loss decreases to  59.75999877917402\n",
      "The norm of grad vector is  17083.83159964409\n",
      "Loss decreases to  59.75642295394356\n",
      "The norm of grad vector is  17081.042641392673\n",
      "Loss decreases to  59.75284829610538\n",
      "The norm of grad vector is  17078.25441414526\n",
      "Loss decreases to  59.749274805162976\n",
      "The norm of grad vector is  17075.46691753215\n",
      "Loss decreases to  59.74570248062028\n",
      "The norm of grad vector is  17072.68015118435\n",
      "Loss decreases to  59.742131321981375\n",
      "The norm of grad vector is  17069.894114733594\n",
      "Loss decreases to  59.738561328750656\n",
      "The norm of grad vector is  17067.108807812438\n",
      "Loss decreases to  59.73499250043289\n",
      "The norm of grad vector is  17064.324230054128\n",
      "Loss decreases to  59.73142483653316\n",
      "The norm of grad vector is  17061.54038109263\n",
      "Loss decreases to  59.72785833655676\n",
      "The norm of grad vector is  17058.75726056272\n",
      "Loss decreases to  59.724293000009276\n",
      "The norm of grad vector is  17055.974868099882\n",
      "Loss decreases to  59.72072882639682\n",
      "The norm of grad vector is  17053.193203340263\n",
      "Loss decreases to  59.7171658152256\n",
      "The norm of grad vector is  17050.412265920866\n",
      "Loss decreases to  59.71360396600202\n",
      "The norm of grad vector is  17047.6320554793\n",
      "Loss decreases to  59.71004327823299\n",
      "The norm of grad vector is  17044.852571653988\n",
      "Loss decreases to  59.70648375142571\n",
      "The norm of grad vector is  17042.073814084062\n",
      "Loss decreases to  59.702925385087646\n",
      "The norm of grad vector is  17039.295782409317\n",
      "Loss decreases to  59.69936817872642\n",
      "The norm of grad vector is  17036.518476270314\n",
      "Loss decreases to  59.69581213185026\n",
      "The norm of grad vector is  17033.741895308343\n",
      "Loss decreases to  59.69225724396729\n",
      "The norm of grad vector is  17030.966039165374\n",
      "Loss decreases to  59.68870351458636\n",
      "The norm of grad vector is  17028.190907484077\n",
      "Loss decreases to  59.685150943216186\n",
      "The norm of grad vector is  17025.41649990788\n",
      "Loss decreases to  59.68159952936613\n",
      "The norm of grad vector is  17022.642816080883\n",
      "Loss decreases to  59.678049272545785\n",
      "The norm of grad vector is  17019.86985564786\n",
      "Loss decreases to  59.67450017226485\n",
      "The norm of grad vector is  17017.097618254356\n",
      "Loss decreases to  59.67095222803348\n",
      "The norm of grad vector is  17014.326103546577\n",
      "Loss decreases to  59.66740543936215\n",
      "The norm of grad vector is  17011.5553111714\n",
      "Loss decreases to  59.663859805761426\n",
      "The norm of grad vector is  17008.78524077645\n",
      "Loss decreases to  59.66031532674247\n",
      "The norm of grad vector is  17006.015892009975\n",
      "Loss decreases to  59.65677200181642\n",
      "The norm of grad vector is  17003.24726452096\n",
      "Loss decreases to  59.65322983049507\n",
      "The norm of grad vector is  17000.479357959077\n",
      "Loss decreases to  59.64968881229013\n",
      "The norm of grad vector is  16997.712171974654\n",
      "Loss decreases to  59.64614894671371\n",
      "The norm of grad vector is  16994.945706218725\n",
      "Loss decreases to  59.64261023327852\n",
      "The norm of grad vector is  16992.179960342954\n",
      "Loss decreases to  59.63907267149703\n",
      "The norm of grad vector is  16989.414933999764\n",
      "Loss decreases to  59.63553626088261\n",
      "The norm of grad vector is  16986.650626842187\n",
      "Loss decreases to  59.63200100094839\n",
      "The norm of grad vector is  16983.887038523942\n",
      "Loss decreases to  59.62846689120795\n",
      "The norm of grad vector is  16981.124168699396\n",
      "Loss decreases to  59.62493393117532\n",
      "The norm of grad vector is  16978.362017023646\n",
      "Loss decreases to  59.621402120364735\n",
      "The norm of grad vector is  16975.60058315241\n",
      "Loss decreases to  59.61787145829061\n",
      "The norm of grad vector is  16972.839866742004\n",
      "Loss decreases to  59.614341944467654\n",
      "The norm of grad vector is  16970.079867449553\n",
      "Loss decreases to  59.61081357841112\n",
      "The norm of grad vector is  16967.320584932695\n",
      "Loss decreases to  59.607286359636326\n",
      "The norm of grad vector is  16964.56201884983\n",
      "Loss decreases to  59.60376028765884\n",
      "The norm of grad vector is  16961.804168859948\n",
      "Loss decreases to  59.60023536199464\n",
      "The norm of grad vector is  16959.047034622687\n",
      "Loss decreases to  59.59671158216004\n",
      "The norm of grad vector is  16956.29061579835\n",
      "Loss decreases to  59.59318894767128\n",
      "The norm of grad vector is  16953.5349120479\n",
      "Loss decreases to  59.58966745804545\n",
      "The norm of grad vector is  16950.779923032922\n",
      "Loss decreases to  59.58614711279937\n",
      "The norm of grad vector is  16948.025648415678\n",
      "Loss decreases to  59.58262791145073\n",
      "The norm of grad vector is  16945.272087859\n",
      "Loss decreases to  59.579109853516854\n",
      "The norm of grad vector is  16942.51924102642\n",
      "Loss decreases to  59.575592938515804\n",
      "The norm of grad vector is  16939.767107582087\n",
      "Loss decreases to  59.57207716596584\n",
      "The norm of grad vector is  16937.015687190764\n",
      "Loss decreases to  59.56856253538532\n",
      "The norm of grad vector is  16934.264979517837\n",
      "Loss decreases to  59.56504904629318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  16931.51498422938\n",
      "Loss decreases to  59.56153669820848\n",
      "The norm of grad vector is  16928.765700992022\n",
      "Loss decreases to  59.558025490650536\n",
      "The norm of grad vector is  16926.017129473057\n",
      "Loss decreases to  59.55451542313887\n",
      "The norm of grad vector is  16923.269269340406\n",
      "Loss decreases to  59.55100649519356\n",
      "The norm of grad vector is  16920.522120262536\n",
      "Loss decreases to  59.54749870633467\n",
      "The norm of grad vector is  16917.77568190864\n",
      "Loss decreases to  59.54399205608273\n",
      "The norm of grad vector is  16915.029953948415\n",
      "Loss decreases to  59.54048654395844\n",
      "The norm of grad vector is  16912.284936052292\n",
      "Loss decreases to  59.53698216948291\n",
      "The norm of grad vector is  16909.54062789123\n",
      "Loss decreases to  59.533478932177424\n",
      "The norm of grad vector is  16906.797029136767\n",
      "Loss decreases to  59.52997683156355\n",
      "The norm of grad vector is  16904.05413946113\n",
      "Loss decreases to  59.526475867163164\n",
      "The norm of grad vector is  16901.31195853713\n",
      "Loss decreases to  59.522976038498435\n",
      "The norm of grad vector is  16898.570486038127\n",
      "Loss decreases to  59.51947734509177\n",
      "The norm of grad vector is  16895.829721638132\n",
      "Loss decreases to  59.515979786465884\n",
      "The norm of grad vector is  16893.089665011732\n",
      "Loss decreases to  59.51248336214372\n",
      "The norm of grad vector is  16890.350315834115\n",
      "Loss decreases to  59.50898807164859\n",
      "The norm of grad vector is  16887.61167378104\n",
      "Loss decreases to  59.50549391450406\n",
      "The norm of grad vector is  16884.87373852892\n",
      "Loss decreases to  59.50200089023382\n",
      "The norm of grad vector is  16882.136509754728\n",
      "Loss decreases to  59.49850899836196\n",
      "The norm of grad vector is  16879.399987135952\n",
      "Loss decreases to  59.49501823841305\n",
      "The norm of grad vector is  16876.664170350785\n",
      "Loss decreases to  59.491528609911526\n",
      "The norm of grad vector is  16873.929059077913\n",
      "Loss decreases to  59.48804011238238\n",
      "The norm of grad vector is  16871.19465299663\n",
      "Loss decreases to  59.484552745350754\n",
      "The norm of grad vector is  16868.46095178682\n",
      "Loss decreases to  59.481066508342145\n",
      "The norm of grad vector is  16865.727955128983\n",
      "Loss decreases to  59.47758140088225\n",
      "The norm of grad vector is  16862.99566270407\n",
      "Loss decreases to  59.47409742249709\n",
      "The norm of grad vector is  16860.26407419374\n",
      "Loss decreases to  59.47061457271294\n",
      "The norm of grad vector is  16857.533189280126\n",
      "Loss decreases to  59.46713285105646\n",
      "The norm of grad vector is  16854.803007645995\n",
      "Loss decreases to  59.4636522570543\n",
      "The norm of grad vector is  16852.073528974663\n",
      "Loss decreases to  59.46017279023371\n",
      "The norm of grad vector is  16849.34475294998\n",
      "Loss decreases to  59.45669445012186\n",
      "The norm of grad vector is  16846.6166792564\n",
      "Loss decreases to  59.45321723624665\n",
      "The norm of grad vector is  16843.88930757893\n",
      "Loss decreases to  59.44974114813579\n",
      "The norm of grad vector is  16841.16263760311\n",
      "Loss decreases to  59.446266185317576\n",
      "The norm of grad vector is  16838.436669015056\n",
      "Loss decreases to  59.44279234732038\n",
      "The norm of grad vector is  16835.711401501452\n",
      "Loss decreases to  59.43931963367291\n",
      "The norm of grad vector is  16832.986834749518\n",
      "Loss decreases to  59.435848043904244\n",
      "The norm of grad vector is  16830.262968447\n",
      "Loss decreases to  59.432377577543605\n",
      "The norm of grad vector is  16827.539802282274\n",
      "Loss decreases to  59.428908234120506\n",
      "The norm of grad vector is  16824.817335944183\n",
      "Loss decreases to  59.425440013164746\n",
      "The norm of grad vector is  16822.09556912212\n",
      "Loss decreases to  59.42197291420642\n",
      "The norm of grad vector is  16819.374501506074\n",
      "Loss decreases to  59.41850693677574\n",
      "The norm of grad vector is  16816.654132786574\n",
      "Loss decreases to  59.41504208040354\n",
      "The norm of grad vector is  16813.934462654615\n",
      "Loss decreases to  59.411578344620395\n",
      "The norm of grad vector is  16811.2154908018\n",
      "Loss decreases to  59.408115728957696\n",
      "The norm of grad vector is  16808.497216920245\n",
      "Loss decreases to  59.404654232946605\n",
      "The norm of grad vector is  16805.779640702618\n",
      "Loss decreases to  59.40119385611906\n",
      "The norm of grad vector is  16803.062761842062\n",
      "Loss decreases to  59.39773459800675\n",
      "The norm of grad vector is  16800.346580032365\n",
      "Loss decreases to  59.39427645814204\n",
      "The norm of grad vector is  16797.631094967688\n",
      "Loss decreases to  59.39081943605728\n",
      "The norm of grad vector is  16794.91630634285\n",
      "Loss decreases to  59.387363531285196\n",
      "The norm of grad vector is  16792.20221385312\n",
      "Loss decreases to  59.38390874335887\n",
      "The norm of grad vector is  16789.488817194353\n",
      "Loss decreases to  59.38045507181145\n",
      "The norm of grad vector is  16786.776116062894\n",
      "Loss decreases to  59.3770025161766\n",
      "The norm of grad vector is  16784.064110155563\n",
      "Loss decreases to  59.37355107598791\n",
      "The norm of grad vector is  16781.352799169763\n",
      "Loss decreases to  59.370100750779564\n",
      "The norm of grad vector is  16778.64218280339\n",
      "Loss decreases to  59.36665154008583\n",
      "The norm of grad vector is  16775.932260754882\n",
      "Loss decreases to  59.36320344344123\n",
      "The norm of grad vector is  16773.223032723094\n",
      "Loss decreases to  59.35975646038069\n",
      "The norm of grad vector is  16770.514498407523\n",
      "Loss decreases to  59.356310590439165\n",
      "The norm of grad vector is  16767.806657508074\n",
      "Loss decreases to  59.35286583315212\n",
      "The norm of grad vector is  16765.099509725205\n",
      "Loss decreases to  59.349422188055186\n",
      "The norm of grad vector is  16762.39305475991\n",
      "Loss decreases to  59.34597965468418\n",
      "The norm of grad vector is  16759.68729231357\n",
      "Loss decreases to  59.34253823257525\n",
      "The norm of grad vector is  16756.982222088252\n",
      "Loss decreases to  59.3390979212648\n",
      "The norm of grad vector is  16754.27784378632\n",
      "Loss decreases to  59.33565872028946\n",
      "The norm of grad vector is  16751.574157110797\n",
      "Loss decreases to  59.332220629186246\n",
      "The norm of grad vector is  16748.871161765117\n",
      "Loss decreases to  59.32878364749212\n",
      "The norm of grad vector is  16746.168857453227\n",
      "Loss decreases to  59.32534777474473\n",
      "The norm of grad vector is  16743.467243879597\n",
      "Loss decreases to  59.321913010481616\n",
      "The norm of grad vector is  16740.766320749168\n",
      "Loss decreases to  59.318479354240836\n",
      "The norm of grad vector is  16738.066087767333\n",
      "Loss decreases to  59.315046805560634\n",
      "The norm of grad vector is  16735.366544640034\n",
      "Loss decreases to  59.311615363979314\n",
      "The norm of grad vector is  16732.66769107366\n",
      "Loss decreases to  59.30818502903566\n",
      "The norm of grad vector is  16729.96952677517\n",
      "Loss decreases to  59.30475580026877\n",
      "The norm of grad vector is  16727.272051451808\n",
      "Loss decreases to  59.301327677217614\n",
      "The norm of grad vector is  16724.57526481154\n",
      "Loss decreases to  59.29790065942209\n",
      "The norm of grad vector is  16721.87916656267\n",
      "Loss decreases to  59.29447474642155\n",
      "The norm of grad vector is  16719.183756413975\n",
      "Loss decreases to  59.291049937756235\n",
      "The norm of grad vector is  16716.48903407479\n",
      "Loss decreases to  59.28762623296622\n",
      "The norm of grad vector is  16713.79499925487\n",
      "Loss decreases to  59.28420363159221\n",
      "The norm of grad vector is  16711.101651664456\n",
      "Loss decreases to  59.28078213317492\n",
      "The norm of grad vector is  16708.40899101423\n",
      "Loss decreases to  59.27736173725525\n",
      "The norm of grad vector is  16705.717017015406\n",
      "Loss decreases to  59.273942443374644\n",
      "The norm of grad vector is  16703.025729379646\n",
      "Loss decreases to  59.27052425107452\n",
      "The norm of grad vector is  16700.33512781904\n",
      "Loss decreases to  59.267107159896746\n",
      "The norm of grad vector is  16697.645212046187\n",
      "Loss decreases to  59.263691169383314\n",
      "The norm of grad vector is  16694.95598177412\n",
      "Loss decreases to  59.26027627907649\n",
      "The norm of grad vector is  16692.267436716338\n",
      "Loss decreases to  59.25686248851886\n",
      "The norm of grad vector is  16689.579576586853\n",
      "Loss decreases to  59.253449797253154\n",
      "The norm of grad vector is  16686.892401100045\n",
      "Loss decreases to  59.250038204822594\n",
      "The norm of grad vector is  16684.205909970846\n",
      "Loss decreases to  59.2466277107703\n",
      "The norm of grad vector is  16681.5201029146\n",
      "Loss decreases to  59.24321831463985\n",
      "The norm of grad vector is  16678.834979647054\n",
      "Loss decreases to  59.239810015975145\n",
      "The norm of grad vector is  16676.150539884522\n",
      "Loss decreases to  59.23640281432012\n",
      "The norm of grad vector is  16673.466783343658\n",
      "Loss decreases to  59.23299670921909\n",
      "The norm of grad vector is  16670.78370974164\n",
      "Loss decreases to  59.22959170021678\n",
      "The norm of grad vector is  16668.101318796067\n",
      "Loss decreases to  59.22618778685774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  16665.41961022497\n",
      "Loss decreases to  59.222784968687186\n",
      "The norm of grad vector is  16662.738583746872\n",
      "Loss decreases to  59.21938324525042\n",
      "The norm of grad vector is  16660.058239080674\n",
      "Loss decreases to  59.21598261609289\n",
      "The norm of grad vector is  16657.378575945782\n",
      "Loss decreases to  59.21258308076055\n",
      "The norm of grad vector is  16654.699594062025\n",
      "Loss decreases to  59.20918463879935\n",
      "The norm of grad vector is  16652.02129314963\n",
      "Loss decreases to  59.20578728975556\n",
      "The norm of grad vector is  16649.343672929313\n",
      "Loss decreases to  59.20239103317582\n",
      "The norm of grad vector is  16646.66673312224\n",
      "Loss decreases to  59.198995868606765\n",
      "The norm of grad vector is  16643.990473449896\n",
      "Loss decreases to  59.19560179559572\n",
      "The norm of grad vector is  16641.314893634357\n",
      "Loss decreases to  59.192208813689724\n",
      "The norm of grad vector is  16638.639993398017\n",
      "Loss decreases to  59.18881692243631\n",
      "The norm of grad vector is  16635.965772463773\n",
      "Loss decreases to  59.18542612138344\n",
      "The norm of grad vector is  16633.292230554907\n",
      "Loss decreases to  59.18203641007901\n",
      "The norm of grad vector is  16630.61936739513\n",
      "Loss decreases to  59.17864778807131\n",
      "The norm of grad vector is  16627.94718270863\n",
      "Loss decreases to  59.1752602549089\n",
      "The norm of grad vector is  16625.275676219888\n",
      "Loss decreases to  59.1718738101405\n",
      "The norm of grad vector is  16622.60484765398\n",
      "Loss decreases to  59.168488453315135\n",
      "The norm of grad vector is  16619.9346967363\n",
      "Loss decreases to  59.1651041839821\n",
      "The norm of grad vector is  16617.265223192695\n",
      "Loss decreases to  59.161721001690786\n",
      "The norm of grad vector is  16614.59642674939\n",
      "Loss decreases to  59.15833890599105\n",
      "The norm of grad vector is  16611.928307133094\n",
      "Loss decreases to  59.154957896432876\n",
      "The norm of grad vector is  16609.26086407089\n",
      "Loss decreases to  59.15157797256637\n",
      "The norm of grad vector is  16606.59409729026\n",
      "Loss decreases to  59.14819913394219\n",
      "The norm of grad vector is  16603.928006519152\n",
      "Loss decreases to  59.144821380110855\n",
      "The norm of grad vector is  16601.262591485873\n",
      "Loss decreases to  59.14144471062357\n",
      "The norm of grad vector is  16598.597851919167\n",
      "Loss decreases to  59.13806912503134\n",
      "The norm of grad vector is  16595.93378754821\n",
      "Loss decreases to  59.13469462288567\n",
      "The norm of grad vector is  16593.270398102526\n",
      "Loss decreases to  59.13132120373824\n",
      "The norm of grad vector is  16590.607683312093\n",
      "Loss decreases to  59.12794886714098\n",
      "The norm of grad vector is  16587.94564290727\n",
      "Loss decreases to  59.124577612646085\n",
      "The norm of grad vector is  16585.284276618837\n",
      "Loss decreases to  59.121207439805985\n",
      "The norm of grad vector is  16582.623584177993\n",
      "Loss decreases to  59.11783834817327\n",
      "The norm of grad vector is  16579.963565316277\n",
      "Loss decreases to  59.1144703373009\n",
      "The norm of grad vector is  16577.30421976566\n",
      "Loss decreases to  59.11110340674202\n",
      "The norm of grad vector is  16574.645547258584\n",
      "Loss decreases to  59.107737556049884\n",
      "The norm of grad vector is  16571.987547527737\n",
      "Loss decreases to  59.104372784778256\n",
      "The norm of grad vector is  16569.330220306318\n",
      "Loss decreases to  59.10100909248101\n",
      "The norm of grad vector is  16566.67356532791\n",
      "Loss decreases to  59.0976464787121\n",
      "The norm of grad vector is  16564.017582326425\n",
      "Loss decreases to  59.09428494302589\n",
      "The norm of grad vector is  16561.362271036243\n",
      "Loss decreases to  59.09092448497701\n",
      "The norm of grad vector is  16558.707631192083\n",
      "Loss decreases to  59.08756510412025\n",
      "The norm of grad vector is  16556.053662529066\n",
      "Loss decreases to  59.084206800010634\n",
      "The norm of grad vector is  16553.40036478273\n",
      "Loss decreases to  59.08084957220345\n",
      "The norm of grad vector is  16550.74773768895\n",
      "Loss decreases to  59.077493420254385\n",
      "The norm of grad vector is  16548.09578098401\n",
      "Loss decreases to  59.07413834371897\n",
      "The norm of grad vector is  16545.4444944046\n",
      "Loss decreases to  59.07078434215338\n",
      "The norm of grad vector is  16542.793877687727\n",
      "Loss decreases to  59.06743141511378\n",
      "The norm of grad vector is  16540.143930570863\n",
      "Loss decreases to  59.064079562156564\n",
      "The norm of grad vector is  16537.494652791822\n",
      "Loss decreases to  59.06072878283864\n",
      "The norm of grad vector is  16534.846044088765\n",
      "Loss decreases to  59.05737907671675\n",
      "The norm of grad vector is  16532.198104200277\n",
      "Loss decreases to  59.05403044334823\n",
      "The norm of grad vector is  16529.550832865287\n",
      "Loss decreases to  59.0506828822905\n",
      "The norm of grad vector is  16526.904229823183\n",
      "Loss decreases to  59.04733639310106\n",
      "The norm of grad vector is  16524.25829481356\n",
      "Loss decreases to  59.043990975337906\n",
      "The norm of grad vector is  16521.61302757654\n",
      "Loss decreases to  59.040646628559294\n",
      "The norm of grad vector is  16518.968427852524\n",
      "Loss decreases to  59.03730335232337\n",
      "The norm of grad vector is  16516.324495382356\n",
      "Loss decreases to  59.03396114618884\n",
      "The norm of grad vector is  16513.6812299072\n",
      "Loss decreases to  59.03062000971454\n",
      "The norm of grad vector is  16511.038631168598\n",
      "Loss decreases to  59.027279942459494\n",
      "The norm of grad vector is  16508.396698908466\n",
      "Loss decreases to  59.023940943983014\n",
      "The norm of grad vector is  16505.755432869064\n",
      "Loss decreases to  59.02060301384457\n",
      "The norm of grad vector is  16503.114832793035\n",
      "Loss decreases to  59.017266151604034\n",
      "The norm of grad vector is  16500.474898423392\n",
      "Loss decreases to  59.01393035682147\n",
      "The norm of grad vector is  16497.835629503472\n",
      "Loss decreases to  59.01059562905681\n",
      "The norm of grad vector is  16495.19702577702\n",
      "Loss decreases to  59.007261967870896\n",
      "The norm of grad vector is  16492.559086988127\n",
      "Loss decreases to  59.00392937282417\n",
      "The norm of grad vector is  16489.921812881195\n",
      "Loss decreases to  59.00059784347771\n",
      "The norm of grad vector is  16487.285203201074\n",
      "Loss decreases to  58.997267379392554\n",
      "The norm of grad vector is  16484.649257692883\n",
      "Loss decreases to  58.99393798013013\n",
      "The norm of grad vector is  16482.01397610213\n",
      "Loss decreases to  58.99060964525215\n",
      "The norm of grad vector is  16479.379358174676\n",
      "Loss decreases to  58.98728237432028\n",
      "The norm of grad vector is  16476.745403656732\n",
      "Loss decreases to  58.98395616689678\n",
      "The norm of grad vector is  16474.112112294883\n",
      "Loss decreases to  58.98063102254392\n",
      "The norm of grad vector is  16471.479483836018\n",
      "Loss decreases to  58.97730694082426\n",
      "The norm of grad vector is  16468.84751802738\n",
      "Loss decreases to  58.97398392130056\n",
      "The norm of grad vector is  16466.21621461663\n",
      "Loss decreases to  58.97066196353575\n",
      "The norm of grad vector is  16463.585573351702\n",
      "Loss decreases to  58.96734106709322\n",
      "The norm of grad vector is  16460.955593980867\n",
      "Loss decreases to  58.964021231536314\n",
      "The norm of grad vector is  16458.326276252807\n",
      "Loss decreases to  58.96070245642884\n",
      "The norm of grad vector is  16455.697619916482\n",
      "Loss decreases to  58.95738474133462\n",
      "The norm of grad vector is  16453.06962472126\n",
      "Loss decreases to  58.95406808581787\n",
      "The norm of grad vector is  16450.442290416766\n",
      "Loss decreases to  58.95075248944296\n",
      "The norm of grad vector is  16447.815616753014\n",
      "Loss decreases to  58.947437951774404\n",
      "The norm of grad vector is  16445.1896034804\n",
      "Loss decreases to  58.944124472377275\n",
      "The norm of grad vector is  16442.564250349522\n",
      "Loss decreases to  58.94081205081638\n",
      "The norm of grad vector is  16439.939557111462\n",
      "Loss decreases to  58.93750068665718\n",
      "The norm of grad vector is  16437.315523517627\n",
      "Loss decreases to  58.93419037946521\n",
      "The norm of grad vector is  16434.69214931958\n",
      "Loss decreases to  58.930881128806135\n",
      "The norm of grad vector is  16432.06943426943\n",
      "Loss decreases to  58.92757293424587\n",
      "The norm of grad vector is  16429.44737811951\n",
      "Loss decreases to  58.924265795350784\n",
      "The norm of grad vector is  16426.82598062248\n",
      "Loss decreases to  58.92095971168716\n",
      "The norm of grad vector is  16424.20524153141\n",
      "Loss decreases to  58.91765468282177\n",
      "The norm of grad vector is  16421.58516059959\n",
      "Loss decreases to  58.91435070832146\n",
      "The norm of grad vector is  16418.965737580762\n",
      "Loss decreases to  58.91104778775332\n",
      "The norm of grad vector is  16416.346972228836\n",
      "Loss decreases to  58.9077459206847\n",
      "The norm of grad vector is  16413.7288642982\n",
      "Loss decreases to  58.90444510668318\n",
      "The norm of grad vector is  16411.111413543444\n",
      "Loss decreases to  58.90114534531645\n",
      "The norm of grad vector is  16408.494619719586\n",
      "Loss decreases to  58.89784663615258\n",
      "The norm of grad vector is  16405.878482581902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss decreases to  58.8945489787598\n",
      "The norm of grad vector is  16403.263001886\n",
      "Loss decreases to  58.891252372706674\n",
      "The norm of grad vector is  16400.64817738783\n",
      "Loss decreases to  58.88795681756173\n",
      "The norm of grad vector is  16398.034008843602\n",
      "Loss decreases to  58.88466231289395\n",
      "The norm of grad vector is  16395.420496009952\n",
      "Loss decreases to  58.881368858272495\n",
      "The norm of grad vector is  16392.807638643717\n",
      "Loss decreases to  58.878076453266544\n",
      "The norm of grad vector is  16390.195436502123\n",
      "Loss decreases to  58.87478509744589\n",
      "The norm of grad vector is  16387.583889342674\n",
      "Loss decreases to  58.87149479038026\n",
      "The norm of grad vector is  16384.9729969232\n",
      "Loss decreases to  58.86820553163963\n",
      "The norm of grad vector is  16382.362759001866\n",
      "Loss decreases to  58.864917320794305\n",
      "The norm of grad vector is  16379.753175337128\n",
      "Loss decreases to  58.86163015741475\n",
      "The norm of grad vector is  16377.144245687718\n",
      "Loss decreases to  58.85834404107152\n",
      "The norm of grad vector is  16374.535969812758\n",
      "Loss decreases to  58.855058971335815\n",
      "The norm of grad vector is  16371.928347471596\n",
      "Loss decreases to  58.8517749477785\n",
      "The norm of grad vector is  16369.321378423949\n",
      "Loss decreases to  58.84849196997104\n",
      "The norm of grad vector is  16366.71506242984\n",
      "Loss decreases to  58.84521003748507\n",
      "The norm of grad vector is  16364.109399249533\n",
      "Loss decreases to  58.84192914989226\n",
      "The norm of grad vector is  16361.504388643634\n",
      "Loss decreases to  58.83864930676476\n",
      "The norm of grad vector is  16358.900030373088\n",
      "Loss decreases to  58.83537050767463\n",
      "The norm of grad vector is  16356.296324199137\n",
      "Loss decreases to  58.832092752194555\n",
      "The norm of grad vector is  16353.693269883255\n",
      "Loss decreases to  58.82881603989708\n",
      "The norm of grad vector is  16351.09086718727\n",
      "Loss decreases to  58.825540370355135\n",
      "The norm of grad vector is  16348.48911587329\n",
      "Loss decreases to  58.82226574314186\n",
      "The norm of grad vector is  16345.888015703797\n",
      "Loss decreases to  58.818992157830536\n",
      "The norm of grad vector is  16343.287566441446\n",
      "Loss decreases to  58.81571961399483\n",
      "The norm of grad vector is  16340.687767849266\n",
      "Loss decreases to  58.81244811120846\n",
      "The norm of grad vector is  16338.088619690596\n",
      "Loss decreases to  58.80917764904547\n",
      "The norm of grad vector is  16335.490121729044\n",
      "Loss decreases to  58.80590822707999\n",
      "The norm of grad vector is  16332.892273728457\n",
      "Loss decreases to  58.80263984488642\n",
      "The norm of grad vector is  16330.295075453065\n",
      "Loss decreases to  58.79937250203957\n",
      "The norm of grad vector is  16327.698526667333\n",
      "Loss decreases to  58.79610619811428\n",
      "The norm of grad vector is  16325.102627136093\n",
      "Loss decreases to  58.79284093268556\n",
      "The norm of grad vector is  16322.507376624351\n",
      "Loss decreases to  58.789576705328805\n",
      "The norm of grad vector is  16319.912774897504\n",
      "Loss decreases to  58.78631351561949\n",
      "The norm of grad vector is  16317.318821721183\n",
      "Loss decreases to  58.783051363133374\n",
      "The norm of grad vector is  16314.725516861325\n",
      "Loss decreases to  58.779790247446414\n",
      "The norm of grad vector is  16312.13286008414\n",
      "Loss decreases to  58.77653016813483\n",
      "The norm of grad vector is  16309.540851156165\n",
      "Loss decreases to  58.77327112477488\n",
      "The norm of grad vector is  16306.949489844159\n",
      "Loss decreases to  58.77001311694342\n",
      "The norm of grad vector is  16304.358775915212\n",
      "Loss decreases to  58.7667561442171\n",
      "The norm of grad vector is  16301.768709136675\n",
      "Loss decreases to  58.76350020617294\n",
      "The norm of grad vector is  16299.179289276188\n",
      "Loss decreases to  58.7602453023883\n",
      "The norm of grad vector is  16296.590516101698\n",
      "Loss decreases to  58.75699143244062\n",
      "The norm of grad vector is  16294.002389381376\n",
      "Loss decreases to  58.75373859590752\n",
      "The norm of grad vector is  16291.414908883733\n",
      "Loss decreases to  58.750486792367106\n",
      "The norm of grad vector is  16288.828074377523\n",
      "Loss decreases to  58.74723602139731\n",
      "The norm of grad vector is  16286.24188563172\n",
      "Loss decreases to  58.743986282576515\n",
      "The norm of grad vector is  16283.656342415727\n",
      "Loss decreases to  58.740737575483344\n",
      "The norm of grad vector is  16281.07144449911\n",
      "Loss decreases to  58.73748989969651\n",
      "The norm of grad vector is  16278.487191651724\n",
      "Loss decreases to  58.73424325479497\n",
      "The norm of grad vector is  16275.90358364373\n",
      "Loss decreases to  58.73099764035796\n",
      "The norm of grad vector is  16273.320620245493\n",
      "Loss decreases to  58.72775305596488\n",
      "The norm of grad vector is  16270.738301227757\n",
      "Loss decreases to  58.72450950119539\n",
      "The norm of grad vector is  16268.156626361457\n",
      "Loss decreases to  58.721266975629256\n",
      "The norm of grad vector is  16265.57559541783\n",
      "Loss decreases to  58.7180254788466\n",
      "The norm of grad vector is  16262.995208168346\n",
      "Loss decreases to  58.71478501042762\n",
      "The norm of grad vector is  16260.415464384823\n",
      "Loss decreases to  58.71154556995281\n",
      "The norm of grad vector is  16257.836363839224\n",
      "Loss decreases to  58.70830715700289\n",
      "The norm of grad vector is  16255.257906303936\n",
      "Loss decreases to  58.70506977115875\n",
      "The norm of grad vector is  16252.68009155147\n",
      "Loss decreases to  58.70183341200143\n",
      "The norm of grad vector is  16250.102919354711\n",
      "Loss decreases to  58.698598079112394\n",
      "The norm of grad vector is  16247.526389486704\n",
      "Loss decreases to  58.695363772073094\n",
      "The norm of grad vector is  16244.950501720847\n",
      "Loss decreases to  58.69213049046534\n",
      "The norm of grad vector is  16242.37525583077\n",
      "Loss decreases to  58.68889823387102\n",
      "The norm of grad vector is  16239.800651590347\n",
      "Loss decreases to  58.68566700187238\n",
      "The norm of grad vector is  16237.226688773726\n",
      "Loss decreases to  58.68243679405171\n",
      "The norm of grad vector is  16234.653367155366\n",
      "Loss decreases to  58.67920760999169\n",
      "The norm of grad vector is  16232.080686509895\n",
      "Loss decreases to  58.67597944927512\n",
      "The norm of grad vector is  16229.508646612248\n",
      "Loss decreases to  58.67275231148497\n",
      "The norm of grad vector is  16226.937247237613\n",
      "Loss decreases to  58.66952619620439\n",
      "The norm of grad vector is  16224.36648816147\n",
      "Loss decreases to  58.66630110301711\n",
      "The norm of grad vector is  16221.796369159485\n",
      "Loss decreases to  58.663077031506475\n",
      "The norm of grad vector is  16219.226890007645\n",
      "Loss decreases to  58.65985398125651\n",
      "The norm of grad vector is  16216.658050482136\n",
      "Loss decreases to  58.65663195185123\n",
      "The norm of grad vector is  16214.089850359462\n",
      "Loss decreases to  58.653410942874984\n",
      "The norm of grad vector is  16211.522289416313\n",
      "Loss decreases to  58.65019095391212\n",
      "The norm of grad vector is  16208.955367429684\n",
      "Loss decreases to  58.646971984547434\n",
      "The norm of grad vector is  16206.389084176799\n",
      "Loss decreases to  58.64375403436578\n",
      "The norm of grad vector is  16203.823439435148\n",
      "Loss decreases to  58.640537102952315\n",
      "The norm of grad vector is  16201.258432982453\n",
      "Loss decreases to  58.63732118989242\n",
      "The norm of grad vector is  16198.69406459666\n",
      "Loss decreases to  58.63410629477146\n",
      "The norm of grad vector is  16196.130334056039\n",
      "Loss decreases to  58.63089241717523\n",
      "The norm of grad vector is  16193.567241139033\n",
      "Loss decreases to  58.62767955668985\n",
      "The norm of grad vector is  16191.004785624364\n",
      "Loss decreases to  58.624467712901264\n",
      "The norm of grad vector is  16188.442967291046\n",
      "Loss decreases to  58.62125688539596\n",
      "The norm of grad vector is  16185.881785918275\n",
      "Loss decreases to  58.61804707376041\n",
      "The norm of grad vector is  16183.321241285486\n",
      "Loss decreases to  58.61483827758149\n",
      "The norm of grad vector is  16180.761333172377\n",
      "Loss decreases to  58.61163049644611\n",
      "The norm of grad vector is  16178.202061358912\n",
      "Loss decreases to  58.608423729941485\n",
      "The norm of grad vector is  16175.643425625318\n",
      "Loss decreases to  58.60521797765512\n",
      "The norm of grad vector is  16173.085425751973\n",
      "Loss decreases to  58.60201323917446\n",
      "The norm of grad vector is  16170.528061519559\n",
      "Loss decreases to  58.59880951408734\n",
      "The norm of grad vector is  16167.971332709\n",
      "Loss decreases to  58.595606801981866\n",
      "The norm of grad vector is  16165.415239101447\n",
      "Loss decreases to  58.59240510244617\n",
      "The norm of grad vector is  16162.859780478295\n",
      "Loss decreases to  58.58920441506883\n",
      "The norm of grad vector is  16160.304956621156\n",
      "Loss decreases to  58.586004739438266\n",
      "The norm of grad vector is  16157.75076731192\n",
      "Loss decreases to  58.58280607514357\n",
      "The norm of grad vector is  16155.197212332683\n",
      "Loss decreases to  58.57960842177354\n",
      "The norm of grad vector is  16152.644291465745\n",
      "Loss decreases to  58.57641177891765\n",
      "The norm of grad vector is  16150.092004493777\n",
      "Loss decreases to  58.57321614616515\n",
      "The norm of grad vector is  16147.540351199481\n",
      "Loss decreases to  58.570021523105915\n",
      "The norm of grad vector is  16144.989331365992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss decreases to  58.56682790932956\n",
      "The norm of grad vector is  16142.438944776528\n",
      "Loss decreases to  58.56363530442648\n",
      "The norm of grad vector is  16139.889191214614\n",
      "Loss decreases to  58.56044370798666\n",
      "The norm of grad vector is  16137.340070464006\n",
      "Loss decreases to  58.557253119600674\n",
      "The norm of grad vector is  16134.791582308628\n",
      "Loss decreases to  58.5540635388593\n",
      "The norm of grad vector is  16132.243726532723\n",
      "Loss decreases to  58.55087496535331\n",
      "The norm of grad vector is  16129.69650292074\n",
      "Loss decreases to  58.54768739867382\n",
      "The norm of grad vector is  16127.14991125729\n",
      "Loss decreases to  58.544500838412155\n",
      "The norm of grad vector is  16124.603951327323\n",
      "Loss decreases to  58.541315284159765\n",
      "The norm of grad vector is  16122.058622915893\n",
      "Loss decreases to  58.538130735508474\n",
      "The norm of grad vector is  16119.513925808373\n",
      "Loss decreases to  58.53494719205002\n",
      "The norm of grad vector is  16116.969859790348\n",
      "Loss decreases to  58.531764653376605\n",
      "The norm of grad vector is  16114.426424647552\n",
      "Loss decreases to  58.52858311908056\n",
      "The norm of grad vector is  16111.883620166062\n",
      "Loss decreases to  58.52540258875426\n",
      "The norm of grad vector is  16109.341446132099\n",
      "Loss decreases to  58.522223061990644\n",
      "The norm of grad vector is  16106.79990233213\n",
      "Loss decreases to  58.519044538382374\n",
      "The norm of grad vector is  16104.258988552845\n",
      "Loss decreases to  58.515867017522794\n",
      "The norm of grad vector is  16101.718704581162\n",
      "Loss decreases to  58.512690499005025\n",
      "The norm of grad vector is  16099.179050204217\n",
      "Loss decreases to  58.50951498242276\n",
      "The norm of grad vector is  16096.640025209359\n",
      "Loss decreases to  58.50634046736965\n",
      "The norm of grad vector is  16094.101629384142\n",
      "Loss decreases to  58.50316695343962\n",
      "The norm of grad vector is  16091.563862516377\n",
      "Loss decreases to  58.49999444022673\n",
      "The norm of grad vector is  16089.026724394076\n",
      "Loss decreases to  58.49682292732546\n",
      "The norm of grad vector is  16086.490214805484\n",
      "Loss decreases to  58.49365241433024\n",
      "The norm of grad vector is  16083.954333538977\n",
      "Loss decreases to  58.49048290083577\n",
      "The norm of grad vector is  16081.419080383324\n",
      "Loss decreases to  58.48731438643702\n",
      "The norm of grad vector is  16078.884455127301\n",
      "Loss decreases to  58.48414687072915\n",
      "The norm of grad vector is  16076.350457560078\n",
      "Loss decreases to  58.48098035330743\n",
      "The norm of grad vector is  16073.81708747093\n",
      "Loss decreases to  58.477814833767376\n",
      "The norm of grad vector is  16071.284344649357\n",
      "Loss decreases to  58.474650311704785\n",
      "The norm of grad vector is  16068.752228885143\n",
      "Loss decreases to  58.47148678671546\n",
      "The norm of grad vector is  16066.22073996824\n",
      "Loss decreases to  58.468324258395675\n",
      "The norm of grad vector is  16063.689877688754\n",
      "Loss decreases to  58.46516272634168\n",
      "The norm of grad vector is  16061.159641837117\n",
      "Loss decreases to  58.462002190149995\n",
      "The norm of grad vector is  16058.630032203859\n",
      "Loss decreases to  58.45884264941742\n",
      "The norm of grad vector is  16056.101048579832\n",
      "Loss decreases to  58.45568410374078\n",
      "The norm of grad vector is  16053.572690756002\n",
      "Loss decreases to  58.45252655271724\n",
      "The norm of grad vector is  16051.044958523595\n",
      "Loss decreases to  58.44936999594409\n",
      "The norm of grad vector is  16048.517851674022\n",
      "Loss decreases to  58.446214433018966\n",
      "The norm of grad vector is  16045.99136999892\n",
      "Loss decreases to  58.44305986353941\n",
      "The norm of grad vector is  16043.465513290115\n",
      "Loss decreases to  58.43990628710344\n",
      "The norm of grad vector is  16040.940281339663\n",
      "Loss decreases to  58.43675370330912\n",
      "The norm of grad vector is  16038.415673939793\n",
      "Loss decreases to  58.43360211175479\n",
      "The norm of grad vector is  16035.891690882983\n",
      "Loss decreases to  58.43045151203895\n",
      "The norm of grad vector is  16033.368331961887\n",
      "Loss decreases to  58.42730190376033\n",
      "The norm of grad vector is  16030.845596969353\n",
      "Loss decreases to  58.424153286517765\n",
      "The norm of grad vector is  16028.323485698476\n",
      "Loss decreases to  58.4210056599104\n",
      "The norm of grad vector is  16025.80199794247\n",
      "Loss decreases to  58.41785902353754\n",
      "The norm of grad vector is  16023.281133494851\n",
      "Loss decreases to  58.41471337699867\n",
      "The norm of grad vector is  16020.760892149277\n",
      "Loss decreases to  58.41156871989342\n",
      "The norm of grad vector is  16018.241273699641\n",
      "Loss decreases to  58.40842505182175\n",
      "The norm of grad vector is  16015.72227794\n",
      "Loss decreases to  58.40528237238368\n",
      "The norm of grad vector is  16013.203904664635\n",
      "Loss decreases to  58.40214068117952\n",
      "The norm of grad vector is  16010.686153667979\n",
      "Loss decreases to  58.39899997780979\n",
      "The norm of grad vector is  16008.169024744751\n",
      "Loss decreases to  58.39586026187507\n",
      "The norm of grad vector is  16005.652517689814\n",
      "Loss decreases to  58.392721532976196\n",
      "The norm of grad vector is  16003.13663229823\n",
      "Loss decreases to  58.3895837907144\n",
      "The norm of grad vector is  16000.621368365277\n",
      "Loss decreases to  58.386447034690754\n",
      "The norm of grad vector is  15998.106725686393\n",
      "Loss decreases to  58.383311264506816\n",
      "The norm of grad vector is  15995.592704057242\n",
      "Loss decreases to  58.3801764797641\n",
      "The norm of grad vector is  15993.079303273656\n",
      "Loss decreases to  58.37704268006463\n",
      "The norm of grad vector is  15990.56652313173\n",
      "Loss decreases to  58.37390986501031\n",
      "The norm of grad vector is  15988.054363427656\n",
      "Loss decreases to  58.37077803420349\n",
      "The norm of grad vector is  15985.542823957914\n",
      "Loss decreases to  58.36764718724644\n",
      "The norm of grad vector is  15983.031904519092\n",
      "Loss decreases to  58.36451732374181\n",
      "The norm of grad vector is  15980.521604908028\n",
      "Loss decreases to  58.361388443292434\n",
      "The norm of grad vector is  15978.01192492173\n",
      "Loss decreases to  58.3582605455014\n",
      "The norm of grad vector is  15975.502864357406\n",
      "Loss decreases to  58.35513362997181\n",
      "The norm of grad vector is  15972.994423012437\n",
      "Loss decreases to  58.35200769630698\n",
      "The norm of grad vector is  15970.486600684397\n",
      "Loss decreases to  58.34888274411066\n",
      "The norm of grad vector is  15967.97939717107\n",
      "Loss decreases to  58.34575877298658\n",
      "The norm of grad vector is  15965.472812270424\n",
      "Loss decreases to  58.34263578253867\n",
      "The norm of grad vector is  15962.966845780613\n",
      "Loss decreases to  58.33951377237115\n",
      "The norm of grad vector is  15960.461497499975\n",
      "Loss decreases to  58.336392742088265\n",
      "The norm of grad vector is  15957.956767227011\n",
      "Loss decreases to  58.33327269129474\n",
      "The norm of grad vector is  15955.452654760464\n",
      "Loss decreases to  58.33015361959508\n",
      "The norm of grad vector is  15952.949159899204\n",
      "Loss decreases to  58.327035526594486\n",
      "The norm of grad vector is  15950.446282442335\n",
      "Loss decreases to  58.323918411897814\n",
      "The norm of grad vector is  15947.944022189144\n",
      "Loss decreases to  58.3208022751106\n",
      "The norm of grad vector is  15945.442378939018\n",
      "Loss decreases to  58.3176871158383\n",
      "The norm of grad vector is  15942.94135249165\n",
      "Loss decreases to  58.314572933686506\n",
      "The norm of grad vector is  15940.440942646877\n",
      "Loss decreases to  58.31145972826125\n",
      "The norm of grad vector is  15937.941149204658\n",
      "Loss decreases to  58.30834749916858\n",
      "The norm of grad vector is  15935.441971965203\n",
      "Loss decreases to  58.30523624601468\n",
      "The norm of grad vector is  15932.943410728882\n",
      "Loss decreases to  58.30212596840613\n",
      "The norm of grad vector is  15930.44546529623\n",
      "Loss decreases to  58.29901666594959\n",
      "The norm of grad vector is  15927.94813546802\n",
      "Loss decreases to  58.29590833825187\n",
      "The norm of grad vector is  15925.451421045116\n",
      "Loss decreases to  58.29280098491986\n",
      "The norm of grad vector is  15922.955321828626\n",
      "Loss decreases to  58.28969460556111\n",
      "The norm of grad vector is  15920.459837619823\n",
      "Loss decreases to  58.28658919978281\n",
      "The norm of grad vector is  15917.964968220187\n",
      "Loss decreases to  58.2834847671927\n",
      "The norm of grad vector is  15915.470713431292\n",
      "Loss decreases to  58.2803813073983\n",
      "The norm of grad vector is  15912.977073054983\n",
      "Loss decreases to  58.277278820007936\n",
      "The norm of grad vector is  15910.484046893223\n",
      "Loss decreases to  58.27417730462976\n",
      "The norm of grad vector is  15907.991634748216\n",
      "Loss decreases to  58.27107676087187\n",
      "The norm of grad vector is  15905.499836422227\n",
      "Loss decreases to  58.267977188343046\n",
      "The norm of grad vector is  15903.00865171785\n",
      "Loss decreases to  58.26487858665208\n",
      "The norm of grad vector is  15900.51808043772\n",
      "Loss decreases to  58.261780955407765\n",
      "The norm of grad vector is  15898.028122384698\n",
      "Loss decreases to  58.258684294219286\n",
      "The norm of grad vector is  15895.538777361868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss decreases to  58.255588602695994\n",
      "The norm of grad vector is  15893.050045172393\n",
      "Loss decreases to  58.252493880447304\n",
      "The norm of grad vector is  15890.561925619673\n",
      "Loss decreases to  58.24940012708298\n",
      "The norm of grad vector is  15888.074418507269\n",
      "Loss decreases to  58.24630734221291\n",
      "The norm of grad vector is  15885.587523638935\n",
      "Loss decreases to  58.243215525447084\n",
      "The norm of grad vector is  15883.101240818536\n",
      "Loss decreases to  58.24012467639582\n",
      "The norm of grad vector is  15880.61556985015\n",
      "Loss decreases to  58.23703479466978\n",
      "The norm of grad vector is  15878.13051053805\n",
      "Loss decreases to  58.23394587987918\n",
      "The norm of grad vector is  15875.646062686625\n",
      "Loss decreases to  58.23085793163508\n",
      "The norm of grad vector is  15873.162226100458\n",
      "Loss decreases to  58.22777094954845\n",
      "The norm of grad vector is  15870.679000584318\n",
      "Loss decreases to  58.22468493323056\n",
      "The norm of grad vector is  15868.196385943129\n",
      "Loss decreases to  58.22159988229271\n",
      "The norm of grad vector is  15865.714381981976\n",
      "Loss decreases to  58.218515796346466\n",
      "The norm of grad vector is  15863.232988506117\n",
      "Loss decreases to  58.215432675003555\n",
      "The norm of grad vector is  15860.752205320992\n",
      "Loss decreases to  58.21235051787612\n",
      "The norm of grad vector is  15858.272032232167\n",
      "Loss decreases to  58.20926932457605\n",
      "The norm of grad vector is  15855.792469045438\n",
      "Loss decreases to  58.206189094715846\n",
      "The norm of grad vector is  15853.313515566726\n",
      "Loss decreases to  58.20310982790796\n",
      "The norm of grad vector is  15850.835171602112\n",
      "Loss decreases to  58.20003152376501\n",
      "The norm of grad vector is  15848.357436957862\n",
      "Loss decreases to  58.19695418189994\n",
      "The norm of grad vector is  15845.880311440396\n",
      "Loss decreases to  58.19387780192586\n",
      "The norm of grad vector is  15843.403794856295\n",
      "Loss decreases to  58.19080238345598\n",
      "The norm of grad vector is  15840.927887012322\n",
      "Loss decreases to  58.18772792610363\n",
      "The norm of grad vector is  15838.452587715372\n",
      "Loss decreases to  58.18465442948262\n",
      "The norm of grad vector is  15835.977896772527\n",
      "Loss decreases to  58.18158189320667\n",
      "The norm of grad vector is  15833.50381399106\n",
      "Loss decreases to  58.17851031688978\n",
      "The norm of grad vector is  15831.03033917837\n",
      "Loss decreases to  58.17543970014599\n",
      "The norm of grad vector is  15828.557472141987\n",
      "Loss decreases to  58.17237004259002\n",
      "The norm of grad vector is  15826.08521268965\n",
      "Loss decreases to  58.1693013438361\n",
      "The norm of grad vector is  15823.61356062925\n",
      "Loss decreases to  58.166233603499016\n",
      "The norm of grad vector is  15821.142515768805\n",
      "Loss decreases to  58.16316682119376\n",
      "The norm of grad vector is  15818.67207791657\n",
      "Loss decreases to  58.16010099653545\n",
      "The norm of grad vector is  15816.202246880885\n",
      "Loss decreases to  58.15703612913931\n",
      "The norm of grad vector is  15813.733022470275\n",
      "Loss decreases to  58.153972218620815\n",
      "The norm of grad vector is  15811.264404493413\n",
      "Loss decreases to  58.15090926459563\n",
      "The norm of grad vector is  15808.79639275915\n",
      "Loss decreases to  58.14784726667971\n",
      "The norm of grad vector is  15806.328987076495\n",
      "Loss decreases to  58.14478622448891\n",
      "The norm of grad vector is  15803.862187254608\n",
      "Loss decreases to  58.141726137639495\n",
      "The norm of grad vector is  15801.395993102753\n",
      "Loss decreases to  58.13866700574795\n",
      "The norm of grad vector is  15798.930404430452\n",
      "Loss decreases to  58.135608828430705\n",
      "The norm of grad vector is  15796.46542104731\n",
      "Loss decreases to  58.13255160530461\n",
      "The norm of grad vector is  15794.001042763086\n",
      "Loss decreases to  58.129495335986576\n",
      "The norm of grad vector is  15791.537269387754\n",
      "Loss decreases to  58.12644002009382\n",
      "The norm of grad vector is  15789.074100731372\n",
      "Loss decreases to  58.12338565724348\n",
      "The norm of grad vector is  15786.611536604227\n",
      "Loss decreases to  58.12033224705313\n",
      "The norm of grad vector is  15784.149576816662\n",
      "Loss decreases to  58.1172797891405\n",
      "The norm of grad vector is  15781.688221179274\n",
      "Loss decreases to  58.114228283123275\n",
      "The norm of grad vector is  15779.227469502726\n",
      "Loss decreases to  58.111177728619765\n",
      "The norm of grad vector is  15776.767321597876\n",
      "Loss decreases to  58.108128125247994\n",
      "The norm of grad vector is  15774.307777275792\n",
      "Loss decreases to  58.10507947262648\n",
      "The norm of grad vector is  15771.848836347563\n",
      "Loss decreases to  58.10203177037371\n",
      "The norm of grad vector is  15769.390498624558\n",
      "Loss decreases to  58.09898501810846\n",
      "The norm of grad vector is  15766.932763918192\n",
      "Loss decreases to  58.095939215449725\n",
      "The norm of grad vector is  15764.475632040105\n",
      "Loss decreases to  58.09289436201653\n",
      "The norm of grad vector is  15762.01910280205\n",
      "Loss decreases to  58.089850457428426\n",
      "The norm of grad vector is  15759.563176015949\n",
      "Loss decreases to  58.086807501304705\n",
      "The norm of grad vector is  15757.107851493873\n",
      "Loss decreases to  58.08376549326513\n",
      "The norm of grad vector is  15754.653129048003\n",
      "Loss decreases to  58.08072443292948\n",
      "The norm of grad vector is  15752.199008490723\n",
      "Loss decreases to  58.07768431991787\n",
      "The norm of grad vector is  15749.745489634526\n",
      "Loss decreases to  58.07464515385045\n",
      "The norm of grad vector is  15747.292572292063\n",
      "Loss decreases to  58.07160693434769\n",
      "The norm of grad vector is  15744.840256276155\n",
      "Loss decreases to  58.068569661030175\n",
      "The norm of grad vector is  15742.388541399743\n",
      "Loss decreases to  58.06553333351864\n",
      "The norm of grad vector is  15739.937427475907\n",
      "Loss decreases to  58.062497951434025\n",
      "The norm of grad vector is  15737.486914317908\n",
      "Loss decreases to  58.05946351439739\n",
      "The norm of grad vector is  15735.037001739138\n",
      "Loss decreases to  58.05643002203013\n",
      "The norm of grad vector is  15732.587689553124\n",
      "Loss decreases to  58.053397473953645\n",
      "The norm of grad vector is  15730.138977573499\n",
      "Loss decreases to  58.050365869789715\n",
      "The norm of grad vector is  15727.690865614164\n",
      "Loss decreases to  58.04733520916\n",
      "The norm of grad vector is  15725.243353489046\n",
      "Loss decreases to  58.04430549168672\n",
      "The norm of grad vector is  15722.796441012244\n",
      "Loss decreases to  58.041276716991966\n",
      "The norm of grad vector is  15720.35012799802\n",
      "Loss decreases to  58.03824888469819\n",
      "The norm of grad vector is  15717.904414260762\n",
      "Loss decreases to  58.03522199442787\n",
      "The norm of grad vector is  15715.45929961503\n",
      "Loss decreases to  58.03219604580382\n",
      "The norm of grad vector is  15713.014783875506\n",
      "Loss decreases to  58.029171038448816\n",
      "The norm of grad vector is  15710.570866857\n",
      "Loss decreases to  58.02614697198608\n",
      "The norm of grad vector is  15708.12754837445\n",
      "Loss decreases to  58.0231238460389\n",
      "The norm of grad vector is  15705.68482824302\n",
      "Loss decreases to  58.02010166023069\n",
      "The norm of grad vector is  15703.242706277915\n",
      "Loss decreases to  58.01708041418504\n",
      "The norm of grad vector is  15700.801182294543\n",
      "Loss decreases to  58.014060107525864\n",
      "The norm of grad vector is  15698.360256108364\n",
      "Loss decreases to  58.011040739877146\n",
      "The norm of grad vector is  15695.919927535151\n",
      "Loss decreases to  58.00802231086291\n",
      "The norm of grad vector is  15693.480196390652\n",
      "Loss decreases to  58.00500482010753\n",
      "The norm of grad vector is  15691.041062490815\n",
      "Loss decreases to  58.00198826723571\n",
      "The norm of grad vector is  15688.602525651713\n",
      "Loss decreases to  57.99897265187202\n",
      "The norm of grad vector is  15686.164585689568\n",
      "Loss decreases to  57.99595797364128\n",
      "The norm of grad vector is  15683.7272424208\n",
      "Loss decreases to  57.9929442321687\n",
      "The norm of grad vector is  15681.29049566181\n",
      "Loss decreases to  57.98993142707942\n",
      "The norm of grad vector is  15678.85434522928\n",
      "Loss decreases to  57.98691955799882\n",
      "The norm of grad vector is  15676.418790940003\n",
      "Loss decreases to  57.9839086245525\n",
      "The norm of grad vector is  15673.983832610811\n",
      "Loss decreases to  57.980898626366354\n",
      "The norm of grad vector is  15671.549470058833\n",
      "Loss decreases to  57.9778895630662\n",
      "The norm of grad vector is  15669.115703101195\n",
      "Loss decreases to  57.97488143427813\n",
      "The norm of grad vector is  15666.682531555234\n",
      "Loss decreases to  57.97187423962856\n",
      "The norm of grad vector is  15664.24995523838\n",
      "Loss decreases to  57.968867978743916\n",
      "The norm of grad vector is  15661.817973968202\n",
      "Loss decreases to  57.965862651250774\n",
      "The norm of grad vector is  15659.386587562483\n",
      "Loss decreases to  57.96285825677605\n",
      "The norm of grad vector is  15656.95579583899\n",
      "Loss decreases to  57.95985479494656\n",
      "The norm of grad vector is  15654.525598615763\n",
      "Loss decreases to  57.95685226538988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  15652.0959957109\n",
      "Loss decreases to  57.95385066773297\n",
      "The norm of grad vector is  15649.666986942657\n",
      "Loss decreases to  57.950850001603506\n",
      "The norm of grad vector is  15647.23857212942\n",
      "Loss decreases to  57.94785026662919\n",
      "The norm of grad vector is  15644.810751089693\n",
      "Loss decreases to  57.944851462437924\n",
      "The norm of grad vector is  15642.383523642153\n",
      "Loss decreases to  57.94185358865765\n",
      "The norm of grad vector is  15639.956889605532\n",
      "Loss decreases to  57.938856644916804\n",
      "The norm of grad vector is  15637.530848798779\n",
      "Loss decreases to  57.93586063084362\n",
      "The norm of grad vector is  15635.105401040913\n",
      "Loss decreases to  57.93286554606678\n",
      "The norm of grad vector is  15632.680546151123\n",
      "Loss decreases to  57.92987139021497\n",
      "The norm of grad vector is  15630.256283948673\n",
      "Loss decreases to  57.92687816291721\n",
      "The norm of grad vector is  15627.83261425303\n",
      "Loss decreases to  57.92388586380246\n",
      "The norm of grad vector is  15625.40953688377\n",
      "Loss decreases to  57.920894492500146\n",
      "The norm of grad vector is  15622.98705166051\n",
      "Loss decreases to  57.91790404863957\n",
      "The norm of grad vector is  15620.565158403148\n",
      "Loss decreases to  57.91491453185054\n",
      "The norm of grad vector is  15618.1438569316\n",
      "Loss decreases to  57.91192594176277\n",
      "The norm of grad vector is  15615.72314706594\n",
      "Loss decreases to  57.90893827800619\n",
      "The norm of grad vector is  15613.303028626382\n",
      "Loss decreases to  57.90595154021101\n",
      "The norm of grad vector is  15610.883501433236\n",
      "Loss decreases to  57.902965728007615\n",
      "The norm of grad vector is  15608.464565306987\n",
      "Loss decreases to  57.899980841026434\n",
      "The norm of grad vector is  15606.046220068183\n",
      "Loss decreases to  57.896996878898086\n",
      "The norm of grad vector is  15603.628465537571\n",
      "Loss decreases to  57.894013841253624\n",
      "The norm of grad vector is  15601.211301535977\n",
      "Loss decreases to  57.89103172772376\n",
      "The norm of grad vector is  15598.794727884366\n",
      "Loss decreases to  57.88805053793989\n",
      "The norm of grad vector is  15596.378744403824\n",
      "Loss decreases to  57.8850702715334\n",
      "The norm of grad vector is  15593.963350915572\n",
      "Loss decreases to  57.882090928135675\n"
     ]
    }
   ],
   "source": [
    "h = linear_regression(d)\n",
    "# h.set_learning_rate(0.000000005)\n",
    "h.set_learning_rate(0.0000000062)\n",
    "\n",
    "\n",
    "h.fit(x, y, iteration=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16622784780702646 52.82199078521046\n"
     ]
    }
   ],
   "source": [
    "print(h.norm_w(),h2.norm_w())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
